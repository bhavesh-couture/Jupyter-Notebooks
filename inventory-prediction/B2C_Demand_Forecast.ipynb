{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7560baa0-f1bf-402d-aef8-792d94839260",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.jars = [\"/app/setup/commons-ip.jar\"]\n",
    "launcher.conf.spark.app.name = \"bhavesh_notebook\"\n",
    "launcher.conf.spark.queue = \"default\"\n",
    "launcher.conf.spark.local.dir = \"/app/tmp\"\n",
    "launcher.conf.spark.sql.shuffle.partitions = 210\n",
    "launcher.conf.spark.sql.shuffle.minPartitions = 20\n",
    "launcher.conf.spark.driver.memory = \"50g\"\n",
    "launcher.conf.spark.ui.showConsoleProgress = \"true\"\n",
    "launcher.master = \"local[15]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c57650b-c4a0-4030-8a75-3307321ec0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://gcdevctrapp01:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[15], app id = local-1686755142293)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import ai.couture.obelisk.commons.Constants._\n",
       "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
       "import ai.couture.obelisk.commons.Constants._\n",
       "import ai.couture.obelisk.commons.io._\n",
       "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
       "import ai.couture.obelisk.commons.utils.DataFrameUtil.minMaxScaler\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.expressions._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.io._\n",
    "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
    "import ai.couture.obelisk.commons.utils.DataFrameUtil.minMaxScaler\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2c227-1992-449c-9818-37054d279281",
   "metadata": {},
   "source": [
    "### Generate train/test Dataset for model per brick at product level training/prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c940baa-230f-42a5-ab75-9680442c1116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generateTransformedTrainForProductLevelDataset: (of: String, datasetPath: String, transformedDatasetPath: String, performNormalization: Boolean)Unit\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generateTransformedTrainForProductLevelDataset(of: String, datasetPath: String, transformedDatasetPath: String, performNormalization: Boolean = true): Unit = {\n",
    "    var xData: DataFrame = ParquetToDF.getDF(s\"$datasetPath/$of/XData\")\n",
    "    \n",
    "    xData = xData\n",
    "    // .select(\"productid\", \"0_sales\", \"1_sales\", \"2_sales\", \"3_sales\", \"4_sales\", \"5_sales\", \"6_sales\", \"7_sales\", \"8_sales\", \"9_sales\", \"10_sales\", \"11_sales\",\n",
    "    //        \"sales_avg_12_months\", \"sales_std_12_months\", \"0_returns\", \"0_users\", \"0_PLPViewsPerDay\", \"0_PLPClicksPerDay\", \"0_PDPCountPerDay\",\n",
    "    //        \"0_TotalAddToCartPerDay\", \"0_wishlist\", \"0_availableQuantity\")\n",
    "    // .toDF(\"productid\", \"0_monthSales\", \"1_monthSales\", \"2_monthSales\", \"3_monthSales\", \"4_monthSales\", \"5_monthSales\", \"6_monthSales\", \"7_monthSales\", \"8_monthSales\", \n",
    "    //       \"9_monthSales\", \"10_monthSales\", \"11_monthSales\", \"avgSales\", \"stddevSales\", \"totalReturn\", \"totalUsers\", \"totalPLPViews\", \"totalPLPClicks\", \n",
    "    //       \"totalPDPCount\", \"totalATC\", \"totalWishList\", \"totalAvailableQuantity\")\n",
    "    var data: DataFrame = null\n",
    "    if (of == \"train\") {\n",
    "        var yData: DataFrame = ParquetToDF.getDF(s\"$datasetPath/$of/YData\")\n",
    "        data = xData.join(yData, Seq(PRODUCTID))\n",
    "    }\n",
    "    else if(of == \"test\"){\n",
    "        data = xData\n",
    "    }\n",
    "    val productAttributesLegosFNL: DataFrame = ParquetToDF.getDF(\"/data/Archive/inventory/productAttributesLegosFNL\")\n",
    "    .select(PRODUCTID, SIMILAR_GROUP_LEVEL, \"colorfamily\", \"brandname\", \"styletype\", \"pattern\", \"sleeve\")\n",
    "    // .select(col(ITEM_ID).as(PRODUCTID), col(SIMILAR_GROUP_LEVEL), col(\"colorfamily\"), col(\"brandname\"), col(\"styletype\"), col(\"pattern\"))\n",
    "    // .dropDuplicates(PRODUCTID)\n",
    "    \n",
    "    data = data.join(productAttributesLegosFNL, Seq(PRODUCTID)).na.fill(\"Null\")\n",
    "    var attributes = Array(\"colorfamily\", \"pattern\", \"brandname\", \"styletype\", \"sleeve\")\n",
    "    attributes.foreach(\n",
    "        attribute => {\n",
    "            var attrDFMS = CSVToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/embeddings/menShirts/${attribute}.csv\", inferSchema=true)\n",
    "            \n",
    "            attrDFMS = attrDFMS\n",
    "            .select(attribute, attrDFMS.columns.filter(_.startsWith(\"normalized\")): _*)\n",
    "            .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830216013\"))\n",
    "            \n",
    "            var attrDFWK = CSVToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/embeddings/womenKurtas/${attribute}.csv\", inferSchema=true)\n",
    "            \n",
    "            attrDFWK = attrDFWK\n",
    "            .select(attribute, attrDFMS.columns.filter(_.startsWith(\"normalized\")): _*)\n",
    "            .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830303011\"))\n",
    "            \n",
    "            var cols = attrDFWK.columns\n",
    "            \n",
    "            var attrDF = attrDFMS.select(cols.head, cols.tail: _*).union(attrDFWK)\n",
    "            \n",
    "            data = data.join(attrDF, Seq(SIMILAR_GROUP_LEVEL, attribute)).drop(attribute)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "//     val color: DataFrame = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\n",
    "//     .csv(\"/data/ayush/AttributeLevel/DenseEmbeddings/MenShirts/Embeddings/color_embeddings.csv\").na.fill(\"\")\n",
    "//     .withColumn(\"magnitude\", sqrt(pow(col(\"color_1\"), 2) + pow(col(\"color_2\"), 2)))\n",
    "//     .withColumn(\"color_1\", col(\"color_1\")/col(\"magnitude\"))\n",
    "//     .withColumn(\"color_2\", col(\"color_2\")/col(\"magnitude\"))\n",
    "//     .drop(\"magnitude\")\n",
    "//     .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830216013\"))\n",
    "//     .select(\"colorfamily\", \"color_1\", \"color_2\", SIMILAR_GROUP_LEVEL)\n",
    "//     .union(\n",
    "//         spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\n",
    "//         .csv(\"/data/ayush/AttributeLevel/DenseEmbeddings/WomenKurtas/Embeddings/color_embeddings.csv\").na.fill(\"\")\n",
    "//         .withColumn(\"magnitude\", sqrt(pow(col(\"color_1\"), 2) + pow(col(\"color_2\"), 2)))\n",
    "//         .withColumn(\"color_1\", col(\"color_1\")/col(\"magnitude\"))\n",
    "//         .withColumn(\"color_2\", col(\"color_2\")/col(\"magnitude\"))\n",
    "//         .drop(\"magnitude\")\n",
    "//         .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830303011\"))\n",
    "//         .select(\"colorfamily\", \"color_1\", \"color_2\", SIMILAR_GROUP_LEVEL)\n",
    "//     )\n",
    "\n",
    "//     val brand: DataFrame = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\n",
    "//     .csv(\"/data/ayush/AttributeLevel/DenseEmbeddings/MenShirts/Embeddings/brand_embeddings.csv\").na.fill(\"\")\n",
    "//     .withColumn(\"magnitude\", sqrt(pow(col(\"brand_1\"), 2) + pow(col(\"brand_2\"), 2)))\n",
    "//     .withColumn(\"brand_1\", col(\"brand_1\")/col(\"magnitude\"))\n",
    "//     .withColumn(\"brand_2\", col(\"brand_2\")/col(\"magnitude\"))\n",
    "//     .drop(\"magnitude\")\n",
    "//     .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830216013\"))\n",
    "//     .select(\"brandname\", \"brand_1\", \"brand_2\", SIMILAR_GROUP_LEVEL)\n",
    "//     .union(\n",
    "//         spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\n",
    "//         .csv(\"/data/ayush/AttributeLevel/DenseEmbeddings/WomenKurtas/Embeddings/brand_embeddings.csv\").na.fill(\"\")\n",
    "//         .withColumn(\"magnitude\", sqrt(pow(col(\"brand_1\"), 2) + pow(col(\"brand_2\"), 2)))\n",
    "//         .withColumn(\"brand_1\", col(\"brand_1\")/col(\"magnitude\"))\n",
    "//         .withColumn(\"brand_2\", col(\"brand_2\")/col(\"magnitude\"))\n",
    "//         .drop(\"magnitude\")\n",
    "//         .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830303011\"))\n",
    "//         .select(\"brandname\", \"brand_1\", \"brand_2\", SIMILAR_GROUP_LEVEL)\n",
    "//     )\n",
    "    \n",
    "\n",
    "//     val pattern: DataFrame = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\n",
    "//     .csv(\"/data/ayush/AttributeLevel/DenseEmbeddings/MenShirts/Embeddings/pattern_embeddings.csv\").na.fill(\"\")\n",
    "//     .withColumn(\"magnitude\", sqrt(pow(col(\"pattern_1\"), 2) + pow(col(\"pattern_2\"), 2)))\n",
    "//     .withColumn(\"pattern_1\", col(\"pattern_1\")/col(\"magnitude\"))\n",
    "//     .withColumn(\"pattern_2\", col(\"pattern_2\")/col(\"magnitude\"))\n",
    "//     .drop(\"magnitude\")\n",
    "//     .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830216013\"))\n",
    "//     .select(\"pattern\", \"pattern_1\", \"pattern_2\", SIMILAR_GROUP_LEVEL)\n",
    "//     .union(\n",
    "//         spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\n",
    "//         .csv(\"/data/ayush/AttributeLevel/DenseEmbeddings/WomenKurtas/Embeddings/pattern_embeddings.csv\").na.fill(\"\")\n",
    "//         .withColumn(\"magnitude\", sqrt(pow(col(\"pattern_1\"), 2) + pow(col(\"pattern_2\"), 2)))\n",
    "//         .withColumn(\"pattern_1\", col(\"pattern_1\")/col(\"magnitude\"))\n",
    "//         .withColumn(\"pattern_2\", col(\"pattern_2\")/col(\"magnitude\"))\n",
    "//         .drop(\"magnitude\")\n",
    "//         .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830303011\"))\n",
    "//         .select(\"pattern\", \"pattern_1\", \"pattern_2\", SIMILAR_GROUP_LEVEL)\n",
    "//     )\n",
    "    \n",
    "//     val style: DataFrame = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\n",
    "//     .csv(\"/data/ayush/AttributeLevel/DenseEmbeddings/MenShirts/Embeddings/styletype_embeddings.csv\").na.fill(\"\")\n",
    "//     .withColumn(\"magnitude\", sqrt(pow(col(\"style_1\"), 2) + pow(col(\"style_2\"), 2)))\n",
    "//     .withColumn(\"style_1\", col(\"style_1\")/col(\"magnitude\"))\n",
    "//     .withColumn(\"style_2\", col(\"style_2\")/col(\"magnitude\"))\n",
    "//     .drop(\"magnitude\")\n",
    "//     .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830216013\"))\n",
    "//     .select(\"styletype\", \"style_1\", \"style_2\", SIMILAR_GROUP_LEVEL)\n",
    "//     .union(\n",
    "//         spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\n",
    "//         .csv(\"/data/ayush/AttributeLevel/DenseEmbeddings/WomenKurtas/Embeddings/styletype_embeddings.csv\").na.fill(\"\")\n",
    "//         .withColumn(\"magnitude\", sqrt(pow(col(\"style_1\"), 2) + pow(col(\"style_2\"), 2)))\n",
    "//         .withColumn(\"style_1\", col(\"style_1\")/col(\"magnitude\"))\n",
    "//         .withColumn(\"style_2\", col(\"style_2\")/col(\"magnitude\"))\n",
    "//         .drop(\"magnitude\")\n",
    "//         .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830303011\"))\n",
    "//         .select(\"styletype\", \"style_1\", \"style_2\", SIMILAR_GROUP_LEVEL)\n",
    "//     )\n",
    "    \n",
    "    // data = data.join(color, Seq(\"colorfamily\", SIMILAR_GROUP_LEVEL), \"inner\")\n",
    "    // .join(brand, Seq(\"brandname\", SIMILAR_GROUP_LEVEL), \"inner\")\n",
    "    // .join(pattern, Seq(\"pattern\", SIMILAR_GROUP_LEVEL), \"inner\")\n",
    "    // .join(style, Seq(\"styletype\", SIMILAR_GROUP_LEVEL), \"inner\")\n",
    "    // .drop(\"styletype\", \"pattern\", \"brandname\", \"colorfamily\")\n",
    "    \n",
    "    var productPriceBucketMap = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/productPriceBucketMap\")\n",
    "    \n",
    "    data = data.join(productPriceBucketMap, Seq(PRODUCTID))\n",
    "    \n",
    "    data.columns.filter(column => column.endsWith(\"monthSales\") || column.equals(\"yQuantity\")).foreach(\n",
    "        column => data = data.withColumn(column, col(column).cast(DoubleType))\n",
    "    )\n",
    "    \n",
    "    val menShirtsData: DataFrame = data.filter(col(SIMILAR_GROUP_LEVEL) === \"830216013\")\n",
    "    val womenKurtasData: DataFrame = data.filter(col(SIMILAR_GROUP_LEVEL) === \"830303011\")\n",
    "    \n",
    "    DFToParquet.putDF(s\"$transformedDatasetPath/combined/data/$of\", data)\n",
    "    DFToParquet.putDF(s\"$transformedDatasetPath/menShirts/data/$of\", menShirtsData)\n",
    "    DFToParquet.putDF(s\"$transformedDatasetPath/womenKurtas/data/$of\", womenKurtasData)\n",
    "    \n",
    "    if (performNormalization){\n",
    "        \n",
    "        var colsToScale: Array[String] = Array(\n",
    "            \"0_monthSales\", \"1_monthSales\", \"2_monthSales\", \"3_monthSales\", \"4_monthSales\", \"5_monthSales\", \"6_monthSales\", \"7_monthSales\", \n",
    "            \"8_monthSales\", \"9_monthSales\", \"10_monthSales\", \"11_monthSales\", \"avgSales\", \"stddevSales\", \"totalReturn\", \"totalUsers\", \n",
    "            \"totalPLPViews\", \"totalPLPClicks\", \"totalPDPCount\", \"totalATC\", \"totalWishList\", \"totalAvailableQuantity\")\n",
    "\n",
    "        if(of == \"train\"){\n",
    "\n",
    "            colsToScale = colsToScale ++ Array(\"yQuantity\")\n",
    "        }\n",
    "\n",
    "        DFToParquet.putDF(s\"$transformedDatasetPath/combined/normalizedData/global/$of\", data.transform(minMaxScaler(colsToScale)))\n",
    "        DFToParquet.putDF(s\"$transformedDatasetPath/menShirts/normalizedData/global/$of\", menShirtsData.transform(minMaxScaler(colsToScale)))\n",
    "        DFToParquet.putDF(s\"$transformedDatasetPath/womenKurtas/normalizedData/global/$of\", womenKurtasData.transform(minMaxScaler(colsToScale)))\n",
    "\n",
    "        // min max scale at similar group level, no meaning doing this for menshirtrs/womenkurtas as they are filtered out and are a single similargoruplevel themselves.\n",
    "        DFToParquet.putDF(\n",
    "            s\"$transformedDatasetPath/combined/normalizedData/$SIMILAR_GROUP_LEVEL/$of\", \n",
    "            ParquetToDF.getDF(s\"$transformedDatasetPath/menShirts/normalizedData/global/$of\").union(\n",
    "                ParquetToDF.getDF(s\"$transformedDatasetPath/womenKurtas/normalizedData/global/$of\")\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e14917-c25b-4806-85fc-ba5f257ab123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/01 19:15:51 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "originalDatasetPath: String = /data/Archive/bhavesh/inventoryPrediction/OriginalDataset/date_when_prediction_is_made=2023-04-16\n",
       "transformedDatasetPath: String = /data/Archive/bhavesh/inventoryPrediction/TransformedDataset/date_when_prediction_is_made=2023-04-16\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val originalDatasetPath: String = \"/data/Archive/bhavesh/inventoryPrediction/OriginalDataset/date_when_prediction_is_made=2023-04-16\"\n",
    "val transformedDatasetPath: String = \"/data/Archive/bhavesh/inventoryPrediction/TransformedDataset/date_when_prediction_is_made=2023-04-16\"\n",
    "generateTransformedTrainForProductLevelDataset(\"train\", originalDatasetPath, transformedDatasetPath)\n",
    "generateTransformedTrainForProductLevelDataset(\"test\", originalDatasetPath, transformedDatasetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed3b1b43-7c6d-479f-8f57-54486cee6611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- 0_monthSales: double (nullable = true)\n",
      " |-- 1_monthSales: double (nullable = true)\n",
      " |-- 2_monthSales: double (nullable = true)\n",
      " |-- 3_monthSales: double (nullable = true)\n",
      " |-- 4_monthSales: double (nullable = true)\n",
      " |-- 5_monthSales: double (nullable = true)\n",
      " |-- 6_monthSales: double (nullable = true)\n",
      " |-- 7_monthSales: double (nullable = true)\n",
      " |-- 8_monthSales: double (nullable = true)\n",
      " |-- 9_monthSales: double (nullable = true)\n",
      " |-- 10_monthSales: double (nullable = true)\n",
      " |-- 11_monthSales: double (nullable = true)\n",
      " |-- avgSales: double (nullable = true)\n",
      " |-- stddevSales: double (nullable = true)\n",
      " |-- totalReturn: double (nullable = true)\n",
      " |-- totalUsers: double (nullable = true)\n",
      " |-- totalPLPViews: double (nullable = true)\n",
      " |-- totalPLPClicks: double (nullable = true)\n",
      " |-- totalPDPCount: double (nullable = true)\n",
      " |-- totalATC: double (nullable = true)\n",
      " |-- totalWishList: double (nullable = true)\n",
      " |-- totalAvailableQuantity: double (nullable = true)\n",
      " |-- yQuantity: double (nullable = true)\n",
      " |-- normalized_color_0: double (nullable = true)\n",
      " |-- normalized_color_1: double (nullable = true)\n",
      " |-- normalized_pattern_0: double (nullable = true)\n",
      " |-- normalized_pattern_1: double (nullable = true)\n",
      " |-- normalized_brand_0: double (nullable = true)\n",
      " |-- normalized_brand_1: double (nullable = true)\n",
      " |-- normalized_style_0: double (nullable = true)\n",
      " |-- normalized_style_1: double (nullable = true)\n",
      " |-- normalized_sleeve_0: double (nullable = true)\n",
      " |-- normalized_sleeve_1: double (nullable = true)\n",
      " |-- pricebucket: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [productid: string, similargrouplevel: string ... 34 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/TransformedDataset/date_when_prediction_is_made=2023-04-16/menShirts/normalizedData/global/train\")\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29588022-2e8b-4ec6-93db-37c396f719ec",
   "metadata": {},
   "source": [
    "### Generate Query sigma map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3ccaf5-718b-4d34-ad56-888b8889fb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PREDICTED: String = predicted\n",
       "ACTUAL: String = actual\n",
       "rundate: String = 2023-04-15\n",
       "predictionend: String = 2023-05-15\n",
       "dateForWhichPredictionsMade: String = 2023-04-16\n",
       "trainendDate: String = 2023-04-14\n",
       "trainStartDate: String = 2022-03-13\n",
       "suffix: String = colorfamily_pricebucket_styletype_pattern_sleeve_brandname\n",
       "date: String = 2023-04-16\n",
       "queriesToProductMap: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [query: string, productid: string ... 2 more fields]\n",
       "baseDir: String = /data/Archive/bhavesh/inventoryPrediction\n",
       "getNumOfMonthsBetweenTwoDatesUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4921/1911919065@4f83421a,LongType,List(Some(class[value[0]: string]), Some(class[value[0]: string])),Some(class[value[0]: bigint]),None,false,tru...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var PREDICTED=\"predicted\"\n",
    "var ACTUAL=\"actual\"\n",
    "\n",
    "var rundate = \"2023-04-15\"\n",
    "// var predictionend = getFutureMonthDateFromHere(1,rundate,\"yyyy-MM-dd\")\n",
    "var predictionend = \"2023-05-15\"\n",
    "var dateForWhichPredictionsMade = getFutureDateFromHere(1,rundate,\"yyyy-MM-dd\")\n",
    "var trainendDate =getOldDateFromHere(1,rundate,\"yyyy-MM-dd\")\n",
    "var trainStartDate = \"2022-03-13\"\n",
    "var suffix = \"colorfamily_pricebucket_styletype_pattern_sleeve_brandname\"\n",
    "\n",
    "var date = getFutureDateFromHere(1, rundate, \"yyyy-MM-dd\")\n",
    "var queriesToProductMap = ParquetToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/queryToProductMap/date_when_prediction_is_made=$date/suffix=$suffix\").na.drop().distinct()\n",
    "var baseDir = \"/data/Archive/bhavesh/inventoryPrediction\"\n",
    "\n",
    "val getNumOfMonthsBetweenTwoDatesUDF = udf((startDate: String, endDate: String)=> {\n",
    "    getNumOfMonthsBetweenTwoDates(startDate, endDate, \"yyyy-MM-dd\", \"yyyy-MM-dd\")\n",
    "})\n",
    "\n",
    "val getOldMonthDateFromHereUDF = udf((numMonths: Int, date: String) => {\n",
    "    getOldMonthDateFromHere(numMonths, date, \"yyyy-MM-dd\", \"yyyy-MM-dd\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699daabb-e695-44f7-a6cb-3df0ad072d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "productLevelDailyQuantity: org.apache.spark.sql.DataFrame = [productid: string, date: date ... 1 more field]\n",
       "productLevelMonthlyQuantity: org.apache.spark.sql.DataFrame = [productid: string, pastmonth: bigint ... 2 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// var productLevelDailyQuantity = spark.read.parquet(\"/data/ecomm/ajio/processed/interactionsDB\").filter(col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" && col(\"productid\").isNotNull && col(\"booked_rev\") > 0 && col(\"userid\").isNotNull && col(\"quantity\") > 0 && col(\"date\")<=predictionend && col(\"date\")>=trainStartDate).groupBy(\"productid\", \"date\").agg(sum(\"quantity\").cast(DoubleType).as(\"quantity\"))\n",
    "// productLevelDailyQuantity.write.mode(\"overwrite\").parquet(s\"$baseDir/historyQuantitySold/date_when_prediction_is_made=${dateForWhichPredictionsMade}/productLevel/dayWise\")\n",
    "\n",
    "var productLevelDailyQuantity = spark.read.parquet(s\"$baseDir/historyQuantitySold/date_when_prediction_is_made=${dateForWhichPredictionsMade}/productLevel/dayWise\")\n",
    "\n",
    "var productLevelMonthlyQuantity = productLevelDailyQuantity.filter(col(\"date\")>=trainStartDate && col(\"date\")<=predictionend).filter(!(col(\"date\")===rundate))\n",
    ".withColumn(\"pastmonth\",when(col(\"date\")<=predictionend && col(\"date\")>= dateForWhichPredictionsMade,0).otherwise(getNumOfMonthsBetweenTwoDatesUDF(col(DATE),lit(trainendDate))+1))\n",
    ".groupBy(\"productid\",\"pastmonth\").agg(sum(\"quantity\").as(\"quantity\")).withColumn(\"tempDateForTimeSeries\", getOldMonthDateFromHereUDF(col(\"pastmonth\"),lit(rundate)))\n",
    "productLevelMonthlyQuantity.write.mode(\"overwrite\").parquet(s\"$baseDir/historyQuantitySold/date_when_prediction_is_made=${dateForWhichPredictionsMade}/productLevel/monthWise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "635c7cef-accc-4d92-b7cf-556ebec7ce2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "queryLevelDailyQuantity: org.apache.spark.sql.DataFrame = [similargrouplevel: int, query: string ... 2 more fields]\n",
       "queryLevelMonthlyQuantity: org.apache.spark.sql.DataFrame = [similargrouplevel: int, query: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// var queryLevelDailyQuantity = spark.read.parquet(s\"$baseDir/historyQuantitySold/date_when_prediction_is_made=${dateForWhichPredictionsMade}/productLevel/dayWise\")\n",
    "// .join(queriesToProductMap, Seq(PRODUCTID))\n",
    "// .groupBy(\"similargrouplevel\",\"query\", \"date\", \"template\").agg(sum(QUANTITY).as(QUANTITY))\n",
    "// queryLevelDailyQuantity.write.mode(\"overwrite\").parquet(s\"$baseDir/historyQuantitySold/date_when_prediction_is_made=${dateForWhichPredictionsMade}/queryLevel/suffix=$suffix/dayWise\")\n",
    "\n",
    "var queryLevelDailyQuantity = ParquetToDF.getDF(s\"$baseDir/historyQuantitySold/date_when_prediction_is_made=${dateForWhichPredictionsMade}/queryLevel/suffix=$suffix/dayWise\")\n",
    "\n",
    "var queryLevelMonthlyQuantity = spark.read.parquet(s\"$baseDir/historyQuantitySold/date_when_prediction_is_made=${dateForWhichPredictionsMade}/queryLevel/suffix=$suffix/dayWise\")\n",
    ".filter(col(\"date\")>=trainStartDate && col(\"date\")<=predictionend).filter(!(col(\"date\")===rundate))\n",
    ".withColumn(\"pastmonth\",when(col(\"date\")<=predictionend && col(\"date\")>= dateForWhichPredictionsMade,0).otherwise(getNumOfMonthsBetweenTwoDatesUDF(col(DATE),lit(trainendDate))+1))\n",
    ".groupBy(\"similargrouplevel\",\"query\",\"pastmonth\", \"template\").agg(sum(\"quantity\").as(\"quantity\")).withColumn(\"tempDateForTimeSeries\",getOldMonthDateFromHereUDF(col(\"pastmonth\"),lit(rundate)))\n",
    "queryLevelMonthlyQuantity.write.mode(\"overwrite\").parquet(s\"$baseDir/historyQuantitySold/date_when_prediction_is_made=${dateForWhichPredictionsMade}/queryLevel/suffix=$suffix/monthWise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecdf225c-2edd-4aa4-8637-33b99cc829cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- pastmonth: long (nullable = false)\n",
      " |-- quantity: double (nullable = true)\n",
      " |-- tempDateForTimeSeries: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- pastmonth: long (nullable = false)\n",
      " |-- quantity: double (nullable = true)\n",
      " |-- tempDateForTimeSeries: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productLevelDailyQuantity.printSchema\n",
    "productLevelMonthlyQuantity.printSchema\n",
    "queryLevelDailyQuantity.printSchema\n",
    "queryLevelMonthlyQuantity.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "055434d8-45ca-4868-bfae-f0ce93dc2d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "productLevelMonthlyQuantity: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productid: string, pastmonth: bigint ... 1 more field]\n",
       "statsDF: org.apache.spark.sql.DataFrame = [productid: string, stddev: double ... 1 more field]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var productLevelMonthlyQuantity = ParquetToDF.getDF(s\"$baseDir/historyQuantitySold/date_when_prediction_is_made=${dateForWhichPredictionsMade}/productLevel/monthWise\")\n",
    ".drop(\"tempDateForTimeSeries\").filter(col(\"pastmonth\")>=1 && col(\"pastmonth\")<=12)\n",
    "\n",
    "var statsDF = productLevelMonthlyQuantity.groupBy(PRODUCTID).agg(stddev(\"quantity\").as(\"stddev\"),avg(\"quantity\").as(\"avg\")).na.fill(0.0, Seq(\"stddev\"))\n",
    "statsDF.write.mode(\"overwrite\").parquet(s\"$baseDir/quantityStatsAcrossMonths/date_when_prediction_is_made=${dateForWhichPredictionsMade}/productLevel/past12MonthsStats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5429ba8-8a0f-4ac5-a314-2f51518c68ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "queryLevelMonthlyQuantity: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [similargrouplevel: int, query: string ... 2 more fields]\n",
       "statsDF: org.apache.spark.sql.DataFrame = [similargrouplevel: int, query: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var queryLevelMonthlyQuantity = ParquetToDF.getDF(s\"$baseDir/historyQuantitySold/date_when_prediction_is_made=${dateForWhichPredictionsMade}/queryLevel/suffix=$suffix/monthWise\")\n",
    ".drop(\"tempDateForTimeSeries\").filter(col(\"pastmonth\")>=1 && col(\"pastmonth\")<=12)\n",
    "var statsDF = queryLevelMonthlyQuantity.groupBy(\"similargrouplevel\",\"query\", \"template\").agg(stddev(\"quantity\").as(\"stddev\"),avg(\"quantity\").as(\"avg\"))\n",
    "statsDF.write.mode(\"overwrite\").parquet(s\"$baseDir/quantityStatsAcrossMonths/date_when_prediction_is_made=${dateForWhichPredictionsMade}/queryLevel/suffix=$suffix/past12MonthsStats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbadcc01-aa11-4060-a067-e5dd410a8905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- avg: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "statsDF: org.apache.spark.sql.DataFrame = [similargrouplevel: int, query: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var statsDF = ParquetToDF.getDF(s\"$baseDir/quantityStatsAcrossMonths/date_when_prediction_is_made=${dateForWhichPredictionsMade}/queryLevel/suffix=$suffix/past12MonthsStats\")\n",
    "statsDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a578ea-4edb-442d-b7aa-576fd922fa0a",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd8b1d-aa72-4e93-b680-bffb1a8525a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.time.format.DateTimeFormatter.ofPattern\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.LocalDate\n",
    "def getMonths: UserDefinedFunction = udf((startDate: String, endDate: String, inputDateFormat: String, outputDateFormat: String) => {\n",
    "\n",
    "val dateBefore: LocalDate = LocalDate.parse(startDate, ofPattern(inputDateFormat))\n",
    "\n",
    "val dateAfter: LocalDate = LocalDate.parse(endDate, ofPattern(outputDateFormat))\n",
    "\n",
    "ChronoUnit.MONTHS.between(dateBefore, dateAfter)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5bc1d-59a2-475d-ad7f-f7ce89c5b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val productAttrs = ParquetToDF.getDF(\"/data/Archive/inventory/productAttributesLegosFNL\")\n",
    ".select(col(PRODUCTID), col(SIMILAR_GROUP_LEVEL))\n",
    ".dropDuplicates(PRODUCTID)\n",
    ".filter(col(SIMILAR_GROUP_LEVEL).isin(\"830216013\", \"830303011\"))\n",
    ".persist()\n",
    "\n",
    "val dateMapX: Map[String, Map[String, String]] = Map(\"train\" -> Map(\"start\" -> \"2022-03-13\", \"end\" -> \"2023-03-13\"),\n",
    "                                                    \"test\" -> Map(\"start\" -> \"2022-04-14\", \"end\" -> \"2023-04-14\"))\n",
    "val dateMapY: Map[String, Map[String, String]] = Map(\"train\" -> Map(\"start\" -> \"2023-03-15\", \"end\" -> \"2023-04-14\"),\n",
    "                                                    \"test\" -> Map(\"start\" -> \"2023-04-16\", \"end\" -> \"2023-05-15\"))\n",
    "\n",
    "val dateForWhichPredictionsAreMade: String = dateMapY(\"test\")(\"start\")\n",
    "val baseDir = s\"/data/Archive/bhavesh/inventoryPrediction/dataset/date_when_prediction_is_made=$dateForWhichPredictionsAreMade\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e6c5cd-4134-4f61-84c9-6fb0177f12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveNLoadDF(path: String)(df: DataFrame): DataFrame = {\n",
    "    DFToParquet.putDF(path, df)\n",
    "    ParquetToDF.getDF(path)\n",
    "}\n",
    "\n",
    "def getMonthlyMeanStd(monthsLimit: Int, colsForStatistics: Array[String], calculateStd: Boolean)(df: DataFrame): DataFrame = {\n",
    "    var intermediateDF = df\n",
    "    val monthsArray = (0 until monthsLimit).toArray\n",
    "    val colsToUseForCalculations = colsForStatistics.map(column => (column, monthsArray.map(month => s\"${month}_${column}\"))).toMap\n",
    "    colsToUseForCalculations.foreach(\n",
    "        columnInfo => {\n",
    "            val attr = columnInfo._1\n",
    "            val colsToUse = columnInfo._2\n",
    "            val meanExpr = colsToUse.map(col).reduce((col1, col2) => col1 + col2)\n",
    "            intermediateDF = intermediateDF.withColumn(s\"${attr}_avg_${monthsLimit}_months\", meanExpr/lit(colsToUse.length).cast(DoubleType))\n",
    "            \n",
    "            if(calculateStd){\n",
    "                print(attr)\n",
    "                val stdExpr = colsToUse.map(column => pow(col(column) - col(s\"${attr}_avg_${monthsLimit}_months\"), lit(2)))\n",
    "                .reduce((col1, col2) => col1 + col2)\n",
    "                intermediateDF = intermediateDF.withColumn(s\"${attr}_std_${monthsLimit}_months\",\n",
    "                                                           sqrt(stdExpr/lit(colsToUse.length - 1)))\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    intermediateDF\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17900c4-3316-46cc-8a89-be8f9876bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSalesXData(of: String, dateMap: Map[String, Map[String, String]], productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val startDate = dateMap(of)(\"start\")\n",
    "    val endDate = dateMap(of)(\"end\")\n",
    "    val features = Array(\"sales\")\n",
    "    val tempDir = \"/data/Archive/bhavesh/inventoryPrediction/temp/generateSalesXData\"\n",
    "    \n",
    "    var interactions = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    .filter(col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "            && col(\"productid\").isNotNull && col(\"booked_rev\") > 0 \n",
    "            && col(\"userid\").isNotNull && col(\"quantity\") > 0\n",
    "            && col(DATE).between(startDate, endDate)\n",
    "           )\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(sum(QUANTITY) as \"sales\")\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/interactions_1\"))\n",
    "    .withColumn(\"months\", getMonths(col(DATE), lit(endDate), lit(\"yyyy-MM-dd\"), lit(\"yyyy-MM-dd\")))\n",
    "    .filter(col(\"months\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"months\")\n",
    "    .agg(sum(\"sales\").cast(DoubleType) as \"sales\", first(\"sales\") as \"temp\")\n",
    "    \n",
    "    val colsToDrop = interactions.columns.filter(_.endsWith(\"temp\"))\n",
    "    interactions = interactions.drop(colsToDrop: _*).na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/interactions_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, Array(\"sales\"), false))\n",
    "    .transform(getMonthlyMeanStd(6, Array(\"sales\"), false))\n",
    "    .transform(getMonthlyMeanStd(12, Array(\"sales\"), true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/sales\", interactions)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "\n",
    "\n",
    "def generateGAXData(of: String, dateMap: Map[String, Map[String, String]], productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val startDate = dateMap(of)(\"start\")\n",
    "    val endDate = dateMap(of)(\"end\")\n",
    "    val features = Array(\"PLPViewsPerDay\", \"PLPClicksPerDay\", \"PDPCountPerDay\", \"TotalAddToCartPerDay\")\n",
    "    val aggExprs = features.map(feature => sum(feature).cast(DoubleType).alias(feature))\n",
    "    val tempDir = \"/data/Archive/bhavesh/inventoryPrediction/temp/generateGAXData\"\n",
    "    \n",
    "    val ga = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/ProcessedGAData\")\n",
    "    .filter(col(DATE).between(startDate, endDate))\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(aggExprs.head, aggExprs.tail: _*)\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/ga_1\"))\n",
    "    .withColumn(\"months\", getMonths(col(DATE), lit(endDate), lit(\"yyyy-MM-dd\"), lit(\"yyyy-MM-dd\")))\n",
    "    .filter(col(\"months\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"months\")\n",
    "    .agg(aggExprs.head, aggExprs.tail: _*)\n",
    "    .na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/ga_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, features, false))\n",
    "    .transform(getMonthlyMeanStd(6, features, false))\n",
    "    .transform(getMonthlyMeanStd(12, features, true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/ga\", ga)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "\n",
    "def generateWishlistData(of: String, dateMap: Map[String, Map[String, String]], productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val startDate = dateMap(of)(\"start\")\n",
    "    val endDate = dateMap(of)(\"end\")\n",
    "    val features = Array(\"wishlist\")\n",
    "    val tempDir = \"/data/Archive/bhavesh/inventoryPrediction/temp/generateWishlistData\"\n",
    "    \n",
    "    var wishlist = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/processedWishlist\")\n",
    "    .filter(col(DATE).between(startDate, endDate))\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(countDistinct(\"wishlistid\").cast(DoubleType) as \"wishlist\")\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/wishlist_1\"))\n",
    "    .withColumn(\"months\", getMonths(col(DATE), lit(endDate), lit(\"yyyy-MM-dd\"), lit(\"yyyy-MM-dd\")))\n",
    "    .filter(col(\"months\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"months\")\n",
    "    .agg(sum(\"wishlist\") as \"wishlist\", first(\"wishlist\") as \"temp\")\n",
    "    \n",
    "    val colsToDrop = wishlist.columns.filter(_.endsWith(\"temp\"))\n",
    "    wishlist = wishlist.drop(colsToDrop: _*).na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/wishlist_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, features, false))\n",
    "    .transform(getMonthlyMeanStd(6, features, false))\n",
    "    // .transform(getMonthlyMeanStd(12, features, true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/wishlist\", wishlist)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "\n",
    "def generateReturnsData(of: String, dateMap: Map[String, Map[String, String]], productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val startDate = dateMap(of)(\"start\")\n",
    "    val endDate = dateMap(of)(\"end\")\n",
    "    val features = Array(\"returns\")\n",
    "    val tempDir = \"/data/Archive/bhavesh/inventoryPrediction/temp/generateReturns\"\n",
    "    \n",
    "    var returns = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    .filter(col(\"event\") === \"Return\" && col(QUANTITY) >= 0 &&\n",
    "        col(USERID).isNotNull && col(PRODUCTID).isNotNull)\n",
    "    .filter(col(DATE).between(startDate, endDate))\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(sum(QUANTITY).cast(DoubleType) as \"returns\")\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/returns_1\"))\n",
    "    .withColumn(\"months\", getMonths(col(DATE), lit(endDate), lit(\"yyyy-MM-dd\"), lit(\"yyyy-MM-dd\")))\n",
    "    .filter(col(\"months\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"months\")\n",
    "    .agg(sum(\"returns\") as \"returns\", first(\"returns\") as \"temp\")\n",
    "    \n",
    "    val colsToDrop = returns.columns.filter(_.endsWith(\"temp\"))\n",
    "    returns = returns.drop(colsToDrop: _*).na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/returns_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, features, false))\n",
    "    .transform(getMonthlyMeanStd(6, features, false))\n",
    "    .transform(getMonthlyMeanStd(12, features, true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/returns\", returns)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "\n",
    "def generateUsersXData(of: String, dateMap: Map[String, Map[String, String]], productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val startDate = dateMap(of)(\"start\")\n",
    "    val endDate = dateMap(of)(\"end\")\n",
    "    val features = Array(\"users\")\n",
    "    val tempDir = \"/data/Archive/bhavesh/inventoryPrediction/temp/generateUsersXData\"\n",
    "    \n",
    "    var interactions = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    .filter(col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "            && col(\"productid\").isNotNull && col(\"booked_rev\") > 0 \n",
    "            && col(\"userid\").isNotNull && col(\"quantity\") > 0\n",
    "            && col(DATE).between(startDate, endDate)\n",
    "           )\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/interactions_1\"))\n",
    "    .withColumn(\"months\", getMonths(col(DATE), lit(endDate), lit(\"yyyy-MM-dd\"), lit(\"yyyy-MM-dd\")))\n",
    "    .filter(col(\"months\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"months\")\n",
    "    .agg(countDistinct(USERID).cast(DoubleType) as \"users\", first(USERID) as \"temp\")\n",
    "    \n",
    "    val colsToDrop = interactions.columns.filter(_.endsWith(\"temp\"))\n",
    "    interactions = interactions.drop(colsToDrop: _*).na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/interactions_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, features, false))\n",
    "    .transform(getMonthlyMeanStd(6, features, false))\n",
    "    .transform(getMonthlyMeanStd(12, features, true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/users\", interactions)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "\n",
    "def generateAvailabilityQuantityXData(of: String, dateMap: Map[String, Map[String, String]], productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val startDate = dateMap(of)(\"start\")\n",
    "    val endDate = dateMap(of)(\"end\")\n",
    "    val features = Array(\"availableQuantity\")\n",
    "    val tempDir = \"/data/Archive/bhavesh/inventoryPrediction/temp/generateAvailabilityQuantityXData\"\n",
    "    \n",
    "    var inventory = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/LiveProductsInventoryHistoryLegos\")\n",
    "    .withColumnRenamed(ITEM_ID, PRODUCTID)\n",
    "    .filter(col(DATE).between(startDate, endDate))\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(sum(\"availablequantity\").as(\"availableQuantity\"))\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/inventory_1\"))\n",
    "    .withColumn(\"months\", getMonths(col(DATE), lit(endDate), lit(\"yyyy-MM-dd\"), lit(\"yyyy-MM-dd\")))\n",
    "    .filter(col(\"months\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"months\")\n",
    "    .agg(sum(\"availableQuantity\").cast(DoubleType) as \"availableQuantity\", first(\"availableQuantity\") as \"temp\")\n",
    "    \n",
    "    val colsToDrop = inventory.columns.filter(_.endsWith(\"temp\"))\n",
    "    inventory = inventory.drop(colsToDrop: _*).na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/inventory_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, features, false))\n",
    "    .transform(getMonthlyMeanStd(6, features, false))\n",
    "    .transform(getMonthlyMeanStd(12, features, true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/availableQuantity\", inventory)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "\n",
    "generateSalesXData(\"train\", dateMapX, productAttrs, baseDir)\n",
    "generateSalesXData(\"test\", dateMapX, productAttrs, baseDir)\n",
    "generateGAXData(\"train\", dateMapX, productAttrs, baseDir)\n",
    "generateGAXData(\"test\", dateMapX, productAttrs, baseDir)\n",
    "generateWishlistData(\"train\", dateMapX, productAttrs, baseDir)\n",
    "generateWishlistData(\"test\", dateMapX, productAttrs, baseDir)\n",
    "generateReturnsData(\"train\", dateMapX, productAttrs, baseDir)\n",
    "generateReturnsData(\"test\", dateMapX, productAttrs, baseDir)\n",
    "generateUsersXData(\"train\", dateMapX, productAttrs, baseDir)\n",
    "generateUsersXData(\"test\", dateMapX, productAttrs, baseDir)\n",
    "generateAvailabilityQuantityXData(\"train\", dateMapX, productAttrs, baseDir)\n",
    "generateAvailabilityQuantityXData(\"test\", dateMapX, productAttrs, baseDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1db62f-2bb6-4000-9923-7c1e11db9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTarget(of: String, dateMap: Map[String, Map[String, String]], productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val startDate = dateMap(of)(\"start\")\n",
    "    val endDate = dateMap(of)(\"end\")\n",
    "    \n",
    "    var interactions = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\").filter(col(DATE).between(startDate, endDate))\n",
    "    .filter(col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" && col(\"productid\").isNotNull && col(\"booked_rev\") > 0 && col(\"userid\").isNotNull && col(\"quantity\") > 0)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .agg(sum(QUANTITY) as \"sales\")\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/target\", interactions)\n",
    "}\n",
    "computeTarget(\"train\", dateMapY, productAttrs, baseDir)\n",
    "computeTarget(\"test\", dateMapY, productAttrs, baseDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbbe9f0-e580-4ab1-8fa7-558c2669e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(of: String, xPath: String, yPath: String, basePath: String): Unit = {\n",
    "    val tempDir = \"/data/Archive/bhavesh/inventoryPrediction/temp/createDataset\"\n",
    "    var xData = ParquetToDF.getDF(s\"$xPath/$of/sales\").drop(SIMILAR_GROUP_LEVEL)\n",
    "    .join(ParquetToDF.getDF(s\"$xPath/$of/ga\").drop(SIMILAR_GROUP_LEVEL), Seq(PRODUCTID), \"outer\")\n",
    "    .transform(saveNLoadDF(s\"$tempDir/dataset_1\"))\n",
    "    .join(ParquetToDF.getDF(s\"$xPath/$of/wishlist\").drop(SIMILAR_GROUP_LEVEL), Seq(PRODUCTID), \"outer\")\n",
    "    .join(ParquetToDF.getDF(s\"$xPath/$of/returns\").drop(SIMILAR_GROUP_LEVEL), Seq(PRODUCTID), \"outer\")\n",
    "    .transform(saveNLoadDF(s\"$tempDir/dataset_2\"))\n",
    "    .join(ParquetToDF.getDF(s\"$xPath/$of/users\").drop(SIMILAR_GROUP_LEVEL), Seq(PRODUCTID), \"outer\")\n",
    "    .join(ParquetToDF.getDF(s\"$xPath/$of/availableQuantity\").drop(SIMILAR_GROUP_LEVEL), Seq(PRODUCTID), \"outer\")\n",
    "    .na.fill(0)\n",
    "    \n",
    "    DFToParquet.putDF(s\"$basePath/$of/XData\", xData)\n",
    "    var yData = ParquetToDF.getDF(s\"$yPath/$of/target\").drop(SIMILAR_GROUP_LEVEL)\n",
    "    DFToParquet.putDF(s\"$basePath/$of/YData\", yData)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "CreateDataset(\"train\", baseDir, baseDir, s\"/data/Archive/bhavesh/inventoryPrediction/OriginalDataset/date_when_prediction_is_made=$dateForWhichPredictionsAreMade\")\n",
    "CreateDataset(\"test\", baseDir, baseDir, s\"/data/Archive/bhavesh/inventoryPrediction/OriginalDataset/date_when_prediction_is_made=$dateForWhichPredictionsAreMade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70345ab-bb55-4d14-a3de-38eefa2eadfb",
   "metadata": {},
   "source": [
    "### Convert local predictions to required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0505fe-88f7-48a9-bed7-5f325362036d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thresholdColumns: (columns: Array[String], minThreshold: Double, maxThreshold: Double)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n",
       "suffix: String = colorfamily_pricebucket_styletype_pattern_sleeve_brandname\n",
       "prodLevelSigma: org.apache.spark.sql.DataFrame = [productid: string, stddev: double]\n",
       "queryLevelSigma: org.apache.spark.sql.DataFrame = [query: string, stddev: double ... 1 more field]\n",
       "productQueryMap: org.apache.spark.sql.DataFrame = [query: string, productid: string ... 2 more fields]\n",
       "convertLocalToRequiredFormat: (dateForWhichPredictionsAreMade: String, basePath: String, level: String, localFile: String, fileType: String)Unit\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def thresholdColumns(columns: Array[String], minThreshold: Double = 0, maxThreshold: Double = Double.PositiveInfinity)(df: DataFrame): DataFrame = {\n",
    "\n",
    "    var scaledDF = df\n",
    "    columns.foreach(\n",
    "    column => {\n",
    "    scaledDF = scaledDF.withColumn(column,\n",
    "      when(col(column) > maxThreshold, lit(maxThreshold)).otherwise(col(column)))\n",
    "      .withColumn(column, when(col(column) < minThreshold, lit(minThreshold)).otherwise(col(column)))\n",
    "    })\n",
    "    scaledDF\n",
    "}\n",
    "\n",
    "var suffix: String = \"colorfamily_pricebucket_styletype_pattern_sleeve_brandname\"\n",
    "var prodLevelSigma = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/quantityStatsAcrossMonths/date_when_prediction_is_made=2023-04-16/productLevel/past12MonthsStats\").select(PRODUCTID, \"stddev\")\n",
    "var queryLevelSigma = ParquetToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/quantityStatsAcrossMonths/date_when_prediction_is_made=2023-04-16/queryLevel/suffix=$suffix/past12MonthsStats\").select(\"query\", \"stddev\", SIMILAR_GROUP_LEVEL, \"template\")\n",
    "var productQueryMap = ParquetToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/queryToProductMap/date_when_prediction_is_made=2023-04-16/suffix=$suffix\")\n",
    "\n",
    "def convertLocalToRequiredFormat(dateForWhichPredictionsAreMade: String, basePath: String, level: String, localFile: String, fileType: String = \"csv\"): Unit = {\n",
    "    \n",
    "    var actual = ParquetToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/actualDataForPredictionPeriod/date=$dateForWhichPredictionsAreMade/sales\")\n",
    "    var fileName: String = if(level == \"product\"){\n",
    "       \"productLevelPredictions\"\n",
    "    } else{\n",
    "        \"queryAggregatedPredictions\"\n",
    "    }\n",
    "    def doConversion(brick: String, brickId: String): DataFrame = {\n",
    "        var localPredictionsPath = s\"$basePath/$brick/$localFile\"\n",
    "        var df = if(fileType==\"csv\"){\n",
    "            CSVToDF.getDF(localPredictionsPath, inferSchema = true)\n",
    "        }\n",
    "        else{\n",
    "            ParquetToDF.getDF(localPredictionsPath)\n",
    "        }\n",
    "\n",
    "        \n",
    "        df = df\n",
    "        .withColumn(SIMILAR_GROUP_LEVEL, lit(brickId))\n",
    "        .withColumn(\"model\", lit(\"lr\"))\n",
    "        .withColumn(\"predicted\", col(\"predictedyQuantity\"))\n",
    "        .select(PRODUCTID, SIMILAR_GROUP_LEVEL, \"predicted\", \"model\")\n",
    "        \n",
    "        if(level == \"product\") {\n",
    "            df.transform(thresholdColumns(Array(\"predicted\")))\n",
    "            .join(prodLevelSigma, Seq(PRODUCTID))\n",
    "            .withColumn(\"lowerBound\", col(\"predicted\") - (lit(2.0) * col(\"stddev\")))\n",
    "            .withColumn(\"upperBound\", col(\"predicted\") + (lit(2.0) * col(\"stddev\")))\n",
    "            .transform(thresholdColumns(Array(\"lowerBound\")))\n",
    "        }\n",
    "        else {\n",
    "            df.transform(thresholdColumns(Array(\"predicted\")))\n",
    "            .join(productQueryMap, Seq(SIMILAR_GROUP_LEVEL, PRODUCTID))\n",
    "            .groupBy(\"model\", \"query\", SIMILAR_GROUP_LEVEL, \"template\")\n",
    "            .agg(sum(\"predicted\") as \"predicted\")\n",
    "            .join(queryLevelSigma, Seq(SIMILAR_GROUP_LEVEL, \"query\", \"template\"))\n",
    "            .withColumn(\"lowerBound\", col(\"predicted\") - (lit(2.0) * col(\"stddev\")))\n",
    "            .withColumn(\"upperBound\", col(\"predicted\") + (lit(2.0) * col(\"stddev\")))\n",
    "            .transform(thresholdColumns(Array(\"lowerBound\")))\n",
    "        }\n",
    "    }\n",
    "    var menShirts = doConversion(\"menShirts\", \"830216013\")\n",
    "    DFToParquet.putDF(s\"$basePath/menShirts/$fileName\", menShirts, partitionedColumn=\"model\")\n",
    "    var womenKurtas = doConversion(\"womenKurtas\", \"830303011\")\n",
    "    DFToParquet.putDF(s\"$basePath/womenKurtas/$fileName\", womenKurtas, partitionedColumn=\"model\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf65b1e-0043-4ccf-9c10-3f679d115bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convertLocal: (exp: String, date: String, filename: String, format: String)Unit\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convertLocal(exp: String, date: String, filename: String, format: String){\n",
    "    convertLocalToRequiredFormat(date, s\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_$exp/predictions/ModelForEachBrickProductLevel\", \"product\", filename, format)\n",
    "    convertLocalToRequiredFormat(date, s\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_$exp/predictions/ModelForEachBrickProductLevel\", \"query\", filename, format)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a78667-5243-4892-850a-79bb29b29892",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertLocal(\n",
    "    exp = \"pca\",\n",
    "    date = \"2023-04-16\",\n",
    "    filename = \"predictions_pca.csv\",\n",
    "    format = \"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1a1501-e16e-4f65-adbb-57fd204590ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertLocal(\n",
    "    exp = \"trials\",\n",
    "    date = \"2023-04-16\",\n",
    "    filename = \"predictions_4_months.csv\",\n",
    "    format = \"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfbd306-7045-4a00-bc1b-df8a906cec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertLocal(\n",
    "    exp = \"neural_network_new\",\n",
    "    date = \"2023-04-16\",\n",
    "    filename = \"predictions\",\n",
    "    format = \"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6836b7dd-b47f-4d89-85c7-123dc0301941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metricsShow: (exp: String)Unit\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def metricsShow(exp: String): Unit = {\n",
    "    var pl = CSVToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_${exp}/metrics/productLevelModelForEachBrick/productLevel\", inferSchema=true).orderBy(\"model\", \"similargrouplevel\").drop(DATE).persist()\n",
    "    pl.printSchema\n",
    "\n",
    "    var show = pl.orderBy(\"metric\").groupBy(\"model\", \"metric\").pivot(\"similargrouplevel\").agg(first(\"value\")).orderBy(\"model\", \"metric\").withColumn(\"final\", concat(lit(\"Men Shirts: \"), col(\"830216013\"), lit(\"\\n\"), lit(\"Women Kurtas: \"), col(\"830303011\"))).groupBy(\"model\").pivot(\"metric\").agg(first(\"final\") as \"final\").persist()\n",
    "    show.orderBy(\"model\").show(false)\n",
    "    // show.orderBy(\"model\").drop(\"model\").show(false)\n",
    "    DFToCSV.putDF(\"/data/Archive/plp/bhavesh/temp/ip/pl_show\", show.orderBy(\"model\").coalesce(1))\n",
    "    pl.unpersist()\n",
    "\n",
    "    var ql = CSVToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_${exp}/metrics/productLevelModelForEachBrick/queryLevel\", inferSchema=true).orderBy(\"model\", \"similargrouplevel\").drop(DATE).persist()\n",
    "    ql.printSchema\n",
    "\n",
    "    show = ql.orderBy(\"metric\").groupBy(\"model\", \"metric\").pivot(\"similargrouplevel\").agg(first(\"value\")).orderBy(\"model\", \"metric\").withColumn(\"final\", concat(lit(\"Men Shirts: \"), col(\"830216013\"), lit(\"\\n\"), lit(\"Women Kurtas: \"), col(\"830303011\"))).groupBy(\"model\").pivot(\"metric\").agg(first(\"final\") as \"final\").persist()\n",
    "    show.orderBy(\"model\").show(false)\n",
    "    // show.orderBy(\"model\").drop(\"model\").show(false)\n",
    "    DFToCSV.putDF(\"/data/Archive/plp/bhavesh/temp/ip/ql_show\", show.orderBy(\"model\").coalesce(1))\n",
    "    ql.unpersist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962d9a1a-c753-4e01-be35-ffdb35aae96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- metric: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      "\n",
      "+-----+----------------------------------------------------------------+----------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------+--------------------------------------------------------------+---------------------------------------------------------------+\n",
      "|model|Bias                                                            |Hit Rate                                                        |Mean Absolute Percent Error                                    |Root Mean Square Error for 100.0 percentile error              |Root Mean Square Error for 25.0 percentile error                |Root Mean Square Error for 50.0 percentile error                |Root Mean Square Error for 75.0 percentile error              |Root Mean Square Error for 90.0 percentile error               |\n",
      "+-----+----------------------------------------------------------------+----------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------+--------------------------------------------------------------+---------------------------------------------------------------+\n",
      "|lr   |Men Shirts: -56262.01243371866\\nWomen Kurtas: -87228.16135549545|Men Shirts: 0.6898819660579012\\nWomen Kurtas: 0.7408379556259905|Men Shirts: 155.8215933530937\\nWomen Kurtas: 129.58692417247042|Men Shirts: 108.1025213624223\\nWomen Kurtas: 107.63389819189032|Men Shirts: 0.5866703076811367\\nWomen Kurtas: 0.4523400044563955|Men Shirts: 1.3153231068834579\\nWomen Kurtas: 1.1990977666615508|Men Shirts: 3.538093745660226\\nWomen Kurtas: 3.719579619909912|Men Shirts: 9.100248033899181\\nWomen Kurtas: 14.398889422282299|\n",
      "+-----+----------------------------------------------------------------+----------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------+--------------------------------------------------------------+---------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- metric: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      "\n",
      "+-----+----------------------------------------------------------------+----------------------------------------------------------------+--------------------------------------------------------------+--------------------------------------------------------------+----------------------------------------------------------------+---------------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------------------+\n",
      "|model|Bias                                                            |Hit Rate                                                        |Mean Absolute Percent Error                                   |Root Mean Square Error for 100.0 percentile error             |Root Mean Square Error for 25.0 percentile error                |Root Mean Square Error for 50.0 percentile error               |Root Mean Square Error for 75.0 percentile error                |Root Mean Square Error for 90.0 percentile error             |\n",
      "+-----+----------------------------------------------------------------+----------------------------------------------------------------+--------------------------------------------------------------+--------------------------------------------------------------+----------------------------------------------------------------+---------------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------------------+\n",
      "|lr   |Men Shirts: 7511446.1689715795\\nWomen Kurtas: -641394.1936058253|Men Shirts: 0.5921913556363761\\nWomen Kurtas: 0.7102184704999415|Men Shirts: 369.7671700165709\\nWomen Kurtas: 274.1599225668784|Men Shirts: 973.2700138513527\\nWomen Kurtas: 930.9876863888153|Men Shirts: 1.6093671098180231\\nWomen Kurtas: 1.2155614614641566|Men Shirts: 4.750991107437573\\nWomen Kurtas: 3.9991737072934272|Men Shirts: 13.563394973743582\\nWomen Kurtas: 14.682017382862364|Men Shirts: 34.20173440273035\\nWomen Kurtas: 43.7065915739133|\n",
      "+-----+----------------------------------------------------------------+----------------------------------------------------------------+--------------------------------------------------------------+--------------------------------------------------------------+----------------------------------------------------------------+---------------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metricsShow(\"neural_network_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd0b63-8ce0-410f-b682-437ef53a8372",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca5d3f2-8988-4ea2-961d-8e32ce420f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "originalDatasetPath: String = /data/Archive/bhavesh/inventoryPrediction/OriginalDataset/date_when_prediction_is_made=2023-04-16\n",
       "transformedDatasetPath: String = /data/Archive/bhavesh/inventoryPrediction/temp/local\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val originalDatasetPath: String = \"/data/Archive/bhavesh/inventoryPrediction/OriginalDataset/date_when_prediction_is_made=2023-04-16\"\n",
    "val transformedDatasetPath: String = \"/data/Archive/bhavesh/inventoryPrediction/temp/local\"\n",
    "generateTransformedTrainForProductLevelDataset(\"train\", originalDatasetPath, transformedDatasetPath, performNormalization=false)\n",
    "generateTransformedTrainForProductLevelDataset(\"test\", originalDatasetPath, transformedDatasetPath, performNormalization=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71d58054-bc4e-45a9-a67d-d62e0f2e15bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [productid: string, yQuantity: bigint ... 6 more fields]\n",
       "productAttrs: org.apache.spark.sql.DataFrame = [productid: string, similargrouplevel: string ... 5 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [productid: string, yQuantity: bigint ... 6 more fields]\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/OriginalDataset/date_when_prediction_is_made=2023-04-16/train/YData\")\n",
    "var productAttrs = ParquetToDF.getDF(\"/data/Archive/inventory/productAttributesLegosFNL\").select(PRODUCTID, SIMILAR_GROUP_LEVEL, \"colorfamily\", \"brandname\", \"sleeve\", \"pattern\", \"styletype\")\n",
    "df = df.join(productAttrs, Seq(PRODUCTID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19e47c82-a3ce-4173-8f90-5600401604b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- yQuantity: long (nullable = true)\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- colorfamily: string (nullable = true)\n",
      " |-- brandname: string (nullable = true)\n",
      " |-- sleeve: string (nullable = true)\n",
      " |-- pattern: string (nullable = true)\n",
      " |-- styletype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "48c38f1b-11dc-4bf6-82a6-c885114ad09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res65: Long = 58731\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7150551f-1f18-4c09-bd6e-ada36871b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFToParquet.putDF(\"/data/Archive/bhavesh/inventoryPrediction/embeddings/data\", df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
