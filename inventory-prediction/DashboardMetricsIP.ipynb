{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'Bhavesh_Inventory_prediction', 'spark.yarn.queue': 'default', 'spark.jars': '/apps/Jars/obelisk-retail-legos.jar,/apps/Jars/mysql-connector-java-5.1.40.jar', 'spark.driver.extraJavaOptions': '-Dscala.color', 'spark.driver.memory': '3g', 'spark.executor.memory': '3g', 'spark.executor.instances': '4'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{\n",
    "    \"conf\":\n",
    "    {\n",
    "        \"spark.app.name\":\"Bhavesh_Inventory_prediction\",\n",
    "        \"spark.yarn.queue\": \"default\",\n",
    "        \"spark.jars\": \"/apps/Jars/obelisk-retail-legos.jar,/apps/Jars/mysql-connector-java-5.1.40.jar\",\n",
    "        \"spark.driver.extraJavaOptions\": \"-Dscala.color\",\n",
    "        \"spark.driver.memory\": \"3g\",\n",
    "        \"spark.executor.memory\": \"3g\",\n",
    "        \"spark.executor.instances\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1359</td><td>application_1685341929318_1985</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://JMNGD1BAF370C10:8088/proxy/application_1685341929318_1985/\">Link</a></td><td><a target=\"_blank\" href=\"http://JMNGD1BAE210V09:8042/node/containerlogs/container_e112_1685341929318_1985_01_000001/couture\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import ai.couture.obelisk.commons.Constants._\n",
      "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
      "import ai.couture.obelisk.commons.Constants._\n",
      "import ai.couture.obelisk.commons.io._\n",
      "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
      "import org.apache.spark.sql._\n",
      "import org.apache.spark.sql.expressions._\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.types._\n",
      "import spark.implicits._\n"
     ]
    }
   ],
   "source": [
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.io._\n",
    "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.text.SimpleDateFormat\n",
      "import java.time.DayOfWeek.{MONDAY, SUNDAY}\n",
      "import java.time.format.DateTimeFormatter\n",
      "import java.time.format.DateTimeFormatter.ofPattern\n",
      "import java.time.temporal.TemporalAdjusters.previousOrSame\n",
      "import java.time.temporal.{ChronoUnit, WeekFields}\n",
      "import java.time.{LocalDate, LocalDateTime}\n",
      "import java.util.{Date, Locale, TimeZone}\n",
      "\u001b[1m\u001b[34mgetFutureMonthDateFromHere\u001b[0m: \u001b[1m\u001b[32m(numMonths: Int, date: String, inputDateFormat: String, outputDateFormat: String)String\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import java.text.SimpleDateFormat\n",
    "import java.time.DayOfWeek.{MONDAY, SUNDAY}\n",
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.format.DateTimeFormatter.ofPattern\n",
    "import java.time.temporal.TemporalAdjusters.previousOrSame\n",
    "import java.time.temporal.{ChronoUnit, WeekFields}\n",
    "import java.time.{LocalDate, LocalDateTime}\n",
    "import java.util.{Date, Locale, TimeZone}  \n",
    "def getFutureMonthDateFromHere(numMonths: Int, date: String, inputDateFormat: String,\n",
    "                                 outputDateFormat: String = \"yyyy-MM-dd\"): String = {\n",
    "    LocalDate.parse(date, ofPattern(inputDateFormat)).plusMonths(numMonths).format(ofPattern(outputDateFormat))\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
      "import java.time.format.DateTimeFormatter.ofPattern\n",
      "import java.time.temporal.ChronoUnit\n",
      "import java.time.LocalDate\n",
      "import org.apache.spark.sql.types._\n",
      "\u001b[1m\u001b[34mgetNumOfMonthsBetweenTwoDates\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = UserDefinedFunction(<function2>,LongType,Some(List(StringType, StringType)))\n",
      "\u001b[1m\u001b[34mgetOldMonthDateFromHere\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = UserDefinedFunction(<function2>,StringType,Some(List(IntegerType, StringType)))\n",
      "\u001b[1m\u001b[34mgetFutureMonthDateFromHere\u001b[0m: \u001b[1m\u001b[32m(numMonths: Int, date: String, inputDateFormat: String, outputDateFormat: String)String\u001b[0m\n",
      "\u001b[1m\u001b[34mgetFutureDateFromHere\u001b[0m: \u001b[1m\u001b[32m(numDays: Int, date: String, inputDateFormat: String, outputDateFormat: String)String\u001b[0m\n",
      "\u001b[1m\u001b[34mgetOldDateFromHere\u001b[0m: \u001b[1m\u001b[32m(numDays: Int, date: String, inputDateFormat: String, outputDateFormat: String)String\u001b[0m\n",
      "\u001b[1m\u001b[34mgetOldMonthDateFromHerenotUdf\u001b[0m: \u001b[1m\u001b[32m(numMonths: Int, date: String, inputDateFormat: String, outputDateFormat: String)String\u001b[0m\n",
      "\u001b[1m\u001b[34mDATE\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = date\n",
      "\u001b[1m\u001b[34mPREDICTED\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = predicted\n",
      "\u001b[1m\u001b[34mACTUAL\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = actual\n",
      "\u001b[1m\u001b[34mrundate\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-04-30\n",
      "\u001b[1m\u001b[34mdateForWhichPredictionsMade\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-05-01\n",
      "\u001b[1m\u001b[34mpredictionend\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-05-31\n",
      "\u001b[1m\u001b[34mtrainendDate\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-04-29\n",
      "\u001b[1m\u001b[34mtrainStartDate\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2022-02-01\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import java.time.format.DateTimeFormatter.ofPattern\n",
    "import java.time.temporal.{ChronoUnit}\n",
    "import java.time.{LocalDate}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "  val getNumOfMonthsBetweenTwoDates = udf((startDate: String, endDate: String)=> {\n",
    "\n",
    "    val inputDateFormat: String = \"yyyy-MM-dd\"\n",
    "    val dateBefore: LocalDate = LocalDate.parse(startDate, ofPattern(inputDateFormat))\n",
    "\n",
    "    val dateAfter: LocalDate = LocalDate.parse(endDate, ofPattern(inputDateFormat))\n",
    "\n",
    "    ChronoUnit.MONTHS.between(dateBefore, dateAfter)\n",
    "\n",
    "  })\n",
    "\n",
    "  val getOldMonthDateFromHere = udf((numMonths: Int, date: String) => {\n",
    "      val inputDateFormat: String = \"yyyy-MM-dd\"\n",
    "    val outputDateFormat: String = \"yyyy-MM-dd\"\n",
    "\n",
    "    LocalDate.parse(date, ofPattern(inputDateFormat)).minusMonths(numMonths).format(ofPattern(outputDateFormat))\n",
    "  })\n",
    "\n",
    "\n",
    "    def getFutureMonthDateFromHere(numMonths: Int, date: String, inputDateFormat: String,\n",
    "                                 outputDateFormat: String = \"yyyy-MM-dd\"): String = {\n",
    "    LocalDate.parse(date, ofPattern(inputDateFormat)).plusMonths(numMonths).format(ofPattern(outputDateFormat))\n",
    "  }\n",
    "\n",
    "\n",
    "  def getFutureDateFromHere(numDays: Int, date: String, inputDateFormat: String,\n",
    "                            outputDateFormat: String = \"yyyy-MM-dd\"): String = {\n",
    "    LocalDate.parse(date, ofPattern(inputDateFormat)).plusDays(numDays).format(ofPattern(outputDateFormat))\n",
    "  }\n",
    "\n",
    "    def getOldDateFromHere(numDays: Int, date: String, inputDateFormat: String,\n",
    "                         outputDateFormat: String = \"yyyy-MM-dd\"): String = {\n",
    "    LocalDate.parse(date, ofPattern(inputDateFormat)).minusDays(numDays).format(ofPattern(outputDateFormat))\n",
    "  }\n",
    "\n",
    "  def getOldMonthDateFromHerenotUdf(numMonths: Int, date: String, inputDateFormat: String,\n",
    "                              outputDateFormat: String = \"yyyy-MM-dd\"): String = {\n",
    "    LocalDate.parse(date, ofPattern(inputDateFormat)).minusMonths(numMonths).format(ofPattern(outputDateFormat))\n",
    "  }\n",
    "\n",
    "var DATE=\"date\"\n",
    "var PREDICTED=\"predicted\"\n",
    "var ACTUAL=\"actual\"\n",
    "\n",
    "var rundate = \"2023-04-30\"\n",
    "var dateForWhichPredictionsMade = getFutureDateFromHere(1,rundate,\"yyyy-MM-dd\")\n",
    "var predictionend = \"2023-05-31\"\n",
    "var trainendDate =getOldDateFromHere(1,rundate,\"yyyy-MM-dd\")\n",
    "var trainStartDate = \"2022-02-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34msaveNLoadDF\u001b[0m: \u001b[1m\u001b[32m(key: String)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\u001b[0m\n",
      "\u001b[1m\u001b[34mload\u001b[0m: \u001b[1m\u001b[32m(key: String)org.apache.spark.sql.DataFrame\u001b[0m\n",
      "\u001b[1m\u001b[34mgetOldDateFromHereUDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = UserDefinedFunction(<function4>,StringType,Some(List(IntegerType, StringType, StringType, StringType)))\n",
      "\u001b[1m\u001b[34mgetMonths\u001b[0m: \u001b[1m\u001b[32m(dateFormat: String)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def saveNLoadDF(key: String)(df: DataFrame): DataFrame = {\n",
    "    var tempPath = \"/data/Archive/bhavesh/InventoryPrediction/temp/dashboard_metrics\"\n",
    "    DFToParquet.putDF(tempPath + \"/\" + key, df)\n",
    "    ParquetToDF.getDF(tempPath + \"/\" + key)\n",
    "  }\n",
    "\n",
    "def load(key: String): DataFrame = {\n",
    "    var tempPath = \"/data/Archive/bhavesh/InventoryPrediction/temp/dashboard_metrics\"\n",
    "    ParquetToDF.getDF(tempPath + \"/\" + key)\n",
    "}\n",
    "\n",
    "val getOldDateFromHereUDF = udf((numDays: Int, date: String, inputDateFormat: String, outputDateFormat: String) => {\n",
    "    getOldDateFromHere(numDays, date, inputDateFormat, outputDateFormat)\n",
    "})\n",
    "\n",
    "def getMonths(dateFormat: String)(df: DataFrame): DataFrame = {\n",
    "    df.withColumn(\"dateFormatted\", getOldDateFromHereUDF(lit(0), col(DATE), lit(dateFormat), lit(\"yyyy-MM-dd\")))\n",
    "    .withColumn(\"year\", split(col(\"dateFormatted\"), \"-\")(0))\n",
    "    .withColumn(\"month\", split(col(\"dateFormatted\"), \"-\")(1))\n",
    "    .withColumn(\"monthsInHis\", dense_rank().over(Window.orderBy(desc(\"year\"), desc(\"month\"))) - lit(1))\n",
    "    .drop(\"dateFormatted\", \"year\", \"month\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mbaseDir\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/Archive/bhavesh/InventoryPrediction\n",
      "\u001b[1m\u001b[34mrunDir\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/Archive/bhavesh/InventoryPrediction/2023-04-30\n"
     ]
    }
   ],
   "source": [
    "var baseDir = \"/data/Archive/bhavesh/InventoryPrediction\"\n",
    "var runDir = s\"$baseDir/$rundate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mproductLevelDailyQuantity\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, date: date ... 1 more field]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var productLevelDailyQuantity = ParquetToDF.getDF(s\"$runDir/DailyQuantity/product\")\n",
    "productLevelDailyQuantity.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mproductLevelQuantity\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [productid: string, quantity: double]\n"
     ]
    }
   ],
   "source": [
    "var productLevelQuantity = (\n",
    "    productLevelDailyQuantity.filter(col(\"date\")>=trainStartDate && col(\"date\")<dateForWhichPredictionsMade).filter(!(col(\"date\")===rundate))\n",
    "    .transform(getMonths(\"yyyy-MM-dd\"))\n",
    "    .withColumnRenamed(\"monthsInHis\", \"pastmonth\")\n",
    "    .filter(col(\"pastmonth\") < 15)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .agg(sum(QUANTITY) as QUANTITY)\n",
    "    .transform(saveNLoadDF(\"productQuantityForPast15MonthsAggregated\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.spark.SparkException: Job aborted.\n",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\n",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n",
      "  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:549)\n",
      "  at ai.couture.obelisk.commons.io.DFToParquet$.putDF(DFToParquet.scala:27)\n",
      "  at saveNLoadDF(<console>:83)\n",
      "  at $anonfun$2.apply(<console>:102)\n",
      "  at $anonfun$2.apply(<console>:102)\n",
      "  at org.apache.spark.sql.Dataset.transform(Dataset.scala:2518)\n",
      "  ... 78 elided\n",
      "Caused by: org.apache.spark.SparkException: Job 9 cancelled\n",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1586)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1838)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2039)\n",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n",
      "  ... 103 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var ga = (\n",
    "    ParquetToDF.getDF(\"/data/ecomm/ajio/processed/ProcessedGAData\")\n",
    "    .select(PRODUCTID, \"PLPViewsPerDay\", DATE)\n",
    "    .filter(col(\"date\")>=trainStartDate && col(\"date\")<dateForWhichPredictionsMade).filter(!(col(\"date\")===rundate))\n",
    "    .transform(getMonths(\"yyyy-MM-dd\"))\n",
    "    .withColumnRenamed(\"monthsInHis\", \"pastmonth\")\n",
    "    .filter(col(\"pastmonth\") < 15)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .agg(sum(\"PLPViewsPerDay\") as \"impressions\")\n",
    "    .transform(saveNLoadDF(\"productImpressionsForPast15MonthsAggregated\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mproductDetails\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, similargrouplevel: string ... 7 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- colorfamily: string (nullable = true)\n",
      " |-- brandname: string (nullable = true)\n",
      " |-- styletype: string (nullable = true)\n",
      " |-- pattern: string (nullable = true)\n",
      " |-- sleeve: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var productDetails = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/inventory/integrationSimulator/productDetails\")\n",
    "productDetails.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175ec68709454d93904e182f76ebc0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-de6fc5766106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'productDetails = productDetails.select(\"productid\").distinct().transform(saveNLoadDF(\"allProducts\"))\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2372\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-125>\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/kernels/kernelmagics.py\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001b[0m in \u001b[0;36mexecute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmimetype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_session_on_spark_statement_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36mrun_command\u001b[0;34m(self, command, client_name)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36m_get_statement_output\u001b[0;34m(self, session, statement_id)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'progress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(self, retries)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "productDetails = productDetails.select(\"productid\").distinct().transform(saveNLoadDF(\"allProducts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mproductDetails\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string]\n"
     ]
    }
   ],
   "source": [
    "var productDetails = load(\"allProducts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres14\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 434089\n"
     ]
    }
   ],
   "source": [
    "productDetails.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var productMetrics = productDetails.join(productLevelQuantity, Seq(PRODUCTID), \"left\").join(ga, Seq(PRODUCTID), \"left\").na.fill(0).transform(saveNLoadDF(\"productLevelMetrics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var productMetrics = load(\"productLevelMetrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34msuffix\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = colorfamily_pricebucket_styletype_pattern_sleeve_brandname\n",
      "\u001b[1m\u001b[34mqueriesToProductMap\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, productid: string ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "var suffix = \"colorfamily_pricebucket_styletype_pattern_sleeve_brandname\"\n",
    "var queriesToProductMap = ParquetToDF.getDF(s\"$runDir/QueryProductMapping/suffix=$suffix\").select(\"query\", \"productid\", \"similargrouplevel\", \"template\").na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var queryLevelMetrics = (\n",
    "    productMetrics.join(queriesToProductMap, Seq(PRODUCTID))\n",
    "    .groupBy(\"query\", \"similargrouplevel\", \"template\")\n",
    "    .agg(sum(QUANTITY).cast(DoubleType) as QUANTITY, sum(\"impressions\").cast(DoubleType) as \"impressions\")\n",
    "    .transform(saveNLoadDF(\"queryLevelMetrics\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mqueryLevelMetrics\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, similargrouplevel: int ... 3 more fields]\n"
     ]
    }
   ],
   "source": [
    "var queryLevelMetrics = load(\"queryLevelMetrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------+------------------------------+--------+-----------+\n",
      "|query                     |similargrouplevel|template                      |quantity|impressions|\n",
      "+--------------------------+-----------------+------------------------------+--------+-----------+\n",
      "|4_full-length sleeve_black|830216013        |pricebucket_sleeve_colorfamily|13921.0 |4.0625933E7|\n",
      "|4_full-length sleeve_black|830303011        |pricebucket_sleeve_colorfamily|197.0   |1308173.0  |\n",
      "+--------------------------+-----------------+------------------------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryLevelMetrics.filter(col(\"query\") === \"4_full-length sleeve_black\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres17\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 1480430\n"
     ]
    }
   ],
   "source": [
    "queryLevelMetrics.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mpredictions\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, similargrouplevel: int ... 7 more fields]\n",
      "root\n",
      " |-- query: string (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- template: string (nullable = true)\n",
      " |-- actual: double (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- lowerBound: double (nullable = true)\n",
      " |-- upperBound: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- category_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var predictions = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/inventory/integrationSimulator/predictions\")\n",
    "predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------+------------------------------+------+------------------+-----------------+------------------+------------------+-------------+\n",
      "|query                     |similargrouplevel|template                      |actual|predicted         |lowerBound       |upperBound        |stddev            |category_name|\n",
      "+--------------------------+-----------------+------------------------------+------+------------------+-----------------+------------------+------------------+-------------+\n",
      "|4_full-length sleeve_black|830303011        |pricebucket_sleeve_colorfamily|3.0   |12.979360222816467|0.0              |54.38606440843298 |20.703352092808256|Women-Kurtas |\n",
      "|4_full-length sleeve_black|830216013        |pricebucket_sleeve_colorfamily|1713.0|1328.1743819713593|738.0569052231813|1918.2918587195372|295.058738374089  |Men-Shirts   |\n",
      "+--------------------------+-----------------+------------------------------+------+------------------+-----------------+------------------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(col(\"query\") === \"4_full-length sleeve_black\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres20\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 383205\n"
     ]
    }
   ],
   "source": [
    "predictions.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var impressions = (\n",
    "    predictions.select(\"query\", \"similargrouplevel\", \"predicted\", \"template\", \"actual\")\n",
    "    .join(queryLevelMetrics, Seq(\"query\", SIMILAR_GROUP_LEVEL, \"template\"), \"left\")\n",
    "    .na.fill(0)\n",
    "    .withColumn(\"impressions\", when(col(\"impressions\") === 0 || col(\"quantity\") === 0, lit(null)).otherwise((col(\"predicted\") * col(\"impressions\"))/col(\"quantity\").cast(DoubleType)))\n",
    "    .transform(saveNLoadDF(\"completeQuantityImpressionsQueryLevel\"))\n",
    "    .select(\"query\", \"similargrouplevel\", \"impressions\", \"template\")\n",
    ")\n",
    "\n",
    "DFToParquet.putDF(\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/dashboardMetrics/impressions\", impressions)\n",
    "impressions = ParquetToDF.getDF(\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/dashboardMetrics/impressions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mimpressions\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, similargrouplevel: int ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "var impressions = ParquetToDF.getDF(\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/dashboardMetrics/impressions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------+------------------+------------------------------+\n",
      "|query                     |similargrouplevel|impressions       |template                      |\n",
      "+--------------------------+-----------------+------------------+------------------------------+\n",
      "|4_full-length sleeve_black|830216013        |3876037.8891088897|pricebucket_sleeve_colorfamily|\n",
      "|4_full-length sleeve_black|830303011        |86189.07919168775 |pricebucket_sleeve_colorfamily|\n",
      "+--------------------------+-----------------+------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "impressions.filter(col(\"query\") === \"4_full-length sleeve_black\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres22\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 383205\n"
     ]
    }
   ],
   "source": [
    "impressions.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mdiscountProductLevel\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, WeightedAvgDiscount: double]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- WeightedAvgDiscount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var discountProductLevel = ParquetToDF.getDF(\"/data/Archive/inventory/2023-04-30/queryProductMap_colorfamily_pricebucket_styletype_pattern_sleeve_brandname/productDiscountDashboard\")\n",
    "discountProductLevel.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discountProductLevel: org.apache.spark.sql.DataFrame = [productid: string, WeightedAvgDiscount: double]\n",
      "\u001b[1m\u001b[34mres27\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 434089\n"
     ]
    }
   ],
   "source": [
    "discountProductLevel = discountProductLevel.join(productDetails, Seq(PRODUCTID))\n",
    "discountProductLevel.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mdiscountQueryLevel\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [query: string, similargrouplevel: int ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "var discountQueryLevel = (\n",
    "    discountProductLevel.join(queriesToProductMap, Seq(PRODUCTID))\n",
    "    .groupBy(\"query\", \"similargrouplevel\", \"template\")\n",
    "    .agg(avg(\"WeightedAvgDiscount\").cast(DoubleType) as \"discount\")\n",
    "    .transform(saveNLoadDF(\"queryLevelDiscount\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------+------------------------------+------------------+\n",
      "|query                     |similargrouplevel|template                      |discount          |\n",
      "+--------------------------+-----------------+------------------------------+------------------+\n",
      "|4_full-length sleeve_black|830216013        |pricebucket_sleeve_colorfamily|14.039917502521597|\n",
      "|4_full-length sleeve_black|830303011        |pricebucket_sleeve_colorfamily|11.759362207500649|\n",
      "+--------------------------+-----------------+------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "discountQueryLevel.filter(col(\"query\") === \"4_full-length sleeve_black\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mdiscount\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, similargrouplevel: int ... 2 more fields]\n",
      "discount: org.apache.spark.sql.DataFrame = [query: string, similargrouplevel: int ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "var discount = (\n",
    "    predictions.select(\"query\", \"similargrouplevel\", \"template\")\n",
    "    .join(discountQueryLevel, Seq(\"query\", SIMILAR_GROUP_LEVEL, \"template\"), \"left\")\n",
    "    .na.fill(0.0)\n",
    "    .select(\"query\", \"similargrouplevel\", \"discount\", \"template\")\n",
    ")\n",
    "\n",
    "DFToParquet.putDF(\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/dashboardMetrics/discount\", discount)\n",
    "discount = ParquetToDF.getDF(\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/dashboardMetrics/discount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres39\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 383205\n"
     ]
    }
   ],
   "source": [
    "discount.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres40\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 383205\n"
     ]
    }
   ],
   "source": [
    "impressions.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derive product level views from predicted impressions for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mproductMetrics\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, quantity: double ... 1 more field]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      " |-- impressions: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var productMetrics = ParquetToDF.getDF(\"/data/Archive/bhavesh/InventoryPrediction/temp/dashboard_metrics/productLevelMetrics\")\n",
    "productMetrics.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mqueryLevelData\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, similargrouplevel: int ... 5 more fields]\n",
      "root\n",
      " |-- query: string (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- template: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- actual: double (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      " |-- impressions: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var queryLevelData = load(\"completeQuantityImpressionsQueryLevel\")\n",
    "queryLevelData.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mqueryMetrics\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [productid: string, quantity: double ... 4 more fields]\n",
      "\u001b[1m\u001b[34mqueryMetricsStats\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, similargrouplevel: int ... 3 more fields]\n",
      "queryMetrics: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [query: string, similargrouplevel: int ... 8 more fields]\n"
     ]
    }
   ],
   "source": [
    "var queryMetrics = productMetrics.join(queriesToProductMap, Seq(PRODUCTID)).transform(saveNLoadDF(\"queryWiseImpressionsQuantity\"))\n",
    "var queryMetricsStats = queryMetrics.groupBy(\"query\", \"similargrouplevel\", \"template\").agg(sum(\"impressions\") as \"sum_impressions\", sum(\"quantity\") as \"sum_quantity\")\n",
    "\n",
    "queryMetrics = (\n",
    "    queryMetrics.join(queryMetricsStats, Seq(\"query\", \"similargrouplevel\", \"template\"))\n",
    "    .withColumn(\"viewsFraction\", col(\"impressions\")/col(\"sum_impressions\").cast(DoubleType))\n",
    "    .withColumn(\"quantityFraction\", col(\"quantity\")/col(\"sum_quantity\").cast(DoubleType))\n",
    "    .select(\"productid\", \"query\", \"similargrouplevel\", \"template\", \"viewsFraction\", \"quantityFraction\")\n",
    "    .transform(saveNLoadDF(\"queryWiseProductFraction\"))\n",
    "    .join(queryLevelData, Seq(\"query\", \"similargrouplevel\", \"template\"))\n",
    "    .withColumn(\"impressions\", col(\"impressions\") * col(\"viewsFraction\"))\n",
    "    .withColumn(\"actual\", col(\"actual\") * col(\"quantityFraction\"))\n",
    "    .withColumn(\"predicted\", col(\"predicted\") * col(\"quantityFraction\"))\n",
    "    .transform(saveNLoadDF(\"featuresProductLevelFromQueryLevel\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- query: string (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- template: string (nullable = true)\n",
      " |-- productid: string (nullable = true)\n",
      " |-- viewsFraction: double (nullable = true)\n",
      " |-- quantityFraction: double (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- actual: double (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      " |-- impressions: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryMetrics.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres46\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 18683197\n"
     ]
    }
   ],
   "source": [
    "queryMetrics.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres47\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 18683065\n"
     ]
    }
   ],
   "source": [
    "queryMetrics.na.drop.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DFToParquet.putDF(\"/data/ecomm/ajio/processed/inventory/integrationSimulator/productPredictions\", queryMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres49\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 434089\n"
     ]
    }
   ],
   "source": [
    "queryMetrics.select(PRODUCTID).distinct.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34maggregatedImpressions\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, similargrouplevel: int ... 2 more fields]\n",
      "\u001b[1m\u001b[34maggregatedDiscount\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, similargrouplevel: int ... 2 more fields]\n",
      "\u001b[1m\u001b[34mdashboardMetricsAggregated\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, similargrouplevel: int ... 3 more fields]\n",
      "383205\n"
     ]
    }
   ],
   "source": [
    "var aggregatedImpressions = ParquetToDF.getDF(\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/dashboardMetrics/impressions\")\n",
    "var aggregatedDiscount = ParquetToDF.getDF(\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/dashboardMetrics/discount\")\n",
    "\n",
    "var dashboardMetricsAggregated = aggregatedImpressions.join(aggregatedDiscount, Seq(\"query\", \"similargrouplevel\", \"template\"))\n",
    "println(dashboardMetricsAggregated.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mdashboardMetricsAvg\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, template: string ... 4 more fields]\n",
      "root\n",
      " |-- query: string (nullable = true)\n",
      " |-- template: string (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- avg_predicted: double (nullable = true)\n",
      " |-- avg_actual: double (nullable = true)\n",
      " |-- avg_impressions: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var dashboardMetricsAvg = (\n",
    "    load(\"featuresProductLevelFromQueryLevel\").filter(col(\"impressions\") =!= 0 && !col(\"impressions\").isNull && col(\"quantityFraction\") > 0)\n",
    "    .groupBy(\"query\", \"template\", \"similargrouplevel\").agg(avg(\"predicted\") as \"avg_predicted\", avg(\"actual\") as \"avg_actual\", avg(\"impressions\") as \"avg_impressions\")\n",
    ")\n",
    "dashboardMetricsAvg.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres17\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 383185\n"
     ]
    }
   ],
   "source": [
    "dashboardMetricsAvg.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mdashboardMetrics\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, similargrouplevel: int ... 6 more fields]\n",
      "root\n",
      " |-- query: string (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- template: string (nullable = true)\n",
      " |-- impressions: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      " |-- avg_predicted: double (nullable = true)\n",
      " |-- avg_actual: double (nullable = true)\n",
      " |-- avg_impressions: double (nullable = true)\n",
      "\n",
      "383205\n"
     ]
    }
   ],
   "source": [
    "var dashboardMetrics = dashboardMetricsAggregated.join(dashboardMetricsAvg, Seq(\"query\", \"similargrouplevel\", \"template\"), \"left\")\n",
    "dashboardMetrics.printSchema\n",
    "println(dashboardMetrics.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dashboardMetrics: org.apache.spark.sql.DataFrame = [query: string, similargrouplevel: int ... 6 more fields]\n",
      "root\n",
      " |-- query: string (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- template: string (nullable = true)\n",
      " |-- impressions: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      " |-- avg_predicted: double (nullable = true)\n",
      " |-- avg_actual: double (nullable = true)\n",
      " |-- avg_impressions: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DFToParquet.putDF(\"/data/ecomm/ajio/processed/inventory/integrationSimulator/dashboardMetrics\", dashboardMetrics)\n",
    "dashboardMetrics = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/inventory/integrationSimulator/dashboardMetrics\")\n",
    "\n",
    "dashboardMetrics.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres59\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 0\n"
     ]
    }
   ],
   "source": [
    "dashboardMetrics.filter(col(\"query\") === \"\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----------------+----------------------------------------+-----------------+-----------------+------------------+------------------+-----------------+\n",
      "|query                      |similargrouplevel|template                                |impressions      |discount         |avg_predicted     |avg_actual        |avg_impressions  |\n",
      "+---------------------------+-----------------+----------------------------------------+-----------------+-----------------+------------------+------------------+-----------------+\n",
      "|1_3-4th sleeve_beige_a-line|830303011        |pricebucket_sleeve_colorfamily_styletype|1877380.228458216|64.06952023169832|22.684167269113903|13.864864864864865|50738.69132844789|\n",
      "+---------------------------+-----------------+----------------------------------------+-----------------+-----------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dashboardMetrics.filter(col(\"query\") === \"1_3-4th sleeve_beige_a-line\").show(false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
