{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'Bhavesh_Relevancy_notebook', 'spark.yarn.queue': 'default', 'spark.jars': '/apps/Jars/obelisk-retail-legos.jar,/apps/Jars/mysql-connector-java-5.1.40.jar', 'spark.driver.extraJavaOptions': '-Dscala.color', 'spark.driver.memory': '2g', 'spark.executor.memory': '2g', 'spark.executor.instances': '4'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1209</td><td>application_1685341929318_0823</td><td>spark</td><td>dead</td><td><a target=\"_blank\" href=\"http://jmngd1baf370c10:8088/cluster/app/application_1685341929318_0823\">Link</a></td><td></td><td></td></tr><tr><td>1214</td><td>application_1685341929318_0829</td><td>spark</td><td>dead</td><td><a target=\"_blank\" href=\"http://JMNGD1BAF370C10:8088/cluster/app/application_1685341929318_0829\">Link</a></td><td></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{\n",
    "    \"conf\":\n",
    "    {\n",
    "        \"spark.app.name\":\"Bhavesh_Relevancy_notebook\",\n",
    "        \"spark.yarn.queue\": \"default\",\n",
    "        \"spark.jars\": \"/apps/Jars/obelisk-retail-legos.jar,/apps/Jars/mysql-connector-java-5.1.40.jar\",\n",
    "        \"spark.driver.extraJavaOptions\": \"-Dscala.color\",\n",
    "        \"spark.driver.memory\": \"2g\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.instances\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1215</td><td>application_1685341929318_0831</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://JMNGD1BAF370C10:8088/proxy/application_1685341929318_0831/\">Link</a></td><td><a target=\"_blank\" href=\"http://JMNGDBAE230C07:8042/node/containerlogs/container_e112_1685341929318_0831_01_000001/couture\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import ai.couture.obelisk.commons.Constants._\n",
      "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
      "import ai.couture.obelisk.commons.Constants._\n",
      "import ai.couture.obelisk.commons.io._\n",
      "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
      "import org.apache.spark.sql._\n",
      "import org.apache.spark.sql.expressions._\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.types._\n",
      "import spark.implicits._\n"
     ]
    }
   ],
   "source": [
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.io._\n",
    "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mgetDaysInBetween\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = UserDefinedFunction(<function2>,LongType,Some(List(StringType, StringType)))\n",
      "\u001b[1m\u001b[34msaveNLoadDF\u001b[0m: \u001b[1m\u001b[32m(key: String)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "var getDaysInBetween: UserDefinedFunction = udf((startDate: String, endDate: String) => {\n",
    "    getNumberOfDaysBetweenTwoDates(startDate, endDate, \"yyyy-MM-dd\")\n",
    "})\n",
    "\n",
    "def saveNLoadDF(key: String)(df: DataFrame): DataFrame = {\n",
    "    var tempPath = \"/data/Archive/bhavesh/inventoryPrediction/temp/exp_files\"\n",
    "    DFToParquet.putDF(tempPath + \"/\" + key, df)\n",
    "    ParquetToDF.getDF(tempPath + \"/\" + key)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34minteractionsDB\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, date: date ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "var interactionsDB = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "interactionsDB = (\n",
    "    interactionsDB.filter(col(DATE).between(\"2022-03-13\", \"2023-03-13\") && col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "                          && col(USERID).isNotNull && col(QUANTITY) > 0 && col(\"booked_rev\") > 0 && col(PRODUCTID).isNotNull\n",
    "                         )\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(sum(QUANTITY) as \"sales\")\n",
    "    .transform(saveNLoadDF(\"salesDayWise\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34moperationDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, date: date ... 3 more fields]\n"
     ]
    }
   ],
   "source": [
    "var distinctDates = interactionsDB.select(DATE).distinct()\n",
    "var distinctProducts = interactionsDB.select(PRODUCTID).distinct()\n",
    "var distinctCombinations = distinctProducts.crossJoin(broadcast(distinctDates))\n",
    "\n",
    "var operationDF = (\n",
    "    distinctCombinations.join(interactionsDB, Seq(PRODUCTID, DATE), \"left\")\n",
    "    .na.fill(0)\n",
    "    .withColumn(\"sales\", col(\"sales\").cast(DoubleType))\n",
    "    .transform(saveNLoadDF(\"salesCompleteTrain\"))\n",
    "    .withColumn(\"daysInHistory\", getDaysInBetween(col(DATE), lit(\"2023-03-15\")) - lit(1))\n",
    "    .withColumn(\"monthsInHistory\", floor(col(\"daysInHistory\")/30) + 1)\n",
    "    .filter(col(\"monthsInHistory\") <= 12)\n",
    "    .repartition(210)\n",
    "    .transform(saveNLoadDF(\"operationDF\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21bb156364a4de4b1d5bdcdec3b06e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://10.144.97.194:8999/sessions/1187/statements/5 with error payload: \"requirement failed: Session isn't active.\"\n"
     ]
    }
   ],
   "source": [
    "var percentileLimits = operationDF.groupBy(PRODUCTID).agg(\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.1)).alias(\"percentile_10\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.2)).alias(\"percentile_20\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.3)).alias(\"percentile_30\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.4)).alias(\"percentile_40\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.5)).alias(\"percentile_50\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.6)).alias(\"percentile_60\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.7)).alias(\"percentile_70\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.8)).alias(\"percentile_80\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.9)).alias(\"percentile_90\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(1.0)).alias(\"percentile_100\")\n",
    ").transform(saveNLoadDF(\"productPercentiles\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val binColumns = Array(\"percentile_10\", \"percentile_30\", \"percentile_50\", \"percentile_70\", \"percentile_90\")\n",
    "\n",
    "// Use the stack function to stack the columns into key-value pairs\n",
    "val stackedDF = percentileLimits.select($\"productid\", expr(s\"stack(${binColumns.length}, ${binColumns.map(c => s\"'$c', $c\").mkString(\", \")})\").as(\"bin\", \"value\"))\n",
    "\n",
    "var percentileLimitsBinned = stackedDF.withColumn(\"bin\", substring(col(\"bin\"), 12, 2).cast(IntegerType)).withColumn((col(\"bin\") + lit(10))/lit(20)).transform(saveNLoadDF(\"percentileLimitsBinned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    var X = operationDF.join(percentileLimits, Seq(PRODUCTID)).withColumn(\n",
    "      \"bin\",\n",
    "      when(col(\"sales\") <= col(\"percentile_20\"), lit(1)).otherwise(\n",
    "        when(col(\"sales\") <= col(\"percentile_40\"), lit(2)).otherwise(\n",
    "          when(col(\"sales\") <= col(\"percentile_60\"), lit(3)).otherwise(\n",
    "            when(col(\"sales\") <= col(\"percentile_80\"), lit(4)).otherwise(\n",
    "              lit(5)\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    ).select(PRODUCTID, \"monthsInHistory\", \"bin\", \"daysInHistory\").transform(saveNLoadDF(\"operationDFBinned\"))\n",
    "X = X.groupBy(PRODUCTID, \"monthsInHistory\", \"bin\").agg(count(\"daysInHistory\") as \"count\").groupBy(PRODUCTID, \"bin\").agg(avg(\"count\") as \"count\").transform(saveNLoadDF(\"featureStats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var y = (\n",
    "    ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    .filter(col(DATE).between(\"2023-03-15\", \"2023-04-14\") && col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "                          && col(USERID).isNotNull && col(QUANTITY) > 0 && col(\"booked_rev\") > 0 && col(PRODUCTID).isNotNull\n",
    "                         )\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(sum(QUANTITY).cast(DoubleType) as \"sales\")\n",
    "    .transform(saveNLoadDF(\"salesDayWiseTarget\"))\n",
    ")\n",
    "\n",
    "var distinctDatesTarget = y.select(DATE).distinct()\n",
    "var distinctProductsTarget = y.select(PRODUCTID).distinct()\n",
    "var distinctCombinationsTarget = distinctProductsTarget.crossJoin(broadcast(distinctDatesTarget))\n",
    "y = distinctCombinationsTarget.join(y, Seq(PRODUCTID, DATE), \"left\").na.fill(0).transform(saveNLoadDF(\"operationDFTarget\"))\n",
    "y = (\n",
    "    y.join(percetileLimits, Seq(PRODUCTID))\n",
    "    .withColumn(\"bin\",\n",
    "                when(col(\"sales\")<=col(\"percentile_20\", lit(1))).otherwise(\n",
    "                    when(col(\"sales\")<=col(\"percentile_40\"), lit(2)).otherwise(\n",
    "                        when(col(\"sales\")<=col(\"percentile_60\"), lit(3)).otherwise(\n",
    "                            when(\"sales\")<=col(\"percentile_80\", lit(4)).otherwise(\n",
    "                                lit(5)\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    .select(PRODUCTID, \"bin\", DATE)\n",
    "    .repartition(210)\n",
    "    .transform(saveNLoadDF(\"operationDFTargetBinned\"))\n",
    "    .groupBy(PRODUCTID, \"bin\")\n",
    "    .agg(count(DATE) as \"count\")\n",
    "    .transform(saveNLoadDF(\"targetStats\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package ai.couture.obelisk.retail.legos.inventoryprediction\n",
    "\n",
    "import ai.couture.obelisk.commons.utils.BaseBlocks\n",
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.io._\n",
    "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "object PercentileBinnedComputation extends BaseBlocks {\n",
    "\n",
    "  var getDaysInBetween: UserDefinedFunction = udf((startDate: String, endDate: String) => {\n",
    "    getNumberOfDaysBetweenTwoDates(startDate, endDate, \"yyyy-MM-dd\")\n",
    "  })\n",
    "\n",
    "  override def load(): Unit = {\n",
    "\n",
    "  }\n",
    "\n",
    "  override def doTransformations(): Unit = {\n",
    "      \n",
    "      var interactionsDB = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    interactionsDB = (\n",
    "        interactionsDB.filter(col(DATE).between(\"2022-03-13\", \"2023-03-13\") && col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "                              && col(USERID).isNotNull && col(QUANTITY) > 0 && col(\"booked_rev\") > 0 && col(PRODUCTID).isNotNull\n",
    "                             )\n",
    "        .groupBy(PRODUCTID, DATE)\n",
    "        .agg(sum(QUANTITY) as \"sales\")\n",
    "        .transform(saveNLoadDF(\"salesDayWise\"))\n",
    "    )\n",
    "\n",
    "    var trimmedProducts = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/TransformedDataset/date_when_prediction_is_made=2023-04-16/combined/data/test\")\n",
    "      .select(PRODUCTID).distinct()\n",
    "\n",
    "    val distinctDates: DataFrame = interactionsDB.select(DATE).distinct()\n",
    "    var operationDF = trimmedProducts.crossJoin(broadcast(distinctDates))\n",
    "      .join(interactionsDB, Seq(PRODUCTID, DATE), \"left\")\n",
    "      .na.fill(0)\n",
    "      .withColumn(\"sales\", col(\"sales\").cast(DoubleType))\n",
    "      .transform(saveNLoadDF(\"trimmed/salesCompleteTrain\"))\n",
    "      .withColumn(\"daysInHistory\", getDaysInBetween(col(DATE), lit(\"2023-03-15\")) - lit(1))\n",
    "      .withColumn(\"monthsInHistory\", floor(col(\"daysInHistory\") / 30) + 1)\n",
    "      .filter(col(\"monthsInHistory\") <= 12)\n",
    "      .transform(saveNLoadDF(\"trimmed/operationDF\"))\n",
    "\n",
    "    var percentileLimits = operationDF.groupBy(PRODUCTID).agg(\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.1)).alias(\"percentile_10\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.2)).alias(\"percentile_20\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.3)).alias(\"percentile_30\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.4)).alias(\"percentile_40\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.5)).alias(\"percentile_50\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.6)).alias(\"percentile_60\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.7)).alias(\"percentile_70\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.8)).alias(\"percentile_80\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.9)).alias(\"percentile_90\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(1.0)).alias(\"percentile_100\")\n",
    "    ).transform(saveNLoadDF(\"trimmed/productPercentiles\"))\n",
    "\n",
    "    var X = operationDF.join(percentileLimits, Seq(PRODUCTID)).withColumn(\n",
    "      \"bin\",\n",
    "      when(col(\"sales\") <= col(\"percentile_20\"), lit(1)).otherwise(\n",
    "        when(col(\"sales\") <= col(\"percentile_40\"), lit(2)).otherwise(\n",
    "          when(col(\"sales\") <= col(\"percentile_60\"), lit(3)).otherwise(\n",
    "            when(col(\"sales\") <= col(\"percentile_80\"), lit(4)).otherwise(\n",
    "              lit(5)\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    ).select(PRODUCTID, \"monthsInHistory\", \"bin\", \"daysInHistory\")\n",
    "      .transform(saveNLoadDF(\"trimmed/operationDFBinned\"))\n",
    "    X = X.groupBy(PRODUCTID, \"monthsInHistory\", \"bin\")\n",
    "      .agg(count(\"daysInHistory\") as \"count\")\n",
    "      .groupBy(PRODUCTID, \"bin\")\n",
    "      .agg(sum(\"count\").cast(DoubleType) / lit(12.0) as \"count\")\n",
    "      .transform(saveNLoadDF(\"trimmed/featureStats\"))\n",
    "\n",
    "    var y = (\n",
    "      ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "        .filter(col(DATE).between(\"2023-03-15\", \"2023-04-14\") && col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\"\n",
    "          && col(USERID).isNotNull && col(QUANTITY) > 0 && col(\"booked_rev\") > 0 && col(PRODUCTID).isNotNull\n",
    "        )\n",
    "        .groupBy(PRODUCTID, DATE)\n",
    "        .agg(sum(QUANTITY).cast(DoubleType) as \"sales\")\n",
    "        .transform(saveNLoadDF(\"salesDayWiseTarget\"))\n",
    "      )\n",
    "\n",
    "    var distinctDatesTarget = y.select(DATE).distinct()\n",
    "    var distinctCombinationsTarget = trimmedProducts.crossJoin(broadcast(distinctDatesTarget))\n",
    "    y = distinctCombinationsTarget.join(y, Seq(PRODUCTID, DATE), \"left\")\n",
    "      .na.fill(0)\n",
    "      .transform(saveNLoadDF(\"trimmed/operationDFTarget\"))\n",
    "    y = (\n",
    "      y.join(percentileLimits, Seq(PRODUCTID))\n",
    "        .withColumn(\"bin\",\n",
    "          when(col(\"sales\") <= col(\"percentile_20\"), lit(1)).otherwise(\n",
    "            when(col(\"sales\") <= col(\"percentile_40\"), lit(2)).otherwise(\n",
    "              when(col(\"sales\") <= col(\"percentile_60\"), lit(3)).otherwise(\n",
    "                when(col(\"sales\") <= col(\"percentile_80\"), lit(4)).otherwise(\n",
    "                  lit(5)\n",
    "                )\n",
    "              )\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "        .select(PRODUCTID, \"bin\", DATE)\n",
    "        .repartition(210)\n",
    "        .transform(saveNLoadDF(\"trimmed/operationDFTargetBinned\"))\n",
    "        .groupBy(PRODUCTID, \"bin\")\n",
    "        .agg(count(DATE) as \"count\")\n",
    "        .transform(saveNLoadDF(\"trimmed/targetStats\"))\n",
    "      )\n",
    "\n",
    "    val binColumns = Array(\"percentile_10\", \"percentile_30\", \"percentile_50\", \"percentile_70\", \"percentile_90\")\n",
    "\n",
    "    // Use the stack function to stack the columns into key-value pairs\n",
    "    val stackedDF = percentileLimits.select(col(\"productid\"),\n",
    "      expr(s\"stack(${binColumns.length}, ${binColumns.map(c => s\"'$c', $c\").mkString(\", \")}) as (bin, value)\"))\n",
    "\n",
    "    var percentileLimitsBinned = stackedDF.withColumn(\"bin\", substring(col(\"bin\"), 12, 2).cast(IntegerType))\n",
    "      .withColumn(\"bin\", (col(\"bin\") + lit(10)) / lit(20))\n",
    "      .transform(saveNLoadDF(\"trimmed/percentileLimitsBinned\"))\n",
    "      \n",
    "    var test = X.toDF(PRODUCTID, \"bin\", \"count_x\").join(y.toDF(PRODUCTID, \"bin\", \"count_y\"), Seq(PRODUCTID, \"bin\"), \"outer\").na.fill(0.0).withColumn(\"count\", (col(\"count_x\") + col(\"count_y\"))/lit(2.0)).drop(\"count_x\", \"count_y\")\n",
    "    test = test.transform(saveNLoadDF(\"trimmed/test\"))\n",
    "    test = test.join(percentileLimitsBinned, Seq(PRODUCTID, \"bin\")).withColumn(\"predictedyQuantity\", col(\"count\") * col(\"value\")).groupBy(PRODUCTID).agg(sum(\"predictedyQuantity\") as \"predictedyQuantity\").transform(saveNLoadDF(\"trimmed/predictions\"))\n",
    "      \n",
    "      // Later split predictions into menShirts/womenKurtas according to productAttrs\n",
    "  }\n",
    "\n",
    "  def saveNLoadDF(key: String)(df: DataFrame): DataFrame = {\n",
    "    var tempPath = \"/data/Archive/bhavesh/inventoryPrediction/temp/exp_files\"\n",
    "    DFToParquet.putDF(tempPath + \"/\" + key, df)\n",
    "    ParquetToDF.getDF(tempPath + \"/\" + key)\n",
    "  }\n",
    "\n",
    "  override def save(): Unit = {\n",
    "\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mx\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, bin: int ... 1 more field]\n",
      "\u001b[1m\u001b[34my\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, bin: int ... 1 more field]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- bin: integer (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- bin: integer (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var x = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/temp/exp_files/trimmed/featureStats\").withColumn(\"count\", col(\"count\").cast(DoubleType))\n",
    "var y = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/temp/exp_files/trimmed/targetStats\").withColumn(\"count\", col(\"count\").cast(DoubleType))\n",
    "x.printSchema\n",
    "y.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mtest\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, bin: int ... 1 more field]\n",
      "test: org.apache.spark.sql.DataFrame = [productid: string, bin: int ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "var test = x.toDF(PRODUCTID, \"bin\", \"count_x\").join(y.toDF(PRODUCTID, \"bin\", \"count_y\"), Seq(PRODUCTID, \"bin\"), \"outer\").na.fill(0.0).withColumn(\"count\", (col(\"count_x\") + col(\"count_y\"))/lit(2.0)).drop(\"count_x\", \"count_y\")\n",
    "test = test.transform(saveNLoadDF(\"trimmed/test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- bin: integer (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mpercentileLimitsBinned\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, bin: double ... 1 more field]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- bin: double (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var percentileLimitsBinned = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/temp/exp_files/trimmed/percentileLimitsBinned\")\n",
    "percentileLimitsBinned.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: org.apache.spark.sql.DataFrame = [productid: string, predictedyQuantity: double]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- predictedyQuantity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = test.join(percentileLimitsBinned, Seq(PRODUCTID, \"bin\")).withColumn(\"predictedyQuantity\", col(\"count\") * col(\"value\")).groupBy(PRODUCTID).agg(sum(\"predictedyQuantity\") as \"predictedyQuantity\").transform(saveNLoadDF(\"trimmed/predictions\"))\n",
    "test.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mproductAttrs\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, similargrouplevel: string]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var productAttrs = ParquetToDF.getDF(\"/data/Archive/inventory/productAttributesLegosFNL\").select(PRODUCTID, SIMILAR_GROUP_LEVEL)\n",
    "productAttrs.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: org.apache.spark.sql.DataFrame = [productid: string, predictedyQuantity: double ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "test = test.join(productAttrs, Seq(PRODUCTID))\n",
    "DFToParquet.putDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_my_exp/predictions/ModelForEachBrickProductLevel/menShirts/predictions\", test.filter(col(SIMILAR_GROUP_LEVEL) === \"830216013\").drop(SIMILAR_GROUP_LEVEL))\n",
    "DFToParquet.putDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_my_exp/predictions/ModelForEachBrickProductLevel/womenKurtas/predictions\", test.filter(col(SIMILAR_GROUP_LEVEL) === \"830303011\").drop(SIMILAR_GROUP_LEVEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mthresholdColumns\u001b[0m: \u001b[1m\u001b[32m(columns: Array[String], minThreshold: Double, maxThreshold: Double)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\u001b[0m\n",
      "\u001b[1m\u001b[34msuffix\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = colorfamily_pricebucket_styletype_pattern_sleeve_brandname\n",
      "\u001b[1m\u001b[34mprodLevelSigma\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, stddev: double]\n",
      "\u001b[1m\u001b[34mqueryLevelSigma\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, stddev: double ... 1 more field]\n",
      "\u001b[1m\u001b[34mproductQueryMap\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, productid: string ... 2 more fields]\n",
      "\u001b[1m\u001b[34mconvertLocalToRequiredFormat\u001b[0m: \u001b[1m\u001b[32m(dateForWhichPredictionsAreMade: String, basePath: String, level: String, localFile: String, fileType: String)Unit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def thresholdColumns(columns: Array[String], minThreshold: Double = 0, maxThreshold: Double = Double.PositiveInfinity)(df: DataFrame): DataFrame = {\n",
    "\n",
    "    var scaledDF = df\n",
    "    columns.foreach(\n",
    "    column => {\n",
    "    scaledDF = scaledDF.withColumn(column,\n",
    "      when(col(column) > maxThreshold, lit(maxThreshold)).otherwise(col(column)))\n",
    "      .withColumn(column, when(col(column) < minThreshold, lit(minThreshold)).otherwise(col(column)))\n",
    "    })\n",
    "    scaledDF\n",
    "}\n",
    "\n",
    "var suffix: String = \"colorfamily_pricebucket_styletype_pattern_sleeve_brandname\"\n",
    "var prodLevelSigma = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/quantityStatsAcrossMonths/date_when_prediction_is_made=2023-04-16/productLevel/past12MonthsStats\").select(PRODUCTID, \"stddev\")\n",
    "var queryLevelSigma = ParquetToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/quantityStatsAcrossMonths/date_when_prediction_is_made=2023-04-16/queryLevel/suffix=$suffix/past12MonthsStats\").select(\"query\", \"stddev\", SIMILAR_GROUP_LEVEL)\n",
    "var productQueryMap = ParquetToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/queryToProductMap/date_when_prediction_is_made=2023-04-16/suffix=$suffix\")\n",
    "\n",
    "def convertLocalToRequiredFormat(dateForWhichPredictionsAreMade: String, basePath: String, level: String, localFile: String, fileType: String = \"csv\"): Unit = {\n",
    "    \n",
    "    var actual = ParquetToDF.getDF(s\"/data/Archive/bhavesh/inventoryPrediction/actualDataForPredictionPeriod/date=$dateForWhichPredictionsAreMade/sales\")\n",
    "    var fileName: String = if(level == \"product\"){\n",
    "       \"productLevelPredictions\"\n",
    "    } else{\n",
    "        \"queryAggregatedPredictions\"\n",
    "    }\n",
    "    def doConversion(brick: String, brickId: String): DataFrame = {\n",
    "        var localPredictionsPath = s\"$basePath/$brick/$localFile\"\n",
    "        var df = if(fileType==\"csv\"){\n",
    "            CSVToDF.getDF(localPredictionsPath, inferSchema = true)\n",
    "        }\n",
    "        else{\n",
    "            ParquetToDF.getDF(localPredictionsPath)\n",
    "        }\n",
    "\n",
    "        \n",
    "        df = df\n",
    "        .withColumn(SIMILAR_GROUP_LEVEL, lit(brickId))\n",
    "        .withColumn(\"model\", lit(\"lr\"))\n",
    "        .withColumn(\"predicted\", col(\"predictedyQuantity\"))\n",
    "        .select(PRODUCTID, SIMILAR_GROUP_LEVEL, \"predicted\", \"model\")\n",
    "        \n",
    "        if(level == \"product\") {\n",
    "            df.transform(thresholdColumns(Array(\"predicted\")))\n",
    "            .join(prodLevelSigma, Seq(PRODUCTID))\n",
    "            .withColumn(\"lowerBound\", col(\"predicted\") - (lit(2.0) * col(\"stddev\")))\n",
    "            .withColumn(\"upperBound\", col(\"predicted\") + (lit(2.0) * col(\"stddev\")))\n",
    "            .transform(thresholdColumns(Array(\"lowerBound\")))\n",
    "        }\n",
    "        else {\n",
    "            df.transform(thresholdColumns(Array(\"predicted\")))\n",
    "            .join(productQueryMap, Seq(SIMILAR_GROUP_LEVEL, PRODUCTID))\n",
    "            .groupBy(\"model\", \"query\", SIMILAR_GROUP_LEVEL)\n",
    "            .agg(sum(\"predicted\") as \"predicted\")\n",
    "            .join(queryLevelSigma, Seq(SIMILAR_GROUP_LEVEL, \"query\"))\n",
    "            .withColumn(\"lowerBound\", col(\"predicted\") - (lit(2.0) * col(\"stddev\")))\n",
    "            .withColumn(\"upperBound\", col(\"predicted\") + (lit(2.0) * col(\"stddev\")))\n",
    "            .transform(thresholdColumns(Array(\"lowerBound\")))\n",
    "        }\n",
    "    }\n",
    "    var menShirts = doConversion(\"menShirts\", \"830216013\")\n",
    "    DFToParquet.putDF(s\"$basePath/menShirts/$fileName\", menShirts, partitionedColumn=\"model\")\n",
    "    var womenKurtas = doConversion(\"womenKurtas\", \"830303011\")\n",
    "    DFToParquet.putDF(s\"$basePath/womenKurtas/$fileName\", womenKurtas, partitionedColumn=\"model\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mconvertLocal\u001b[0m: \u001b[1m\u001b[32m(exp: String, date: String, filename: String, format: String)Unit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def convertLocal(exp: String, date: String, filename: String, format: String){\n",
    "    convertLocalToRequiredFormat(date, s\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_$exp/predictions/ModelForEachBrickProductLevel\", \"product\", filename, format)\n",
    "    convertLocalToRequiredFormat(date, s\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_$exp/predictions/ModelForEachBrickProductLevel\", \"query\", filename, format)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convertLocal(\n",
    "    exp = \"my_exp\",\n",
    "    date = \"2023-04-16\",\n",
    "    filename = \"predictions\",\n",
    "    format = \"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine arima and ml predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mml_predictions_ms\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [similargrouplevel: string, query: string ... 4 more fields]\n",
      "\u001b[1m\u001b[34mml_predictions_wk\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [similargrouplevel: string, query: string ... 4 more fields]\n",
      "root\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- lowerBound: double (nullable = true)\n",
      " |-- upperBound: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- lowerBound: double (nullable = true)\n",
      " |-- upperBound: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var ml_predictions_ms = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_my_exp/predictions/ModelForEachBrickProductLevel/menShirts/queryAggregatedPredictions/model=lr\")\n",
    "var ml_predictions_wk = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_my_exp/predictions/ModelForEachBrickProductLevel/womenKurtas/queryAggregatedPredictions/model=lr\")\n",
    "\n",
    "ml_predictions_ms.printSchema\n",
    "ml_predictions_wk.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mml_predictions\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [similargrouplevel: string, query: string ... 1 more field]\n",
      "root\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var ml_predictions = ml_predictions_ms.select(SIMILAR_GROUP_LEVEL, \"query\", \"predicted\").union(ml_predictions_wk.select(SIMILAR_GROUP_LEVEL, \"query\", \"predicted\"))\n",
    "ml_predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml_predictions: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [similargrouplevel: string, query: string ... 1 more field]\n",
      "root\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DFToParquet.putDF(\"/data/Archive/bhavesh/inventoryPrediction/temp/ml_predictions\", ml_predictions)\n",
    "ml_predictions = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/temp/ml_predictions\")\n",
    "ml_predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mbaseline\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, predicted: double ... 4 more fields]\n",
      "root\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- lowerBound: double (nullable = true)\n",
      " |-- upperBound: double (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var baseline = ParquetToDF.getDF(\"/data/Archive/inventory/2023-04-15/queryProductMap_colorfamily_pricebucket_styletype_pattern_sleeve_brandname/past1MonthHistorical/queryPredictions\")\n",
    "baseline.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: org.apache.spark.sql.DataFrame = [similargrouplevel: int, query: string ... 1 more field]\n",
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = baseline.select(SIMILAR_GROUP_LEVEL, \"query\", \"predicted\")\n",
    "baseline.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34marima_predictions\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, predicted: double ... 5 more fields]\n",
      "root\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- avg: double (nullable = true)\n",
      " |-- lowerBound: double (nullable = true)\n",
      " |-- upperBound: double (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var arima_predictions = ParquetToDF.getDF(\"/data/Archive/inventory/2023-04-15/queryProductMap_colorfamily_pricebucket_styletype_pattern_sleeve_brandname/autoarima/queryPredictions\")\n",
    "arima_predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arima_predictions: org.apache.spark.sql.DataFrame = [similargrouplevel: int, query: string ... 1 more field]\n",
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arima_predictions = arima_predictions.select(SIMILAR_GROUP_LEVEL, \"query\", \"predicted\")\n",
    "arima_predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377851,318156)\n"
     ]
    }
   ],
   "source": [
    "println(ml_predictions.count, arima_predictions.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(337326,318156)\n"
     ]
    }
   ],
   "source": [
    "println(baseline.count, arima_predictions.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml_predictions: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [similargrouplevel: string, query: string ... 2 more fields]\n",
      "arima_predictions: org.apache.spark.sql.DataFrame = [similargrouplevel: int, query: string ... 2 more fields]\n",
      "\u001b[1m\u001b[34mpredictions\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [similargrouplevel: string, query: string ... 2 more fields]\n",
      "root\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- model: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_predictions = ml_predictions.withColumn(\"model\", lit(\"ml\"))\n",
    "arima_predictions = arima_predictions.withColumn(\"model\", lit(\"arima\"))\n",
    "\n",
    "var predictions = ml_predictions.select(arima_predictions.columns.map(col): _*).union(arima_predictions)\n",
    "predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: org.apache.spark.sql.DataFrame = [similargrouplevel: int, query: string ... 2 more fields]\n",
      "arima_predictions: org.apache.spark.sql.DataFrame = [similargrouplevel: int, query: string ... 2 more fields]\n",
      "\u001b[1m\u001b[34mpredictions\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [similargrouplevel: int, query: string ... 2 more fields]\n",
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- model: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = baseline.withColumn(\"model\", lit(\"baseline\"))\n",
    "arima_predictions = arima_predictions.withColumn(\"model\", lit(\"arima\"))\n",
    "\n",
    "var predictions = baseline.select(arima_predictions.columns.map(col): _*).union(arima_predictions)\n",
    "predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [similargrouplevel: int, query: string ... 1 more field]\n",
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions.groupBy(SIMILAR_GROUP_LEVEL, \"query\").agg(avg(\"predicted\") as \"predicted\")\n",
    "predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [similargrouplevel: int, query: string ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "DFToParquet.putDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_combined_arima_baseline/combinedPredictions2\", predictions)\n",
    "predictions = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_combined_arima_baseline/combinedPredictions2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------+\n",
      "|similargrouplevel|count(DISTINCT query)|\n",
      "+-----------------+---------------------+\n",
      "|830216013        |227431               |\n",
      "|830303011        |142073               |\n",
      "+-----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy(SIMILAR_GROUP_LEVEL).agg(countDistinct(\"query\")).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres29\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 369504\n"
     ]
    }
   ],
   "source": [
    "predictions.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mpredictions\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [similargrouplevel: int, query: string ... 1 more field]\n",
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var predictions = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_combined_arima_baseline/combinedPredictions2\")\n",
    "predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.sql.DataFrame = [similargrouplevel: int, query: string ... 4 more fields]\n",
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- lowerBound: double (nullable = true)\n",
      " |-- upperBound: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = (\n",
    "    predictions.join(queryLevelSigma, Seq(SIMILAR_GROUP_LEVEL, \"query\"))\n",
    "    .withColumn(\"lowerBound\", col(\"predicted\") - (lit(2.0) * col(\"stddev\")))\n",
    "    .withColumn(\"upperBound\", col(\"predicted\") + (lit(2.0) * col(\"stddev\")))\n",
    "    .transform(thresholdColumns(Array(\"lowerBound\")))\n",
    ")\n",
    "predictions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.sql.DataFrame = [query: string, predicted: double ... 4 more fields]\n",
      "root\n",
      " |-- query: string (nullable = true)\n",
      " |-- predicted: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- lowerBound: double (nullable = true)\n",
      " |-- upperBound: double (nullable = true)\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DFToParquet.putDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_combined_arima_baseline/combinedPredictions\", predictions, partitionedColumn=\"similargrouplevel\")\n",
    "HdfsUtils.delete(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_combined_arima_baseline/combinedPredictions2\")\n",
    "predictions = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_combined_arima_baseline/combinedPredictions\")\n",
    "predictions.printSchema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
