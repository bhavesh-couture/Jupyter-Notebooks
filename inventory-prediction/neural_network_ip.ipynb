{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jioapp/anaconda3/envs/api_env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/tmp/ipykernel_172410/75781886.py:11: FutureWarning: pyarrow.hdfs.connect is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  fs = pa.hdfs.connect()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "fs = pa.hdfs.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_percentage_error as mape, r2_score\n",
    "def metrics(actual, predicted):    \n",
    "    y_true = actual\n",
    "    y_pred = predicted\n",
    "    print(f\"rmse: {mse(y_true, y_pred)**(0.5)}\")\n",
    "    print(f\"mape: {mape(y_true, y_pred)}\")\n",
    "    print(f\"r2_score: {r2_score(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate = 0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.fc1(x)\n",
    "      x = self.relu1(x)\n",
    "      x = self.dropout1(x)\n",
    "      x = self.fc2(x)\n",
    "      x = self.relu2(x)\n",
    "      x = self.dropout2(x)\n",
    "      x = self.fc3(x)\n",
    "      x = self.sigmoid(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "  class _Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "      self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "      self.x = torch.tensor(x).float().to(self.device)\n",
    "      self.y = torch.tensor(y).float().reshape(-1, 1).to(self.device)\n",
    "    \n",
    "    def __len__(self, ):\n",
    "      return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "      x = self.x[idx]\n",
    "      y = self.y[idx]\n",
    "      return x, y\n",
    "\n",
    "  def __init__(self, neural_network, neural_network_layers_info):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self._neural_network = neural_network\n",
    "    self._neural_network_layers_info = neural_network_layers_info\n",
    "    self.model = self._neural_network(**self._neural_network_layers_info).to(self.device)\n",
    "    self._training_done = False\n",
    "    self._compile_done = False\n",
    "\n",
    "  def __clear(self, ):\n",
    "    self.model = self._neural_network(**self._neural_network_layers_info)\n",
    "    self._training_done = False\n",
    "    self._compile_done = False\n",
    "    self._validation_loss_history = []\n",
    "\n",
    "  def compile(self, criterion, optimizer, learning_rate):\n",
    "    if self._training_done:\n",
    "      raise Exception(\"Cannot compile an already trained model\")\n",
    "    \n",
    "    if self._compile_done:\n",
    "      print(\"Warn: Previously trained Model will be replaced.\")\n",
    "      self.__clear()\n",
    "\n",
    "    self.criterion = criterion\n",
    "    self._optimizer = optimizer\n",
    "    self._lr = learning_rate\n",
    "    self.optimizer = self._optimizer(self.model.parameters(), lr = self._lr)\n",
    "    self._compile_done = True\n",
    "\n",
    "  def _get_dataset_loader(self, x, y, batch_size):\n",
    "    dataset = self._Dataset(x, y)\n",
    "    dataset_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "    return dataset_loader\n",
    "  \n",
    "  def fit(self, x_train, y_train, num_epochs, batch_size = 32, x_val = None, y_val = None, early_stopping = None):\n",
    "    if not self._compile_done:\n",
    "      raise Exception(\"compile before training\")\n",
    "    \n",
    "    if self._training_done:\n",
    "      print(\"Warn: Previously trained Model will be replaced.\")\n",
    "      self.__clear()\n",
    "      self.compile(self.criterion, self._optimizer, self._lr)\n",
    "    \n",
    "    do_validation = (x_val is not None) and (y_val is not None)\n",
    "\n",
    "    train_data_loader = self._get_dataset_loader(x_train, y_train, batch_size)\n",
    "    if do_validation:\n",
    "      validation_data = self._Dataset(x_val, y_val)\n",
    "      if not early_stopping:\n",
    "        early_stopping = np.inf\n",
    "      self._fit_with_validation(train_data_loader, validation_data, epochs = num_epochs, early_stopping = early_stopping)\n",
    "    else:\n",
    "      self._fit_without_validation(train_data_loader, epochs = num_epochs)\n",
    "\n",
    "    self._training_done = True\n",
    "\n",
    "  def predict(self, x_test):\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "      x_test = torch.tensor(x_test).float().to(self.device)\n",
    "      y_test = self.model(x_test)\n",
    "    \n",
    "      return y_test.cpu().detach().numpy()\n",
    "    \n",
    "  def _fit_with_validation(self, train_data_loader, validation_data, epochs, early_stopping):\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    val_loss_history = []\n",
    "    best_model = None\n",
    "    for epoch in range(epochs):\n",
    "      if epoch - best_epoch > early_stopping:\n",
    "        print(\"Early stopping at epoch:\", epoch+1)\n",
    "        break\n",
    "      \n",
    "      self.model.train()\n",
    "      train_loss = []\n",
    "\n",
    "      for x, y in train_data_loader:\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        train_loss.append(float(loss.item()))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "      self.model.eval()\n",
    "      with torch.no_grad():\n",
    "        y_pred = self.model(validation_data.x)\n",
    "        val_loss = float(self.criterion(y_pred, validation_data.y))\n",
    "        val_loss_history.append(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "          print(f\"Best validation loss at epoch={epoch+1}, saving model\")\n",
    "          best_val_loss = val_loss\n",
    "          best_epoch = epoch + 1\n",
    "          best_model = self.model.state_dict().copy()\n",
    "      \n",
    "        print(f\"Epoch #{epoch + 1} - training loss: {sum(train_loss)/len(train_loss)} \\t validation loss: {val_loss}\")\n",
    "    self.model.load_state_dict(best_model)\n",
    "    self._validation_loss_history = val_loss_history\n",
    "\n",
    "  def _fit_without_validation(self, train_data_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "      self.model.train()\n",
    "      train_loss = []\n",
    "\n",
    "      for x, y in train_data_loader:\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        train_loss.append(float(loss.item()))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "      print(f\"Epoch #{epoch + 1} - training loss: {sum(train_loss)/len(train_loss)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train, val_size = 0.2):\n",
    "    train = train.sort_values(by=[\"yQuantity\"]).reset_index(drop=True)\n",
    "    val_indexes = np.array([])\n",
    "    for i in range(0, train.shape[0], 50):\n",
    "        left = i\n",
    "        right = min(i+50, train.shape[0])\n",
    "        cnt = (right - left) * val_size\n",
    "\n",
    "        # get cnt random numbers between [left, right)\n",
    "        idx = np.random.randint(left, right, int(cnt))\n",
    "        val_indexes = np.concatenate((val_indexes, idx))\n",
    "    \n",
    "    validation_data = train.iloc[val_indexes.astype(int)]\n",
    "    train_data = train[~train.index.isin(val_indexes.astype(int))]\n",
    "    return train_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with baseline model's features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/data/Archive/bhavesh/InventoryPrediction\"\n",
    "run_date = \"2023-04-30\"\n",
    "run_dir = f\"{base_dir}/{run_date}\"\n",
    "train_path = {\n",
    "    \"menShirts\": f\"{run_dir}/TransformedDataset/baseline/menShirts/data/train\",\n",
    "    \"womenKurtas\": f\"{run_dir}/TransformedDataset/baseline/womenKurtas/data/train\",\n",
    "}\n",
    "test_path = {\n",
    "    \"menShirts\": f\"{run_dir}/TransformedDataset/baseline/menShirts/data/test\",\n",
    "    \"womenKurtas\": f\"{run_dir}/TransformedDataset/baseline/womenKurtas/data/test\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(of):\n",
    "    train = pq.ParquetDataset(train_path[of], fs).read().to_pandas()\n",
    "    test = pq.ParquetDataset(test_path[of], fs).read().to_pandas()\n",
    "\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    train.info()\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(of, epochs = 200, lr = 0.001):\n",
    "    train, test = fetch_data(of)\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    cols = list(filter(lambda x: not (x.endswith(\"0\") or x.endswith(\"1\")), train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]).columns))\n",
    "    print(cols)\n",
    "    train.loc[:, cols] = scaler_x.fit_transform(train[cols])\n",
    "    train[\"yQuantity\"] = scaler_y.fit_transform(train[[\"yQuantity\"]])\n",
    "    print(scaler_y.data_min_, scaler_y.data_max_)\n",
    "\n",
    "    train, val = train_val_split(train)\n",
    "    print(train.shape, val.shape)\n",
    "\n",
    "    x_train, y_train = train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), train[\"yQuantity\"]\n",
    "    x_val, y_val = val.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), val[\"yQuantity\"]\n",
    "\n",
    "    neural_network = Model(NeuralNetwork, {\"input_size\": 33, \"output_size\": 1, \"hidden_size1\": 66, \"hidden_size2\": 33})\n",
    "    print(neural_network.device)\n",
    "    neural_network.compile(nn.BCELoss(), optim.Adam, lr)\n",
    "    neural_network.fit(x_train.values, y_train.values, epochs, x_val=x_val.values, y_val=y_val.values)\n",
    "\n",
    "    x_test = test.drop(columns=[\"productid\", \"similargrouplevel\"])\n",
    "    x_test.loc[:, cols] = scaler_x.fit_transform(x_test[cols])\n",
    "    y_pred = neural_network.predict(x_test.values).reshape(-1)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    predictions = pd.DataFrame({\"productid\": test[\"productid\"], \"predictedyQuantity\": y_pred})\n",
    "    print(predictions.head())\n",
    "\n",
    "    pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=f\"{run_dir}/experiments/exp_neural_network/predictions/ModelForEachBrickProductLevel/{of}/predictions\", filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(\"menShirts\", epochs = 500, lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(\"womenKurtas\", epochs = 500, lr = 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with 0-4 months sales, wishlist, availableQuantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/data/Archive/bhavesh/inventoryPrediction/temp/local\"\n",
    "train_path = {\n",
    "    \"menShirts\": f\"{base_dir}/menShirts/data/train\",\n",
    "    \"womenKurtas\": f\"{base_dir}/womenKurtas/data/train\",\n",
    "}\n",
    "test_path = {\n",
    "    \"menShirts\": f\"{base_dir}/menShirts/data/test\",\n",
    "    \"womenKurtas\": f\"{base_dir}/womenKurtas/data/test\",\n",
    "}\n",
    "\n",
    "def fetch_data2(of):\n",
    "    cols_to_read = [f\"{i}_sales\" for i in range(4)] + [f\"{i}_wishlist\" for i in range(4)] + [f\"{i}_availableQuantity\" for i in range(4)]\n",
    "    cols_to_read = [\"productid\", \"similargrouplevel\"] + cols_to_read\n",
    "    train = pq.ParquetDataset(train_path[of], fs).read(columns=cols_to_read + [\"yQuantity\"]).to_pandas()\n",
    "    test = pq.ParquetDataset(test_path[of], fs).read(columns=cols_to_read).to_pandas()\n",
    "\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    train.info()\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment2(of, epochs = 200, lr = 0.001):\n",
    "    train, test = fetch_data2(of)\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    cols = train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]).columns\n",
    "    print(cols)\n",
    "    train.loc[:, cols] = scaler_x.fit_transform(train[cols])\n",
    "    train[\"yQuantity\"] = scaler_y.fit_transform(train[[\"yQuantity\"]])\n",
    "    print(scaler_y.data_min_, scaler_y.data_max_)\n",
    "\n",
    "    train, val = train_val_split(train)\n",
    "    print(train.shape, val.shape)\n",
    "\n",
    "    x_train, y_train = train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), train[\"yQuantity\"]\n",
    "    x_val, y_val = val.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), val[\"yQuantity\"]\n",
    "\n",
    "    neural_network = Model(NeuralNetwork, {\"input_size\": 12, \"output_size\": 1, \"hidden_size1\": 24, \"hidden_size2\": 6})\n",
    "    print(neural_network.device)\n",
    "    neural_network.compile(nn.BCELoss(), optim.Adam, lr)\n",
    "    neural_network.fit(x_train.values, y_train.values, epochs, x_val=x_val.values, y_val=y_val.values)\n",
    "\n",
    "    x_test = test.drop(columns=[\"productid\", \"similargrouplevel\"])\n",
    "    x_test.loc[:, cols] = scaler_x.fit_transform(x_test[cols])\n",
    "    y_pred = neural_network.predict(x_test.values).reshape(-1)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    predictions = pd.DataFrame({\"productid\": test[\"productid\"], \"predictedyQuantity\": y_pred})\n",
    "    print(predictions.head())\n",
    "\n",
    "    pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=f\"{base_dir}/experiments/exp_neural_network_new/predictions/ModelForEachBrickProductLevel/{of}/predictions\", filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2(\"menShirts\", epochs = 200, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2(\"womenKurtas\", epochs = 200, lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Months Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_months_fetch(of, n_months):\n",
    "    base_dir = f\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/NMonthsAheadExperiment/{n_months}MonthsAhead\"\n",
    "    train_path = {\n",
    "        \"menShirts\": f\"{base_dir}/data/train/menShirts\",\n",
    "        \"womenKurtas\": f\"{base_dir}/data/train/womenKurtas\",\n",
    "    }\n",
    "    test_path = {\n",
    "        \"menShirts\": f\"{base_dir}/data/test/menShirts\",\n",
    "        \"womenKurtas\": f\"{base_dir}/data/test/womenKurtas\",\n",
    "    }\n",
    "    \n",
    "    train = pq.ParquetDataset(train_path[of], fs).read().to_pandas()\n",
    "    cols = train.drop(columns=[\"yQuantity\"]).columns\n",
    "    test = pq.ParquetDataset(test_path[of], fs).read().to_pandas()[cols]\n",
    "    train = train[list(cols) + [\"yQuantity\"]]\n",
    "\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    train.info()\n",
    "    test.info()\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment3(of, n_months, epochs = 200, lr = 0.001):\n",
    "    train, test = n_months_fetch(of, n_months)\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    cols = list(filter(lambda x: not (x.endswith(\"0\") or x.endswith(\"1\")), train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]).columns))\n",
    "    print(cols)\n",
    "    train.loc[:, cols] = scaler_x.fit_transform(train[cols])\n",
    "    train[\"yQuantity\"] = scaler_y.fit_transform(train[[\"yQuantity\"]])\n",
    "    print(scaler_y.data_min_, scaler_y.data_max_)\n",
    "\n",
    "    train, val = train_val_split(train)\n",
    "    print(train.shape, val.shape)\n",
    "\n",
    "    x_train, y_train = train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), train[\"yQuantity\"]\n",
    "    x_val, y_val = val.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), val[\"yQuantity\"]\n",
    "\n",
    "    input_size = x_train.shape[1]\n",
    "    hidden_size1 = input_size * 2\n",
    "    hidden_size2 = input_size\n",
    "    neural_network = Model(NeuralNetwork, {\"input_size\": input_size, \"output_size\": 1, \"hidden_size1\": hidden_size1, \"hidden_size2\": hidden_size2})\n",
    "    print(neural_network.device)\n",
    "    # print(neural_network.model)\n",
    "    neural_network.compile(nn.BCELoss(), optim.Adam, lr)\n",
    "    neural_network.fit(x_train.values, y_train.values, epochs, x_val=x_val.values, y_val=y_val.values)\n",
    "\n",
    "    x_test = test.drop(columns=[\"productid\", \"similargrouplevel\"])\n",
    "    x_test.loc[:, cols] = scaler_x.fit_transform(x_test[cols])\n",
    "    y_pred = neural_network.predict(x_test.values).reshape(-1)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    predictions = pd.DataFrame({\"productid\": test[\"productid\"], \"predictedyQuantity\": y_pred})\n",
    "    print(predictions.head())\n",
    "\n",
    "    path = f\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/NMonthsAheadExperiment/{n_months}MonthsAhead/predictions/{of}/predictions\"\n",
    "    pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=path, filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33469, 35)\n",
      "(97077, 34)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33469 entries, 0 to 33468\n",
      "Data columns (total 35 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   similargrouplevel       33469 non-null  object \n",
      " 1   productid               33469 non-null  object \n",
      " 2   0_sales                 33469 non-null  float64\n",
      " 3   1_sales                 33469 non-null  float64\n",
      " 4   2_sales                 33469 non-null  float64\n",
      " 5   3_sales                 33469 non-null  float64\n",
      " 6   4_sales                 33469 non-null  float64\n",
      " 7   5_sales                 33469 non-null  float64\n",
      " 8   6_sales                 33469 non-null  float64\n",
      " 9   7_sales                 33469 non-null  float64\n",
      " 10  8_sales                 33469 non-null  float64\n",
      " 11  9_sales                 33469 non-null  float64\n",
      " 12  10_sales                33469 non-null  float64\n",
      " 13  sales_avg_11_months     33469 non-null  float64\n",
      " 14  sales_std_11_months     33469 non-null  float64\n",
      " 15  totalPLPViews           33469 non-null  float64\n",
      " 16  totalPLPClicks          33469 non-null  float64\n",
      " 17  totalPDPCount           33469 non-null  float64\n",
      " 18  totalATC                33469 non-null  float64\n",
      " 19  totalReturns            33469 non-null  float64\n",
      " 20  totalUsers              33469 non-null  float64\n",
      " 21  totalAvailableQuantity  33469 non-null  float64\n",
      " 22  totalWishlist           33469 non-null  float64\n",
      " 23  pricebucket             33469 non-null  int32  \n",
      " 24  normalized_color_0      33469 non-null  float64\n",
      " 25  normalized_color_1      33469 non-null  float64\n",
      " 26  normalized_pattern_0    33469 non-null  float64\n",
      " 27  normalized_pattern_1    33469 non-null  float64\n",
      " 28  normalized_brand_0      33469 non-null  float64\n",
      " 29  normalized_brand_1      33469 non-null  float64\n",
      " 30  normalized_style_0      33469 non-null  float64\n",
      " 31  normalized_style_1      33469 non-null  float64\n",
      " 32  normalized_sleeve_0     33469 non-null  float64\n",
      " 33  normalized_sleeve_1     33469 non-null  float64\n",
      " 34  yQuantity               33469 non-null  float64\n",
      "dtypes: float64(32), int32(1), object(2)\n",
      "memory usage: 8.8+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97077 entries, 0 to 97076\n",
      "Data columns (total 34 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   similargrouplevel       97077 non-null  object \n",
      " 1   productid               97077 non-null  object \n",
      " 2   0_sales                 97077 non-null  float64\n",
      " 3   1_sales                 97077 non-null  float64\n",
      " 4   2_sales                 97077 non-null  float64\n",
      " 5   3_sales                 97077 non-null  float64\n",
      " 6   4_sales                 97077 non-null  float64\n",
      " 7   5_sales                 97077 non-null  float64\n",
      " 8   6_sales                 97077 non-null  float64\n",
      " 9   7_sales                 97077 non-null  float64\n",
      " 10  8_sales                 97077 non-null  float64\n",
      " 11  9_sales                 97077 non-null  float64\n",
      " 12  10_sales                97077 non-null  float64\n",
      " 13  sales_avg_11_months     97077 non-null  float64\n",
      " 14  sales_std_11_months     97077 non-null  float64\n",
      " 15  totalPLPViews           97077 non-null  float64\n",
      " 16  totalPLPClicks          97077 non-null  float64\n",
      " 17  totalPDPCount           97077 non-null  float64\n",
      " 18  totalATC                97077 non-null  float64\n",
      " 19  totalReturns            97077 non-null  float64\n",
      " 20  totalUsers              97077 non-null  float64\n",
      " 21  totalAvailableQuantity  97077 non-null  float64\n",
      " 22  totalWishlist           97077 non-null  float64\n",
      " 23  pricebucket             97077 non-null  int32  \n",
      " 24  normalized_color_0      97077 non-null  float64\n",
      " 25  normalized_color_1      97077 non-null  float64\n",
      " 26  normalized_pattern_0    97077 non-null  float64\n",
      " 27  normalized_pattern_1    97077 non-null  float64\n",
      " 28  normalized_brand_0      97077 non-null  float64\n",
      " 29  normalized_brand_1      97077 non-null  float64\n",
      " 30  normalized_style_0      97077 non-null  float64\n",
      " 31  normalized_style_1      97077 non-null  float64\n",
      " 32  normalized_sleeve_0     97077 non-null  float64\n",
      " 33  normalized_sleeve_1     97077 non-null  float64\n",
      "dtypes: float64(31), int32(1), object(2)\n",
      "memory usage: 24.8+ MB\n",
      "['0_sales', '1_sales', '2_sales', '3_sales', '4_sales', '5_sales', '6_sales', '7_sales', '8_sales', '9_sales', '10_sales', 'sales_avg_11_months', 'sales_std_11_months', 'totalPLPViews', 'totalPLPClicks', 'totalPDPCount', 'totalATC', 'totalReturns', 'totalUsers', 'totalAvailableQuantity', 'totalWishlist', 'pricebucket']\n",
      "[1.] [7795.]\n",
      "(27338, 35) (6693, 35)\n",
      "cuda:0\n",
      "Best validation loss at epoch=1, saving model\n",
      "Epoch #1 - training loss: 0.0902839484204706 \t validation loss: 0.022865144535899162\n",
      "Best validation loss at epoch=2, saving model\n",
      "Epoch #2 - training loss: 0.02600051575248725 \t validation loss: 0.021042363718152046\n",
      "Best validation loss at epoch=3, saving model\n",
      "Epoch #3 - training loss: 0.023927479879865182 \t validation loss: 0.020102383568882942\n",
      "Epoch #4 - training loss: 0.02337357102930371 \t validation loss: 0.02013249881565571\n",
      "Best validation loss at epoch=5, saving model\n",
      "Epoch #5 - training loss: 0.022813413907235066 \t validation loss: 0.01976224035024643\n",
      "Best validation loss at epoch=6, saving model\n",
      "Epoch #6 - training loss: 0.022634624312708122 \t validation loss: 0.01932431012392044\n",
      "Epoch #7 - training loss: 0.02250060367733851 \t validation loss: 0.019496869295835495\n",
      "Epoch #8 - training loss: 0.022115897491958157 \t validation loss: 0.01969306543469429\n",
      "Best validation loss at epoch=9, saving model\n",
      "Epoch #9 - training loss: 0.02177673367351573 \t validation loss: 0.019215676933526993\n",
      "Epoch #10 - training loss: 0.02155065289545865 \t validation loss: 0.01924675889313221\n",
      "Best validation loss at epoch=11, saving model\n",
      "Epoch #11 - training loss: 0.021210384355046605 \t validation loss: 0.01917273923754692\n",
      "Best validation loss at epoch=12, saving model\n",
      "Epoch #12 - training loss: 0.021151677171527397 \t validation loss: 0.018689516931772232\n",
      "Epoch #13 - training loss: 0.020953521914932816 \t validation loss: 0.018862519413232803\n",
      "Best validation loss at epoch=14, saving model\n",
      "Epoch #14 - training loss: 0.020894505300578733 \t validation loss: 0.018603896722197533\n",
      "Best validation loss at epoch=15, saving model\n",
      "Epoch #15 - training loss: 0.020590095193260535 \t validation loss: 0.018520819023251534\n",
      "Best validation loss at epoch=16, saving model\n",
      "Epoch #16 - training loss: 0.0205508873833941 \t validation loss: 0.018498212099075317\n",
      "Best validation loss at epoch=17, saving model\n",
      "Epoch #17 - training loss: 0.02040417573525978 \t validation loss: 0.018371663987636566\n",
      "Best validation loss at epoch=18, saving model\n",
      "Epoch #18 - training loss: 0.020312187841502335 \t validation loss: 0.018296845257282257\n",
      "Epoch #19 - training loss: 0.020107026879328894 \t validation loss: 0.01847226172685623\n",
      "Best validation loss at epoch=20, saving model\n",
      "Epoch #20 - training loss: 0.020005146209455905 \t validation loss: 0.018284820020198822\n",
      "Epoch #21 - training loss: 0.019887585072959194 \t validation loss: 0.01831444725394249\n",
      "Best validation loss at epoch=22, saving model\n",
      "Epoch #22 - training loss: 0.019823692999104043 \t validation loss: 0.01823076605796814\n",
      "Best validation loss at epoch=23, saving model\n",
      "Epoch #23 - training loss: 0.019852331587041465 \t validation loss: 0.018100503832101822\n",
      "Best validation loss at epoch=24, saving model\n",
      "Epoch #24 - training loss: 0.019831952522781436 \t validation loss: 0.018082065507769585\n",
      "Best validation loss at epoch=25, saving model\n",
      "Epoch #25 - training loss: 0.019691379661224695 \t validation loss: 0.018062736839056015\n",
      "Epoch #26 - training loss: 0.019435853681546707 \t validation loss: 0.01811339519917965\n",
      "Best validation loss at epoch=27, saving model\n",
      "Epoch #27 - training loss: 0.01947753531536615 \t validation loss: 0.017993934452533722\n",
      "Best validation loss at epoch=28, saving model\n",
      "Epoch #28 - training loss: 0.01954455995131764 \t validation loss: 0.017972832545638084\n",
      "Epoch #29 - training loss: 0.01951589377976483 \t validation loss: 0.018067508935928345\n",
      "Epoch #30 - training loss: 0.01936932224169618 \t validation loss: 0.01797921396791935\n",
      "Best validation loss at epoch=31, saving model\n",
      "Epoch #31 - training loss: 0.019284123168774037 \t validation loss: 0.017880339175462723\n",
      "Epoch #32 - training loss: 0.01928861754721759 \t validation loss: 0.018070295453071594\n",
      "Epoch #33 - training loss: 0.01919332555379605 \t validation loss: 0.017915427684783936\n",
      "Epoch #34 - training loss: 0.01917288341751405 \t validation loss: 0.017920181155204773\n",
      "Best validation loss at epoch=35, saving model\n",
      "Epoch #35 - training loss: 0.01917697241436036 \t validation loss: 0.017845874652266502\n",
      "Epoch #36 - training loss: 0.019107614732478295 \t validation loss: 0.0178654994815588\n",
      "Epoch #37 - training loss: 0.019109403817593806 \t validation loss: 0.017894161865115166\n",
      "Best validation loss at epoch=38, saving model\n",
      "Epoch #38 - training loss: 0.01910962757547615 \t validation loss: 0.017837369814515114\n",
      "Epoch #39 - training loss: 0.018981408223671187 \t validation loss: 0.01789136789739132\n",
      "Epoch #40 - training loss: 0.019081919994946805 \t validation loss: 0.01789283938705921\n",
      "Best validation loss at epoch=41, saving model\n",
      "Epoch #41 - training loss: 0.0190426739337865 \t validation loss: 0.017761511728167534\n",
      "Epoch #42 - training loss: 0.01897736069630718 \t validation loss: 0.017863351851701736\n",
      "Epoch #43 - training loss: 0.018943522402207504 \t validation loss: 0.017780879512429237\n",
      "Epoch #44 - training loss: 0.01890623159881079 \t validation loss: 0.017845679074525833\n",
      "Epoch #45 - training loss: 0.018984598147986018 \t validation loss: 0.017853733152151108\n",
      "Epoch #46 - training loss: 0.018878027422416878 \t validation loss: 0.017914053052663803\n",
      "Epoch #47 - training loss: 0.018808264455995774 \t validation loss: 0.017874836921691895\n",
      "Epoch #48 - training loss: 0.018851774342824864 \t validation loss: 0.017843641340732574\n",
      "Epoch #49 - training loss: 0.01872440255672368 \t validation loss: 0.017770538106560707\n",
      "Epoch #50 - training loss: 0.018847806649653357 \t validation loss: 0.018041187897324562\n",
      "Epoch #51 - training loss: 0.01885664482127692 \t validation loss: 0.017795398831367493\n",
      "Epoch #52 - training loss: 0.018722251755298843 \t validation loss: 0.017826685681939125\n",
      "Epoch #53 - training loss: 0.01879557896137185 \t validation loss: 0.017802434042096138\n",
      "Epoch #54 - training loss: 0.018818132886859375 \t validation loss: 0.01778239943087101\n",
      "Epoch #55 - training loss: 0.01876105775072979 \t validation loss: 0.017767125740647316\n",
      "Epoch #56 - training loss: 0.018686000223923693 \t validation loss: 0.017854025587439537\n",
      "Epoch #57 - training loss: 0.01873634099323451 \t validation loss: 0.01779799908399582\n",
      "Best validation loss at epoch=58, saving model\n",
      "Epoch #58 - training loss: 0.018751035042573857 \t validation loss: 0.017728744074702263\n",
      "Epoch #59 - training loss: 0.01873833702853172 \t validation loss: 0.017814144492149353\n",
      "Epoch #60 - training loss: 0.018711656496415145 \t validation loss: 0.017793932929635048\n",
      "Epoch #61 - training loss: 0.018736820162516044 \t validation loss: 0.01778842695057392\n",
      "Best validation loss at epoch=62, saving model\n",
      "Epoch #62 - training loss: 0.018521925598596695 \t validation loss: 0.017683209851384163\n",
      "Epoch #63 - training loss: 0.018814538615326063 \t validation loss: 0.01779639534652233\n",
      "Epoch #64 - training loss: 0.01872320903412806 \t validation loss: 0.01780685968697071\n",
      "Epoch #65 - training loss: 0.01865576852965812 \t validation loss: 0.017792485654354095\n",
      "Epoch #66 - training loss: 0.018695340491013398 \t validation loss: 0.017802748829126358\n",
      "Epoch #67 - training loss: 0.01866401683275241 \t validation loss: 0.01775786653161049\n",
      "Epoch #68 - training loss: 0.01858297319056584 \t validation loss: 0.01772211492061615\n",
      "Epoch #69 - training loss: 0.01866400122982766 \t validation loss: 0.017851145938038826\n",
      "Epoch #70 - training loss: 0.018577055371447142 \t validation loss: 0.017811406403779984\n",
      "Epoch #71 - training loss: 0.01864333291411557 \t validation loss: 0.01777702197432518\n",
      "Epoch #72 - training loss: 0.018642098142872335 \t validation loss: 0.017817895859479904\n",
      "Epoch #73 - training loss: 0.018570919554959764 \t validation loss: 0.017764940857887268\n",
      "Epoch #74 - training loss: 0.018544505085835376 \t validation loss: 0.017840156331658363\n",
      "Epoch #75 - training loss: 0.018736298794997747 \t validation loss: 0.017872270196676254\n",
      "Epoch #76 - training loss: 0.018543869741173762 \t validation loss: 0.017783109098672867\n",
      "Epoch #77 - training loss: 0.018681964216223836 \t validation loss: 0.01785917766392231\n",
      "Epoch #78 - training loss: 0.018615420475608692 \t validation loss: 0.01777571067214012\n",
      "Epoch #79 - training loss: 0.01857391421751837 \t validation loss: 0.01777394488453865\n",
      "Epoch #80 - training loss: 0.01849804684057378 \t validation loss: 0.017771456390619278\n",
      "Epoch #81 - training loss: 0.018557888757223723 \t validation loss: 0.01784268580377102\n",
      "Epoch #82 - training loss: 0.018533628090967445 \t validation loss: 0.017803717404603958\n",
      "Epoch #83 - training loss: 0.018496948933944244 \t validation loss: 0.017825547605752945\n",
      "Epoch #84 - training loss: 0.018560375136582822 \t validation loss: 0.01779101975262165\n",
      "Epoch #85 - training loss: 0.018492394430667546 \t validation loss: 0.017791718244552612\n",
      "Epoch #86 - training loss: 0.018481288025639864 \t validation loss: 0.01779504492878914\n",
      "Epoch #87 - training loss: 0.01850869906226509 \t validation loss: 0.017822664231061935\n",
      "Epoch #88 - training loss: 0.018554734238979582 \t validation loss: 0.017686504870653152\n",
      "Epoch #89 - training loss: 0.018531951066368287 \t validation loss: 0.017687156796455383\n",
      "Epoch #90 - training loss: 0.018599721229585014 \t validation loss: 0.01779787987470627\n",
      "Epoch #91 - training loss: 0.018590393469644934 \t validation loss: 0.017726821824908257\n",
      "Epoch #92 - training loss: 0.01855915057308213 \t validation loss: 0.01785285398364067\n",
      "Epoch #93 - training loss: 0.018460467638801215 \t validation loss: 0.0178176611661911\n",
      "Epoch #94 - training loss: 0.018453572139545262 \t validation loss: 0.017706630751490593\n",
      "Epoch #95 - training loss: 0.018533233629236437 \t validation loss: 0.017708811908960342\n",
      "Epoch #96 - training loss: 0.018509218754984038 \t validation loss: 0.017834292724728584\n",
      "Epoch #97 - training loss: 0.018509202056610195 \t validation loss: 0.017757827416062355\n",
      "Epoch #98 - training loss: 0.018448831416465847 \t validation loss: 0.017788343131542206\n",
      "Epoch #99 - training loss: 0.01851212934087046 \t validation loss: 0.017728546634316444\n",
      "Epoch #100 - training loss: 0.018438727853370425 \t validation loss: 0.017811568453907967\n",
      "Epoch #101 - training loss: 0.01837165869087574 \t validation loss: 0.017895648255944252\n",
      "Epoch #102 - training loss: 0.018545828775210476 \t validation loss: 0.01783541776239872\n",
      "Epoch #103 - training loss: 0.018543804028405168 \t validation loss: 0.017787719145417213\n",
      "Epoch #104 - training loss: 0.018467460070307033 \t validation loss: 0.017699159681797028\n",
      "Epoch #105 - training loss: 0.01842040747684368 \t validation loss: 0.017746707424521446\n",
      "Epoch #106 - training loss: 0.018423773410492325 \t validation loss: 0.017688140273094177\n",
      "Best validation loss at epoch=107, saving model\n",
      "Epoch #107 - training loss: 0.01843846546195936 \t validation loss: 0.017678575590252876\n",
      "Epoch #108 - training loss: 0.01847421871612734 \t validation loss: 0.017897535115480423\n",
      "Epoch #109 - training loss: 0.018385330359047974 \t validation loss: 0.017841234803199768\n",
      "Best validation loss at epoch=110, saving model\n",
      "Epoch #110 - training loss: 0.018393217372779998 \t validation loss: 0.01765848509967327\n",
      "Epoch #111 - training loss: 0.018414159632828827 \t validation loss: 0.017717277631163597\n",
      "Epoch #112 - training loss: 0.018353304468843413 \t validation loss: 0.017687397077679634\n",
      "Epoch #113 - training loss: 0.018403621270229705 \t validation loss: 0.01775278151035309\n",
      "Epoch #114 - training loss: 0.018379917333940746 \t validation loss: 0.017711490392684937\n",
      "Epoch #115 - training loss: 0.018370293224758232 \t validation loss: 0.01782018505036831\n",
      "Epoch #116 - training loss: 0.018342309300427868 \t validation loss: 0.017739906907081604\n",
      "Epoch #117 - training loss: 0.018438080886261105 \t validation loss: 0.017768392339348793\n",
      "Epoch #118 - training loss: 0.018503517838330194 \t validation loss: 0.017744166776537895\n",
      "Epoch #119 - training loss: 0.018275016396408966 \t validation loss: 0.017669331282377243\n",
      "Epoch #120 - training loss: 0.018431341136801352 \t validation loss: 0.01774860918521881\n",
      "Epoch #121 - training loss: 0.018351834373705665 \t validation loss: 0.01772790402173996\n",
      "Epoch #122 - training loss: 0.01854275495677628 \t validation loss: 0.017777075991034508\n",
      "Epoch #123 - training loss: 0.01841937912550626 \t validation loss: 0.017745209857821465\n",
      "Epoch #124 - training loss: 0.01835608762302883 \t validation loss: 0.017680699005723\n",
      "Best validation loss at epoch=125, saving model\n",
      "Epoch #125 - training loss: 0.018373088322828503 \t validation loss: 0.017634714022278786\n",
      "Epoch #126 - training loss: 0.018331605140872266 \t validation loss: 0.017680499702692032\n",
      "Epoch #127 - training loss: 0.018360360766558163 \t validation loss: 0.01771179400384426\n",
      "Epoch #128 - training loss: 0.01830815987806523 \t validation loss: 0.01778038963675499\n",
      "Epoch #129 - training loss: 0.01828139514822635 \t validation loss: 0.017743859440088272\n",
      "Epoch #130 - training loss: 0.01847686261277803 \t validation loss: 0.017790649086236954\n",
      "Epoch #131 - training loss: 0.018450724499723254 \t validation loss: 0.017697429284453392\n",
      "Epoch #132 - training loss: 0.018427713949721855 \t validation loss: 0.01777207851409912\n",
      "Epoch #133 - training loss: 0.018325814324053794 \t validation loss: 0.01779010146856308\n",
      "Epoch #134 - training loss: 0.018478816486916894 \t validation loss: 0.017667820677161217\n",
      "Epoch #135 - training loss: 0.01841550557334325 \t validation loss: 0.017751706764101982\n",
      "Epoch #136 - training loss: 0.01828691179796495 \t validation loss: 0.017694739624857903\n",
      "Epoch #137 - training loss: 0.01840891622920767 \t validation loss: 0.01779419369995594\n",
      "Epoch #138 - training loss: 0.01838310271042423 \t validation loss: 0.01775081641972065\n",
      "Epoch #139 - training loss: 0.01833918295414634 \t validation loss: 0.017727671191096306\n",
      "Epoch #140 - training loss: 0.018409478477588406 \t validation loss: 0.017679039388895035\n",
      "Epoch #141 - training loss: 0.018288142317186264 \t validation loss: 0.01773892343044281\n",
      "Epoch #142 - training loss: 0.018406185576151316 \t validation loss: 0.017718061804771423\n",
      "Epoch #143 - training loss: 0.01855668991142164 \t validation loss: 0.0177823044359684\n",
      "Epoch #144 - training loss: 0.018309619234911922 \t validation loss: 0.017754197120666504\n",
      "Epoch #145 - training loss: 0.018319151666074274 \t validation loss: 0.01775730960071087\n",
      "Epoch #146 - training loss: 0.018378185829969966 \t validation loss: 0.017836546525359154\n",
      "Epoch #147 - training loss: 0.018289293359640104 \t validation loss: 0.0177322905510664\n",
      "Epoch #148 - training loss: 0.0184822094935348 \t validation loss: 0.01789204590022564\n",
      "Epoch #149 - training loss: 0.0183267963742641 \t validation loss: 0.01776975579559803\n",
      "Epoch #150 - training loss: 0.018476983330559325 \t validation loss: 0.017837820574641228\n",
      "Epoch #151 - training loss: 0.01839175213300634 \t validation loss: 0.017704179510474205\n",
      "Epoch #152 - training loss: 0.018396769848759056 \t validation loss: 0.01773468777537346\n",
      "Epoch #153 - training loss: 0.018334599779984223 \t validation loss: 0.017751773819327354\n",
      "Epoch #154 - training loss: 0.01842591527465565 \t validation loss: 0.017671803012490273\n",
      "Epoch #155 - training loss: 0.018374626560734294 \t validation loss: 0.017781194299459457\n",
      "Epoch #156 - training loss: 0.01835940915200231 \t validation loss: 0.017754677683115005\n",
      "Epoch #157 - training loss: 0.018300378868799108 \t validation loss: 0.017807623371481895\n",
      "Epoch #158 - training loss: 0.018305403850012975 \t validation loss: 0.01770693250000477\n",
      "Epoch #159 - training loss: 0.018317068416071267 \t validation loss: 0.017790092155337334\n",
      "Epoch #160 - training loss: 0.01833425975018628 \t validation loss: 0.01767614670097828\n",
      "Epoch #161 - training loss: 0.01834798757116631 \t validation loss: 0.017748411744832993\n",
      "Best validation loss at epoch=162, saving model\n",
      "Epoch #162 - training loss: 0.018196534347421637 \t validation loss: 0.017627781257033348\n",
      "Epoch #163 - training loss: 0.018292283279705678 \t validation loss: 0.017689965665340424\n",
      "Epoch #164 - training loss: 0.018404733847916143 \t validation loss: 0.01766444370150566\n",
      "Best validation loss at epoch=165, saving model\n",
      "Epoch #165 - training loss: 0.018248804459409393 \t validation loss: 0.017589682713150978\n",
      "Epoch #166 - training loss: 0.018336429434067263 \t validation loss: 0.017731668427586555\n",
      "Epoch #167 - training loss: 0.018520204118612184 \t validation loss: 0.01767263561487198\n",
      "Epoch #168 - training loss: 0.018325193618823375 \t validation loss: 0.017794683575630188\n",
      "Epoch #169 - training loss: 0.01827302482360821 \t validation loss: 0.01765846461057663\n",
      "Best validation loss at epoch=170, saving model\n",
      "Epoch #170 - training loss: 0.018424417218302465 \t validation loss: 0.017568333074450493\n",
      "Epoch #171 - training loss: 0.018409588731349874 \t validation loss: 0.01766238547861576\n",
      "Epoch #172 - training loss: 0.018514201071209317 \t validation loss: 0.017764737829566002\n",
      "Epoch #173 - training loss: 0.01829176816371743 \t validation loss: 0.01764249987900257\n",
      "Epoch #174 - training loss: 0.01824415318555563 \t validation loss: 0.017744872719049454\n",
      "Epoch #175 - training loss: 0.018322072262315196 \t validation loss: 0.017648570239543915\n",
      "Epoch #176 - training loss: 0.018284409113917544 \t validation loss: 0.017673982307314873\n",
      "Epoch #177 - training loss: 0.018189619891552704 \t validation loss: 0.01773941144347191\n",
      "Epoch #178 - training loss: 0.018280917628116435 \t validation loss: 0.017646756023168564\n",
      "Epoch #179 - training loss: 0.018236970464073 \t validation loss: 0.017660988494753838\n",
      "Epoch #180 - training loss: 0.01835971616756005 \t validation loss: 0.01769392378628254\n",
      "Epoch #181 - training loss: 0.01846653014226256 \t validation loss: 0.01775374263525009\n",
      "Epoch #182 - training loss: 0.01827184831482561 \t validation loss: 0.017745541408658028\n",
      "Epoch #183 - training loss: 0.018363325102789114 \t validation loss: 0.017737019807100296\n",
      "Epoch #184 - training loss: 0.01839922838822548 \t validation loss: 0.01765141636133194\n",
      "Epoch #185 - training loss: 0.01836619255857093 \t validation loss: 0.01768413931131363\n",
      "Epoch #186 - training loss: 0.018205491291572812 \t validation loss: 0.017774410545825958\n",
      "Epoch #187 - training loss: 0.018337480356072706 \t validation loss: 0.017809923738241196\n",
      "Epoch #188 - training loss: 0.018186087789345506 \t validation loss: 0.017778852954506874\n",
      "Epoch #189 - training loss: 0.018435924223174867 \t validation loss: 0.017777670174837112\n",
      "Epoch #190 - training loss: 0.018237760116834742 \t validation loss: 0.017738737165927887\n",
      "Epoch #191 - training loss: 0.018214770033573885 \t validation loss: 0.017690230160951614\n",
      "Epoch #192 - training loss: 0.01824137471088446 \t validation loss: 0.017830410972237587\n",
      "Epoch #193 - training loss: 0.018348818159590212 \t validation loss: 0.0177057683467865\n",
      "Epoch #194 - training loss: 0.018322105540193843 \t validation loss: 0.017765160650014877\n",
      "Epoch #195 - training loss: 0.018270999718287335 \t validation loss: 0.01775183714926243\n",
      "Epoch #196 - training loss: 0.018331047377071454 \t validation loss: 0.017696574330329895\n",
      "Epoch #197 - training loss: 0.018209638161283747 \t validation loss: 0.017839202657341957\n",
      "Epoch #198 - training loss: 0.018287422803974333 \t validation loss: 0.017749259248375893\n",
      "Epoch #199 - training loss: 0.018403526064579918 \t validation loss: 0.01767568476498127\n",
      "Epoch #200 - training loss: 0.01822766731275067 \t validation loss: 0.01783803477883339\n",
      "Epoch #201 - training loss: 0.01823028346629425 \t validation loss: 0.01784186251461506\n",
      "Epoch #202 - training loss: 0.018282300148942273 \t validation loss: 0.01770920492708683\n",
      "Epoch #203 - training loss: 0.018282609493896994 \t validation loss: 0.017697855830192566\n",
      "Epoch #204 - training loss: 0.01826511190051488 \t validation loss: 0.01767439767718315\n",
      "Epoch #205 - training loss: 0.018303745602905333 \t validation loss: 0.017731130123138428\n",
      "Epoch #206 - training loss: 0.01831366139217618 \t validation loss: 0.01771913468837738\n",
      "Epoch #207 - training loss: 0.018161343772398347 \t validation loss: 0.017658904194831848\n",
      "Epoch #208 - training loss: 0.018187238872967294 \t validation loss: 0.017747143283486366\n",
      "Epoch #209 - training loss: 0.018340331731817795 \t validation loss: 0.01771850883960724\n",
      "Epoch #210 - training loss: 0.018230010595868408 \t validation loss: 0.017745204269886017\n",
      "Epoch #211 - training loss: 0.018204945079607134 \t validation loss: 0.017687546089291573\n",
      "Epoch #212 - training loss: 0.018150961726677083 \t validation loss: 0.01766757294535637\n",
      "Epoch #213 - training loss: 0.01827135024045919 \t validation loss: 0.01775996945798397\n",
      "Epoch #214 - training loss: 0.018224631230874682 \t validation loss: 0.017685959115624428\n",
      "Epoch #215 - training loss: 0.018309458495342375 \t validation loss: 0.017741987481713295\n",
      "Epoch #216 - training loss: 0.018138866392847595 \t validation loss: 0.017702898010611534\n",
      "Epoch #217 - training loss: 0.018261114136747737 \t validation loss: 0.017703991383314133\n",
      "Epoch #218 - training loss: 0.01821934824086403 \t validation loss: 0.01769598387181759\n",
      "Epoch #219 - training loss: 0.01837214384314144 \t validation loss: 0.01773039810359478\n",
      "Epoch #220 - training loss: 0.018294953223671195 \t validation loss: 0.01775153912603855\n",
      "Epoch #221 - training loss: 0.018229446584081814 \t validation loss: 0.017689112573862076\n",
      "Epoch #222 - training loss: 0.01832974572236316 \t validation loss: 0.017678244039416313\n",
      "Epoch #223 - training loss: 0.018304779910781274 \t validation loss: 0.017779842019081116\n",
      "Epoch #224 - training loss: 0.018199577907414395 \t validation loss: 0.017680732533335686\n",
      "Epoch #225 - training loss: 0.01827798007368012 \t validation loss: 0.017592808231711388\n",
      "Epoch #226 - training loss: 0.018455961562662583 \t validation loss: 0.017770854756236076\n",
      "Epoch #227 - training loss: 0.018285818409449497 \t validation loss: 0.017651991918683052\n",
      "Epoch #228 - training loss: 0.01820170186570163 \t validation loss: 0.017684772610664368\n",
      "Epoch #229 - training loss: 0.018406017642551096 \t validation loss: 0.01776677928864956\n",
      "Epoch #230 - training loss: 0.01812068578668279 \t validation loss: 0.01763235591351986\n",
      "Epoch #231 - training loss: 0.018166549525068815 \t validation loss: 0.01768365502357483\n",
      "Epoch #232 - training loss: 0.018295481596011187 \t validation loss: 0.01766539178788662\n",
      "Epoch #233 - training loss: 0.018250601086438365 \t validation loss: 0.01779639720916748\n",
      "Epoch #234 - training loss: 0.018242390865331356 \t validation loss: 0.017752237617969513\n",
      "Epoch #235 - training loss: 0.01825283952160963 \t validation loss: 0.017664406448602676\n",
      "Epoch #236 - training loss: 0.018231301932689455 \t validation loss: 0.01768987812101841\n",
      "Epoch #237 - training loss: 0.018221982042370605 \t validation loss: 0.01761506125330925\n",
      "Epoch #238 - training loss: 0.018317269063740097 \t validation loss: 0.017625845968723297\n",
      "Epoch #239 - training loss: 0.018164159469709968 \t validation loss: 0.017683815211057663\n",
      "Epoch #240 - training loss: 0.018315810803705673 \t validation loss: 0.017640531063079834\n",
      "Epoch #241 - training loss: 0.018260794871953475 \t validation loss: 0.017624540254473686\n",
      "Epoch #242 - training loss: 0.018211698648138125 \t validation loss: 0.017680611461400986\n",
      "Epoch #243 - training loss: 0.018381636412007258 \t validation loss: 0.017622588202357292\n",
      "Epoch #244 - training loss: 0.018202889131314494 \t validation loss: 0.01765740104019642\n",
      "Epoch #245 - training loss: 0.018266101799626122 \t validation loss: 0.01773877814412117\n",
      "Epoch #246 - training loss: 0.018221744361270296 \t validation loss: 0.01771496795117855\n",
      "Epoch #247 - training loss: 0.018214472985566884 \t validation loss: 0.01772451400756836\n",
      "Epoch #248 - training loss: 0.018217924435947972 \t validation loss: 0.01766422763466835\n",
      "Epoch #249 - training loss: 0.018183851293975213 \t validation loss: 0.017791438847780228\n",
      "Epoch #250 - training loss: 0.01831389459414927 \t validation loss: 0.017718743532896042\n",
      "Epoch #251 - training loss: 0.018163261742088987 \t validation loss: 0.01762271672487259\n",
      "Epoch #252 - training loss: 0.01821155635317225 \t validation loss: 0.017686888575553894\n",
      "Epoch #253 - training loss: 0.018361706385908068 \t validation loss: 0.017695410177111626\n",
      "Epoch #254 - training loss: 0.01818668927377923 \t validation loss: 0.017714574933052063\n",
      "Epoch #255 - training loss: 0.018127736905415213 \t validation loss: 0.017685389146208763\n",
      "Epoch #256 - training loss: 0.01811444605248097 \t validation loss: 0.017708828672766685\n",
      "Epoch #257 - training loss: 0.018282253235748662 \t validation loss: 0.017771843820810318\n",
      "Epoch #258 - training loss: 0.01836477188269152 \t validation loss: 0.017715953290462494\n",
      "Epoch #259 - training loss: 0.018107808617639422 \t validation loss: 0.017669402062892914\n",
      "Epoch #260 - training loss: 0.01831296453851142 \t validation loss: 0.017678892239928246\n",
      "Epoch #261 - training loss: 0.018213757880324328 \t validation loss: 0.017749205231666565\n",
      "Epoch #262 - training loss: 0.018365341501225405 \t validation loss: 0.01781102642416954\n",
      "Epoch #263 - training loss: 0.01833529614808531 \t validation loss: 0.01777157187461853\n",
      "Epoch #264 - training loss: 0.018278354184268004 \t validation loss: 0.017840979620814323\n",
      "Epoch #265 - training loss: 0.01825909735853031 \t validation loss: 0.017781458795070648\n",
      "Epoch #266 - training loss: 0.01812164038932278 \t validation loss: 0.017669694498181343\n",
      "Epoch #267 - training loss: 0.018163651454783714 \t validation loss: 0.0176999494433403\n",
      "Epoch #268 - training loss: 0.018124937826867894 \t validation loss: 0.017646940425038338\n",
      "Epoch #269 - training loss: 0.018300165951718735 \t validation loss: 0.017847776412963867\n",
      "Epoch #270 - training loss: 0.018334508990482876 \t validation loss: 0.0177156925201416\n",
      "Epoch #271 - training loss: 0.018126811541832846 \t validation loss: 0.017673863098025322\n",
      "Epoch #272 - training loss: 0.018291606778274586 \t validation loss: 0.017673350870609283\n",
      "Epoch #273 - training loss: 0.018256593192982066 \t validation loss: 0.017719853669404984\n",
      "Epoch #274 - training loss: 0.01830178666729 \t validation loss: 0.017820512875914574\n",
      "Epoch #275 - training loss: 0.01819019279616124 \t validation loss: 0.017652159556746483\n",
      "Epoch #276 - training loss: 0.018140592762548258 \t validation loss: 0.01767515204846859\n",
      "Epoch #277 - training loss: 0.018244836810950092 \t validation loss: 0.017703671008348465\n",
      "Epoch #278 - training loss: 0.018276704096320397 \t validation loss: 0.017717644572257996\n",
      "Epoch #279 - training loss: 0.01826848712257945 \t validation loss: 0.01772475242614746\n",
      "Epoch #280 - training loss: 0.018200250673740318 \t validation loss: 0.01774282194674015\n",
      "Epoch #281 - training loss: 0.01833907766234941 \t validation loss: 0.01785174570977688\n",
      "Epoch #282 - training loss: 0.01828302026549638 \t validation loss: 0.017774876207113266\n",
      "Epoch #283 - training loss: 0.018296244747247503 \t validation loss: 0.017737170681357384\n",
      "Epoch #284 - training loss: 0.01821007579047041 \t validation loss: 0.017689483240246773\n",
      "Epoch #285 - training loss: 0.018193733910604682 \t validation loss: 0.01768561452627182\n",
      "Epoch #286 - training loss: 0.018245286170548086 \t validation loss: 0.017690768465399742\n",
      "Epoch #287 - training loss: 0.018171392759336574 \t validation loss: 0.01767140068113804\n",
      "Epoch #288 - training loss: 0.018104525046876742 \t validation loss: 0.017611188814044\n",
      "Epoch #289 - training loss: 0.018235734779228494 \t validation loss: 0.017701806500554085\n",
      "Epoch #290 - training loss: 0.018140641670069926 \t validation loss: 0.01778891496360302\n",
      "Epoch #291 - training loss: 0.018261276814209807 \t validation loss: 0.017823519185185432\n",
      "Epoch #292 - training loss: 0.01823330184867915 \t validation loss: 0.017723383381962776\n",
      "Epoch #293 - training loss: 0.01813723210330433 \t validation loss: 0.01778239756822586\n",
      "Epoch #294 - training loss: 0.01817383851956323 \t validation loss: 0.017710676416754723\n",
      "Epoch #295 - training loss: 0.01815527315121755 \t validation loss: 0.017699100077152252\n",
      "Epoch #296 - training loss: 0.01821430907824646 \t validation loss: 0.017733927816152573\n",
      "Epoch #297 - training loss: 0.018294729132083107 \t validation loss: 0.01773550733923912\n",
      "Epoch #298 - training loss: 0.018270440938919608 \t validation loss: 0.017657486721873283\n",
      "Epoch #299 - training loss: 0.0181456794924118 \t validation loss: 0.017750661820173264\n",
      "Epoch #300 - training loss: 0.018194214157341376 \t validation loss: 0.01772204227745533\n",
      "Epoch #301 - training loss: 0.01828389176095572 \t validation loss: 0.017727384343743324\n",
      "Epoch #302 - training loss: 0.018216898667285833 \t validation loss: 0.017769919708371162\n",
      "Epoch #303 - training loss: 0.018185298974119756 \t validation loss: 0.017695635557174683\n",
      "Epoch #304 - training loss: 0.01822349616758246 \t validation loss: 0.017717158421874046\n",
      "Epoch #305 - training loss: 0.018245362436760924 \t validation loss: 0.017752118408679962\n",
      "Epoch #306 - training loss: 0.018312940556409855 \t validation loss: 0.017680050805211067\n",
      "Epoch #307 - training loss: 0.018191656183922527 \t validation loss: 0.017662040889263153\n",
      "Epoch #308 - training loss: 0.018190324823132147 \t validation loss: 0.017661036923527718\n",
      "Epoch #309 - training loss: 0.018266565163142158 \t validation loss: 0.017742322757840157\n",
      "Epoch #310 - training loss: 0.018084547741707212 \t validation loss: 0.01766105927526951\n",
      "Epoch #311 - training loss: 0.018177440514312554 \t validation loss: 0.017777597531676292\n",
      "Epoch #312 - training loss: 0.018202245881861683 \t validation loss: 0.01770882122218609\n",
      "Epoch #313 - training loss: 0.01837947869354933 \t validation loss: 0.017696602270007133\n",
      "Epoch #314 - training loss: 0.018223952306260026 \t validation loss: 0.017683425918221474\n",
      "Epoch #315 - training loss: 0.01826041144541349 \t validation loss: 0.017697015777230263\n",
      "Epoch #316 - training loss: 0.01830953739903021 \t validation loss: 0.01768992841243744\n",
      "Epoch #317 - training loss: 0.018330761999223354 \t validation loss: 0.017734559252858162\n",
      "Epoch #318 - training loss: 0.01819574011574811 \t validation loss: 0.017716754227876663\n",
      "Epoch #319 - training loss: 0.018274290992908913 \t validation loss: 0.01767697185277939\n",
      "Epoch #320 - training loss: 0.01812448712917529 \t validation loss: 0.01775328256189823\n",
      "Epoch #321 - training loss: 0.018067316072545858 \t validation loss: 0.017776301130652428\n",
      "Epoch #322 - training loss: 0.018188481881616928 \t validation loss: 0.01776379719376564\n",
      "Epoch #323 - training loss: 0.018273351164927284 \t validation loss: 0.017684638500213623\n",
      "Epoch #324 - training loss: 0.01809204603977222 \t validation loss: 0.01774708181619644\n",
      "Epoch #325 - training loss: 0.018071195868605124 \t validation loss: 0.017681477591395378\n",
      "Epoch #326 - training loss: 0.01818262204741601 \t validation loss: 0.017734479159116745\n",
      "Epoch #327 - training loss: 0.0181929978767772 \t validation loss: 0.017673220485448837\n",
      "Epoch #328 - training loss: 0.018135369697528995 \t validation loss: 0.01760362833738327\n",
      "Epoch #329 - training loss: 0.018171293508867783 \t validation loss: 0.017655540257692337\n",
      "Epoch #330 - training loss: 0.018214913428623657 \t validation loss: 0.01766856014728546\n",
      "Epoch #331 - training loss: 0.018311098511319877 \t validation loss: 0.017749611288309097\n",
      "Epoch #332 - training loss: 0.018162718206857803 \t validation loss: 0.017700765281915665\n",
      "Epoch #333 - training loss: 0.018125674426959133 \t validation loss: 0.017699655145406723\n",
      "Epoch #334 - training loss: 0.018097450648624214 \t validation loss: 0.017596714198589325\n",
      "Epoch #335 - training loss: 0.018323307732595634 \t validation loss: 0.01773901842534542\n",
      "Epoch #336 - training loss: 0.01812914949083871 \t validation loss: 0.01765633374452591\n",
      "Epoch #337 - training loss: 0.01821296684202941 \t validation loss: 0.017637301236391068\n",
      "Epoch #338 - training loss: 0.018179260407775136 \t validation loss: 0.017738178372383118\n",
      "Epoch #339 - training loss: 0.01830725313275529 \t validation loss: 0.017641780897974968\n",
      "Epoch #340 - training loss: 0.01810735498775294 \t validation loss: 0.017685675993561745\n",
      "Epoch #341 - training loss: 0.018144597901375986 \t validation loss: 0.017627231776714325\n",
      "Epoch #342 - training loss: 0.01817381909892478 \t validation loss: 0.017765026539564133\n",
      "Epoch #343 - training loss: 0.0180562561365887 \t validation loss: 0.017656229436397552\n",
      "Epoch #344 - training loss: 0.018316599644559155 \t validation loss: 0.01764521189033985\n",
      "Epoch #345 - training loss: 0.01805830135994942 \t validation loss: 0.017632314935326576\n",
      "Epoch #346 - training loss: 0.018282453688210534 \t validation loss: 0.017656181007623672\n",
      "Epoch #347 - training loss: 0.01820739276927462 \t validation loss: 0.017640631645917892\n",
      "Epoch #348 - training loss: 0.018165936022866036 \t validation loss: 0.01770155318081379\n",
      "Epoch #349 - training loss: 0.01807022096525956 \t validation loss: 0.017648523673415184\n",
      "Epoch #350 - training loss: 0.018121597993970614 \t validation loss: 0.01770152896642685\n",
      "Epoch #351 - training loss: 0.018129126053735335 \t validation loss: 0.01762845739722252\n",
      "Epoch #352 - training loss: 0.0182112245160685 \t validation loss: 0.017753690481185913\n",
      "Epoch #353 - training loss: 0.018293828720093837 \t validation loss: 0.01769828237593174\n",
      "Epoch #354 - training loss: 0.018127453376035222 \t validation loss: 0.017690885812044144\n",
      "Epoch #355 - training loss: 0.018196644105804557 \t validation loss: 0.017788279801607132\n",
      "Epoch #356 - training loss: 0.018141713010014167 \t validation loss: 0.017684128135442734\n",
      "Epoch #357 - training loss: 0.018177604518145915 \t validation loss: 0.017719995230436325\n",
      "Epoch #358 - training loss: 0.018102733787493802 \t validation loss: 0.017725901678204536\n",
      "Epoch #359 - training loss: 0.018175218919558183 \t validation loss: 0.017755622044205666\n",
      "Epoch #360 - training loss: 0.018212749229076664 \t validation loss: 0.017752354964613914\n",
      "Epoch #361 - training loss: 0.018196205685702557 \t validation loss: 0.01777971163392067\n",
      "Epoch #362 - training loss: 0.018250418733151093 \t validation loss: 0.017768826335668564\n",
      "Epoch #363 - training loss: 0.018315235060689845 \t validation loss: 0.017732836306095123\n",
      "Epoch #364 - training loss: 0.01817688234782924 \t validation loss: 0.017686543986201286\n",
      "Epoch #365 - training loss: 0.018374843774276155 \t validation loss: 0.017674438655376434\n",
      "Epoch #366 - training loss: 0.01816657036368406 \t validation loss: 0.017688453197479248\n",
      "Epoch #367 - training loss: 0.018123093061967025 \t validation loss: 0.0177216324955225\n",
      "Epoch #368 - training loss: 0.018124265734496717 \t validation loss: 0.017741385847330093\n",
      "Epoch #369 - training loss: 0.018116076652994445 \t validation loss: 0.017702698707580566\n",
      "Epoch #370 - training loss: 0.018157825828693474 \t validation loss: 0.017757711932063103\n",
      "Epoch #371 - training loss: 0.018259769978451017 \t validation loss: 0.017783716320991516\n",
      "Epoch #372 - training loss: 0.018079384106300788 \t validation loss: 0.017677372321486473\n",
      "Epoch #373 - training loss: 0.01816295154923813 \t validation loss: 0.01765727624297142\n",
      "Epoch #374 - training loss: 0.018235762154277788 \t validation loss: 0.017810437828302383\n",
      "Epoch #375 - training loss: 0.01818798098574661 \t validation loss: 0.017752092331647873\n",
      "Epoch #376 - training loss: 0.018153045932267218 \t validation loss: 0.017803436145186424\n",
      "Epoch #377 - training loss: 0.018255424260192353 \t validation loss: 0.01771865040063858\n",
      "Epoch #378 - training loss: 0.018195253918743506 \t validation loss: 0.017719214782118797\n",
      "Epoch #379 - training loss: 0.018091991860844824 \t validation loss: 0.017759086564183235\n",
      "Epoch #380 - training loss: 0.018152425015744354 \t validation loss: 0.017650170251727104\n",
      "Epoch #381 - training loss: 0.018305382186130335 \t validation loss: 0.017681388184428215\n",
      "Epoch #382 - training loss: 0.01828774290548668 \t validation loss: 0.01769077405333519\n",
      "Epoch #383 - training loss: 0.01810321437393422 \t validation loss: 0.01774836704134941\n",
      "Epoch #384 - training loss: 0.01815413928588534 \t validation loss: 0.017707811668515205\n",
      "Epoch #385 - training loss: 0.018243899541651825 \t validation loss: 0.01778237335383892\n",
      "Epoch #386 - training loss: 0.01826004131448475 \t validation loss: 0.01766885258257389\n",
      "Epoch #387 - training loss: 0.01808146377769135 \t validation loss: 0.01770317740738392\n",
      "Epoch #388 - training loss: 0.018107046464248724 \t validation loss: 0.017734495922923088\n",
      "Epoch #389 - training loss: 0.018243796545990353 \t validation loss: 0.01787770912051201\n",
      "Epoch #390 - training loss: 0.01811739072570058 \t validation loss: 0.017697593197226524\n",
      "Epoch #391 - training loss: 0.018220286345517207 \t validation loss: 0.017635179683566093\n",
      "Epoch #392 - training loss: 0.018248834997847397 \t validation loss: 0.017647085711359978\n",
      "Epoch #393 - training loss: 0.018186028300843416 \t validation loss: 0.01772724837064743\n",
      "Epoch #394 - training loss: 0.018176410528253195 \t validation loss: 0.017726708203554153\n",
      "Epoch #395 - training loss: 0.018107014841812312 \t validation loss: 0.017706705257296562\n",
      "Epoch #396 - training loss: 0.01808127554665719 \t validation loss: 0.01768704317510128\n",
      "Epoch #397 - training loss: 0.01811733974669504 \t validation loss: 0.017772754654288292\n",
      "Epoch #398 - training loss: 0.01827067695558071 \t validation loss: 0.017712119966745377\n",
      "Epoch #399 - training loss: 0.018073420163540636 \t validation loss: 0.017721155658364296\n",
      "Epoch #400 - training loss: 0.01824296314860378 \t validation loss: 0.01775776408612728\n",
      "Epoch #401 - training loss: 0.018204265918072303 \t validation loss: 0.017744101583957672\n",
      "Epoch #402 - training loss: 0.01829744924887125 \t validation loss: 0.017765512689948082\n",
      "Epoch #403 - training loss: 0.018124414129679984 \t validation loss: 0.01760530099272728\n",
      "Epoch #404 - training loss: 0.018240153255427487 \t validation loss: 0.01760888658463955\n",
      "Epoch #405 - training loss: 0.018140103711868773 \t validation loss: 0.01778252050280571\n",
      "Epoch #406 - training loss: 0.01830759345754475 \t validation loss: 0.017778046429157257\n",
      "Epoch #407 - training loss: 0.018250113295334495 \t validation loss: 0.017658641561865807\n",
      "Epoch #408 - training loss: 0.01807103981182145 \t validation loss: 0.017623646184802055\n",
      "Epoch #409 - training loss: 0.018144650416954722 \t validation loss: 0.017685433849692345\n",
      "Epoch #410 - training loss: 0.018290207160481926 \t validation loss: 0.01776430942118168\n",
      "Epoch #411 - training loss: 0.018333677538077346 \t validation loss: 0.017765169963240623\n",
      "Epoch #412 - training loss: 0.018070231480807483 \t validation loss: 0.01771879568696022\n",
      "Epoch #413 - training loss: 0.018197106344972146 \t validation loss: 0.01776333525776863\n",
      "Epoch #414 - training loss: 0.018116586287647694 \t validation loss: 0.017724711447954178\n",
      "Epoch #415 - training loss: 0.018105137163001075 \t validation loss: 0.017758091911673546\n",
      "Epoch #416 - training loss: 0.018145696403441266 \t validation loss: 0.017687011510133743\n",
      "Epoch #417 - training loss: 0.018027293057981397 \t validation loss: 0.017695505172014236\n",
      "Epoch #418 - training loss: 0.01815329789973586 \t validation loss: 0.017733562737703323\n",
      "Epoch #419 - training loss: 0.018165963144637035 \t validation loss: 0.017722398042678833\n",
      "Epoch #420 - training loss: 0.018227382680560424 \t validation loss: 0.01776753179728985\n",
      "Epoch #421 - training loss: 0.018173443494840097 \t validation loss: 0.017770597711205482\n",
      "Epoch #422 - training loss: 0.018152284589966875 \t validation loss: 0.01769440807402134\n",
      "Epoch #423 - training loss: 0.018151344546875905 \t validation loss: 0.017750587314367294\n",
      "Epoch #424 - training loss: 0.018162137043577307 \t validation loss: 0.017778610810637474\n",
      "Epoch #425 - training loss: 0.018311103074418803 \t validation loss: 0.017765497788786888\n",
      "Epoch #426 - training loss: 0.01811864022089199 \t validation loss: 0.017784319818019867\n",
      "Epoch #427 - training loss: 0.018179775697432145 \t validation loss: 0.017715342342853546\n",
      "Epoch #428 - training loss: 0.018081544963610238 \t validation loss: 0.017702199518680573\n",
      "Epoch #429 - training loss: 0.018180562331847217 \t validation loss: 0.01777700148522854\n",
      "Epoch #430 - training loss: 0.01815530460243892 \t validation loss: 0.01763908937573433\n",
      "Epoch #431 - training loss: 0.01823032211481792 \t validation loss: 0.017743362113833427\n",
      "Epoch #432 - training loss: 0.018259642250395315 \t validation loss: 0.01777694933116436\n",
      "Epoch #433 - training loss: 0.018251202466997997 \t validation loss: 0.0177511777728796\n",
      "Epoch #434 - training loss: 0.018222079827696677 \t validation loss: 0.01780165173113346\n",
      "Epoch #435 - training loss: 0.018179712576063163 \t validation loss: 0.01778149977326393\n",
      "Epoch #436 - training loss: 0.01822036957074899 \t validation loss: 0.017677059397101402\n",
      "Epoch #437 - training loss: 0.018105162634782547 \t validation loss: 0.01767820306122303\n",
      "Epoch #438 - training loss: 0.01818491194497541 \t validation loss: 0.017683155834674835\n",
      "Epoch #439 - training loss: 0.018122302268771345 \t validation loss: 0.017705025151371956\n",
      "Epoch #440 - training loss: 0.01824316999763774 \t validation loss: 0.017703209072351456\n",
      "Epoch #441 - training loss: 0.018090878808227493 \t validation loss: 0.01771015115082264\n",
      "Epoch #442 - training loss: 0.01817453063836875 \t validation loss: 0.01781800016760826\n",
      "Epoch #443 - training loss: 0.018127176181884346 \t validation loss: 0.017727605998516083\n",
      "Epoch #444 - training loss: 0.017987116308669088 \t validation loss: 0.01765323244035244\n",
      "Epoch #445 - training loss: 0.018079003680658807 \t validation loss: 0.01776038482785225\n",
      "Epoch #446 - training loss: 0.018220594059021607 \t validation loss: 0.017759086564183235\n",
      "Epoch #447 - training loss: 0.018212685982022758 \t validation loss: 0.017748771235346794\n",
      "Epoch #448 - training loss: 0.018144294870657458 \t validation loss: 0.017711414024233818\n",
      "Epoch #449 - training loss: 0.018315416233465772 \t validation loss: 0.01767747476696968\n",
      "Epoch #450 - training loss: 0.01825126335657348 \t validation loss: 0.017724258825182915\n",
      "Epoch #451 - training loss: 0.018038529314349384 \t validation loss: 0.0177155788987875\n",
      "Epoch #452 - training loss: 0.01819718348416358 \t validation loss: 0.017698198556900024\n",
      "Epoch #453 - training loss: 0.018172046853953523 \t validation loss: 0.017709653824567795\n",
      "Epoch #454 - training loss: 0.01809891416097989 \t validation loss: 0.01771273836493492\n",
      "Epoch #455 - training loss: 0.018117030310407456 \t validation loss: 0.017776982858777046\n",
      "Epoch #456 - training loss: 0.018150839641083914 \t validation loss: 0.017783403396606445\n",
      "Epoch #457 - training loss: 0.018044739714889096 \t validation loss: 0.017723912373185158\n",
      "Epoch #458 - training loss: 0.018192186128278318 \t validation loss: 0.01780114322900772\n",
      "Epoch #459 - training loss: 0.01813349773779084 \t validation loss: 0.01765603758394718\n",
      "Epoch #460 - training loss: 0.01821458550983608 \t validation loss: 0.01767999492585659\n",
      "Epoch #461 - training loss: 0.01801324834849738 \t validation loss: 0.01774017885327339\n",
      "Epoch #462 - training loss: 0.018060038534464178 \t validation loss: 0.017681440338492393\n",
      "Epoch #463 - training loss: 0.01815069246921575 \t validation loss: 0.017678095027804375\n",
      "Epoch #464 - training loss: 0.018123773086938213 \t validation loss: 0.01765737310051918\n",
      "Epoch #465 - training loss: 0.018136328616260227 \t validation loss: 0.017680926248431206\n",
      "Epoch #466 - training loss: 0.018180079519066007 \t validation loss: 0.017680540680885315\n",
      "Epoch #467 - training loss: 0.018112916276360528 \t validation loss: 0.017654316499829292\n",
      "Epoch #468 - training loss: 0.018157889631378356 \t validation loss: 0.017675817012786865\n",
      "Epoch #469 - training loss: 0.018159168500854552 \t validation loss: 0.017672980204224586\n",
      "Epoch #470 - training loss: 0.01821585397390906 \t validation loss: 0.017716815695166588\n",
      "Epoch #471 - training loss: 0.01806122536740464 \t validation loss: 0.01772601157426834\n",
      "Epoch #472 - training loss: 0.018112653972738067 \t validation loss: 0.01771506853401661\n",
      "Epoch #473 - training loss: 0.018143721801922405 \t validation loss: 0.017730936408042908\n",
      "Epoch #474 - training loss: 0.018205403285951677 \t validation loss: 0.017660493031144142\n",
      "Epoch #475 - training loss: 0.018104753313058292 \t validation loss: 0.017676418647170067\n",
      "Epoch #476 - training loss: 0.018173952910839744 \t validation loss: 0.017718259245157242\n",
      "Epoch #477 - training loss: 0.018150749974022565 \t validation loss: 0.01765507645905018\n",
      "Epoch #478 - training loss: 0.01809784762623482 \t validation loss: 0.01776384562253952\n",
      "Epoch #479 - training loss: 0.01810440161419199 \t validation loss: 0.017807327210903168\n",
      "Epoch #480 - training loss: 0.01806720744178301 \t validation loss: 0.01766209863126278\n",
      "Epoch #481 - training loss: 0.018287346783951275 \t validation loss: 0.01769639365375042\n",
      "Epoch #482 - training loss: 0.01805843119738532 \t validation loss: 0.017656387761235237\n",
      "Epoch #483 - training loss: 0.01810371804850468 \t validation loss: 0.017793284729123116\n",
      "Epoch #484 - training loss: 0.01806605861912671 \t validation loss: 0.017696605995297432\n",
      "Epoch #485 - training loss: 0.018102098184919084 \t validation loss: 0.017637254670262337\n",
      "Epoch #486 - training loss: 0.018237846175566386 \t validation loss: 0.017660291865468025\n",
      "Epoch #487 - training loss: 0.01822125762335703 \t validation loss: 0.017757730558514595\n",
      "Epoch #488 - training loss: 0.018163743130622793 \t validation loss: 0.017628146335482597\n",
      "Epoch #489 - training loss: 0.018211257154344466 \t validation loss: 0.0176945049315691\n",
      "Epoch #490 - training loss: 0.0180778364562026 \t validation loss: 0.017708001658320427\n",
      "Epoch #491 - training loss: 0.01810125115003646 \t validation loss: 0.017712600529193878\n",
      "Epoch #492 - training loss: 0.018087059117644258 \t validation loss: 0.017685774713754654\n",
      "Epoch #493 - training loss: 0.0183470174659314 \t validation loss: 0.017698831856250763\n",
      "Epoch #494 - training loss: 0.018152593130374487 \t validation loss: 0.017785372212529182\n",
      "Epoch #495 - training loss: 0.018144365416161403 \t validation loss: 0.017678692936897278\n",
      "Epoch #496 - training loss: 0.018039589898826543 \t validation loss: 0.017637096345424652\n",
      "Epoch #497 - training loss: 0.018160121933070712 \t validation loss: 0.017693541944026947\n",
      "Epoch #498 - training loss: 0.01820622650613088 \t validation loss: 0.01764095202088356\n",
      "Epoch #499 - training loss: 0.018124702046581693 \t validation loss: 0.017647894099354744\n",
      "Epoch #500 - training loss: 0.018183236500571114 \t validation loss: 0.01761128567159176\n",
      "         productid  predictedyQuantity\n",
      "0   440936473_navy            3.205025\n",
      "1   460084976_navy            4.167571\n",
      "2  460084976_olive            3.832538\n",
      "3  460156075_black            2.651532\n",
      "4  469263105_white            1.707066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172410/1884702822.py:36: FutureWarning: Passing 'use_legacy_dataset=True' to get the legacy behaviour is deprecated as of pyarrow 8.0.0, and the legacy implementation will be removed in a future version.\n",
      "  pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=path, filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20240, 35)\n",
      "(55630, 34)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20240 entries, 0 to 20239\n",
      "Data columns (total 35 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   similargrouplevel       20240 non-null  object \n",
      " 1   productid               20240 non-null  object \n",
      " 2   0_sales                 20240 non-null  float64\n",
      " 3   1_sales                 20240 non-null  float64\n",
      " 4   2_sales                 20240 non-null  float64\n",
      " 5   3_sales                 20240 non-null  float64\n",
      " 6   4_sales                 20240 non-null  float64\n",
      " 7   5_sales                 20240 non-null  float64\n",
      " 8   6_sales                 20240 non-null  float64\n",
      " 9   7_sales                 20240 non-null  float64\n",
      " 10  8_sales                 20240 non-null  float64\n",
      " 11  9_sales                 20240 non-null  float64\n",
      " 12  10_sales                20240 non-null  float64\n",
      " 13  sales_avg_11_months     20240 non-null  float64\n",
      " 14  sales_std_11_months     20240 non-null  float64\n",
      " 15  totalPLPViews           20240 non-null  float64\n",
      " 16  totalPLPClicks          20240 non-null  float64\n",
      " 17  totalPDPCount           20240 non-null  float64\n",
      " 18  totalATC                20240 non-null  float64\n",
      " 19  totalReturns            20240 non-null  float64\n",
      " 20  totalUsers              20240 non-null  float64\n",
      " 21  totalAvailableQuantity  20240 non-null  float64\n",
      " 22  totalWishlist           20240 non-null  float64\n",
      " 23  pricebucket             20240 non-null  int32  \n",
      " 24  normalized_color_0      20240 non-null  float64\n",
      " 25  normalized_color_1      20240 non-null  float64\n",
      " 26  normalized_pattern_0    20240 non-null  float64\n",
      " 27  normalized_pattern_1    20240 non-null  float64\n",
      " 28  normalized_brand_0      20240 non-null  float64\n",
      " 29  normalized_brand_1      20240 non-null  float64\n",
      " 30  normalized_style_0      20240 non-null  float64\n",
      " 31  normalized_style_1      20240 non-null  float64\n",
      " 32  normalized_sleeve_0     20240 non-null  float64\n",
      " 33  normalized_sleeve_1     20240 non-null  float64\n",
      " 34  yQuantity               20240 non-null  float64\n",
      "dtypes: float64(32), int32(1), object(2)\n",
      "memory usage: 5.3+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55630 entries, 0 to 55629\n",
      "Data columns (total 34 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   similargrouplevel       55630 non-null  object \n",
      " 1   productid               55630 non-null  object \n",
      " 2   0_sales                 55630 non-null  float64\n",
      " 3   1_sales                 55630 non-null  float64\n",
      " 4   2_sales                 55630 non-null  float64\n",
      " 5   3_sales                 55630 non-null  float64\n",
      " 6   4_sales                 55630 non-null  float64\n",
      " 7   5_sales                 55630 non-null  float64\n",
      " 8   6_sales                 55630 non-null  float64\n",
      " 9   7_sales                 55630 non-null  float64\n",
      " 10  8_sales                 55630 non-null  float64\n",
      " 11  9_sales                 55630 non-null  float64\n",
      " 12  10_sales                55630 non-null  float64\n",
      " 13  sales_avg_11_months     55630 non-null  float64\n",
      " 14  sales_std_11_months     55630 non-null  float64\n",
      " 15  totalPLPViews           55630 non-null  float64\n",
      " 16  totalPLPClicks          55630 non-null  float64\n",
      " 17  totalPDPCount           55630 non-null  float64\n",
      " 18  totalATC                55630 non-null  float64\n",
      " 19  totalReturns            55630 non-null  float64\n",
      " 20  totalUsers              55630 non-null  float64\n",
      " 21  totalAvailableQuantity  55630 non-null  float64\n",
      " 22  totalWishlist           55630 non-null  float64\n",
      " 23  pricebucket             55630 non-null  int32  \n",
      " 24  normalized_color_0      55630 non-null  float64\n",
      " 25  normalized_color_1      55630 non-null  float64\n",
      " 26  normalized_pattern_0    55630 non-null  float64\n",
      " 27  normalized_pattern_1    55630 non-null  float64\n",
      " 28  normalized_brand_0      55630 non-null  float64\n",
      " 29  normalized_brand_1      55630 non-null  float64\n",
      " 30  normalized_style_0      55630 non-null  float64\n",
      " 31  normalized_style_1      55630 non-null  float64\n",
      " 32  normalized_sleeve_0     55630 non-null  float64\n",
      " 33  normalized_sleeve_1     55630 non-null  float64\n",
      "dtypes: float64(31), int32(1), object(2)\n",
      "memory usage: 14.2+ MB\n",
      "['0_sales', '1_sales', '2_sales', '3_sales', '4_sales', '5_sales', '6_sales', '7_sales', '8_sales', '9_sales', '10_sales', 'sales_avg_11_months', 'sales_std_11_months', 'totalPLPViews', 'totalPLPClicks', 'totalPDPCount', 'totalATC', 'totalReturns', 'totalUsers', 'totalAvailableQuantity', 'totalWishlist', 'pricebucket']\n",
      "[1.] [6698.]\n",
      "(16548, 35) (4048, 35)\n",
      "cuda:0\n",
      "Best validation loss at epoch=1, saving model\n",
      "Epoch #1 - training loss: 0.15390332402729895 \t validation loss: 0.041288670152425766\n",
      "Best validation loss at epoch=2, saving model\n",
      "Epoch #2 - training loss: 0.0431306351181588 \t validation loss: 0.035273097455501556\n",
      "Best validation loss at epoch=3, saving model\n",
      "Epoch #3 - training loss: 0.03912282851479616 \t validation loss: 0.03377341106534004\n",
      "Best validation loss at epoch=4, saving model\n",
      "Epoch #4 - training loss: 0.03750278589512065 \t validation loss: 0.033209607005119324\n",
      "Best validation loss at epoch=5, saving model\n",
      "Epoch #5 - training loss: 0.03704011429288574 \t validation loss: 0.03298090770840645\n",
      "Best validation loss at epoch=6, saving model\n",
      "Epoch #6 - training loss: 0.03632357053646036 \t validation loss: 0.032708361744880676\n",
      "Best validation loss at epoch=7, saving model\n",
      "Epoch #7 - training loss: 0.03604953142686439 \t validation loss: 0.031978968530893326\n",
      "Epoch #8 - training loss: 0.03563353039929553 \t validation loss: 0.032455798238515854\n",
      "Best validation loss at epoch=9, saving model\n",
      "Epoch #9 - training loss: 0.034917348143743705 \t validation loss: 0.031752388924360275\n",
      "Epoch #10 - training loss: 0.03486665896080593 \t validation loss: 0.031831976026296616\n",
      "Epoch #11 - training loss: 0.03409658471479061 \t validation loss: 0.03213201463222504\n",
      "Best validation loss at epoch=12, saving model\n",
      "Epoch #12 - training loss: 0.033943169213456734 \t validation loss: 0.0313480943441391\n",
      "Epoch #13 - training loss: 0.03369529755530263 \t validation loss: 0.031506557017564774\n",
      "Best validation loss at epoch=14, saving model\n",
      "Epoch #14 - training loss: 0.03325442955117651 \t validation loss: 0.031207900494337082\n",
      "Best validation loss at epoch=15, saving model\n",
      "Epoch #15 - training loss: 0.032934149184103916 \t validation loss: 0.030870959162712097\n",
      "Epoch #16 - training loss: 0.03276437176913888 \t validation loss: 0.0308920256793499\n",
      "Epoch #17 - training loss: 0.03275618557070654 \t validation loss: 0.03088216856122017\n",
      "Best validation loss at epoch=18, saving model\n",
      "Epoch #18 - training loss: 0.03237663551703874 \t validation loss: 0.03071490116417408\n",
      "Epoch #19 - training loss: 0.03223687564013832 \t validation loss: 0.03126047924160957\n",
      "Epoch #20 - training loss: 0.03228487435014958 \t validation loss: 0.030797496438026428\n",
      "Epoch #21 - training loss: 0.03196076662558882 \t validation loss: 0.030728427693247795\n",
      "Best validation loss at epoch=22, saving model\n",
      "Epoch #22 - training loss: 0.03165927235076719 \t validation loss: 0.03058091178536415\n",
      "Epoch #23 - training loss: 0.031631164982324066 \t validation loss: 0.0308466088026762\n",
      "Epoch #24 - training loss: 0.03121134930612732 \t validation loss: 0.030689364299178123\n",
      "Best validation loss at epoch=25, saving model\n",
      "Epoch #25 - training loss: 0.031241165408306185 \t validation loss: 0.030405370518565178\n",
      "Best validation loss at epoch=26, saving model\n",
      "Epoch #26 - training loss: 0.031158104937927117 \t validation loss: 0.030391091480851173\n",
      "Epoch #27 - training loss: 0.031170234065214658 \t validation loss: 0.030708307400345802\n",
      "Epoch #28 - training loss: 0.030965006861626756 \t validation loss: 0.030467068776488304\n",
      "Best validation loss at epoch=29, saving model\n",
      "Epoch #29 - training loss: 0.030882263765985605 \t validation loss: 0.030285203829407692\n",
      "Epoch #30 - training loss: 0.03072090654379942 \t validation loss: 0.030339278280735016\n",
      "Best validation loss at epoch=31, saving model\n",
      "Epoch #31 - training loss: 0.03071797182297107 \t validation loss: 0.03017612174153328\n",
      "Epoch #32 - training loss: 0.03071364922164372 \t validation loss: 0.030218439176678658\n",
      "Epoch #33 - training loss: 0.030556739566440567 \t validation loss: 0.030292879790067673\n",
      "Epoch #34 - training loss: 0.03050376876298792 \t validation loss: 0.030263520777225494\n",
      "Best validation loss at epoch=35, saving model\n",
      "Epoch #35 - training loss: 0.03040507399230273 \t validation loss: 0.030074121430516243\n",
      "Epoch #36 - training loss: 0.03053722067644218 \t validation loss: 0.030248669907450676\n",
      "Epoch #37 - training loss: 0.030305510699367096 \t validation loss: 0.03033255785703659\n",
      "Epoch #38 - training loss: 0.030310776208057222 \t validation loss: 0.03019890747964382\n",
      "Epoch #39 - training loss: 0.0302789651615837 \t validation loss: 0.030143212527036667\n",
      "Epoch #40 - training loss: 0.03015653560536137 \t validation loss: 0.030098354443907738\n",
      "Epoch #41 - training loss: 0.030120364091281618 \t validation loss: 0.030085820704698563\n",
      "Epoch #42 - training loss: 0.030077840763592743 \t validation loss: 0.030224427580833435\n",
      "Epoch #43 - training loss: 0.03001685653195304 \t validation loss: 0.030117899179458618\n",
      "Epoch #44 - training loss: 0.02992264747194499 \t validation loss: 0.03017662838101387\n",
      "Best validation loss at epoch=45, saving model\n",
      "Epoch #45 - training loss: 0.029869112985053712 \t validation loss: 0.030070187523961067\n",
      "Epoch #46 - training loss: 0.029831678602966972 \t validation loss: 0.03009025752544403\n",
      "Epoch #47 - training loss: 0.029866134030906558 \t validation loss: 0.030095268040895462\n",
      "Best validation loss at epoch=48, saving model\n",
      "Epoch #48 - training loss: 0.02966841463111269 \t validation loss: 0.02983361855149269\n",
      "Epoch #49 - training loss: 0.029843447652485085 \t validation loss: 0.02992275357246399\n",
      "Epoch #50 - training loss: 0.029920460335636508 \t validation loss: 0.029955333098769188\n",
      "Epoch #51 - training loss: 0.0297278974284517 \t validation loss: 0.029926329851150513\n",
      "Epoch #52 - training loss: 0.02970715440680026 \t validation loss: 0.030013322830200195\n",
      "Epoch #53 - training loss: 0.02971489870257863 \t validation loss: 0.02988957241177559\n",
      "Epoch #54 - training loss: 0.029770601427315857 \t validation loss: 0.03016519919037819\n",
      "Epoch #55 - training loss: 0.029700513585962398 \t validation loss: 0.029837077483534813\n",
      "Epoch #56 - training loss: 0.029608545127744485 \t validation loss: 0.02985377050936222\n",
      "Epoch #57 - training loss: 0.029639478066730778 \t validation loss: 0.029889587312936783\n",
      "Epoch #58 - training loss: 0.029623323960528785 \t validation loss: 0.02988029643893242\n",
      "Epoch #59 - training loss: 0.029609567881156502 \t validation loss: 0.0299540925770998\n",
      "Epoch #60 - training loss: 0.02962550156744381 \t validation loss: 0.029955152422189713\n",
      "Epoch #61 - training loss: 0.02949118963397708 \t validation loss: 0.029888983815908432\n",
      "Epoch #62 - training loss: 0.029505632624094964 \t validation loss: 0.029965372756123543\n",
      "Epoch #63 - training loss: 0.029406710209890594 \t validation loss: 0.0300016887485981\n",
      "Best validation loss at epoch=64, saving model\n",
      "Epoch #64 - training loss: 0.02944497541502853 \t validation loss: 0.0298322644084692\n",
      "Best validation loss at epoch=65, saving model\n",
      "Epoch #65 - training loss: 0.029400728334000828 \t validation loss: 0.02980269305408001\n",
      "Best validation loss at epoch=66, saving model\n",
      "Epoch #66 - training loss: 0.029429898708986706 \t validation loss: 0.029730526730418205\n",
      "Epoch #67 - training loss: 0.029485640551974165 \t validation loss: 0.029811803251504898\n",
      "Epoch #68 - training loss: 0.029374997981786152 \t validation loss: 0.02978418581187725\n",
      "Epoch #69 - training loss: 0.029409448829672266 \t validation loss: 0.029858466237783432\n",
      "Epoch #70 - training loss: 0.029324985728049705 \t validation loss: 0.029901867732405663\n",
      "Epoch #71 - training loss: 0.02948131737510027 \t validation loss: 0.029837949201464653\n",
      "Epoch #72 - training loss: 0.029379050180719485 \t validation loss: 0.029759803786873817\n",
      "Best validation loss at epoch=73, saving model\n",
      "Epoch #73 - training loss: 0.029325490140102127 \t validation loss: 0.029709840193390846\n",
      "Epoch #74 - training loss: 0.02933605866888592 \t validation loss: 0.029754644259810448\n",
      "Epoch #75 - training loss: 0.02935801201447758 \t validation loss: 0.02981993369758129\n",
      "Epoch #76 - training loss: 0.02944665133855819 \t validation loss: 0.029814690351486206\n",
      "Epoch #77 - training loss: 0.029303566905772547 \t validation loss: 0.029824156314134598\n",
      "Epoch #78 - training loss: 0.02937984542211435 \t validation loss: 0.029714684933423996\n",
      "Epoch #79 - training loss: 0.029341986120049797 \t validation loss: 0.029726821929216385\n",
      "Best validation loss at epoch=80, saving model\n",
      "Epoch #80 - training loss: 0.029379205575583923 \t validation loss: 0.029644755646586418\n",
      "Epoch #81 - training loss: 0.029319254010447392 \t validation loss: 0.029665205627679825\n",
      "Epoch #82 - training loss: 0.029389767045591104 \t validation loss: 0.029721107333898544\n",
      "Epoch #83 - training loss: 0.029338770174931167 \t validation loss: 0.029717154800891876\n",
      "Epoch #84 - training loss: 0.0292636371403212 \t validation loss: 0.029777536168694496\n",
      "Epoch #85 - training loss: 0.029294358935689904 \t validation loss: 0.029792744666337967\n",
      "Epoch #86 - training loss: 0.029352299475455135 \t validation loss: 0.029741227626800537\n",
      "Best validation loss at epoch=87, saving model\n",
      "Epoch #87 - training loss: 0.02938973571928632 \t validation loss: 0.02956915646791458\n",
      "Epoch #88 - training loss: 0.029250170813956273 \t validation loss: 0.02967708930373192\n",
      "Epoch #89 - training loss: 0.02927163747324819 \t validation loss: 0.029622765257954597\n",
      "Epoch #90 - training loss: 0.029316969244692265 \t validation loss: 0.02962864376604557\n",
      "Epoch #91 - training loss: 0.029246970166306317 \t validation loss: 0.029662689194083214\n",
      "Epoch #92 - training loss: 0.02923039378007474 \t validation loss: 0.029640721157193184\n",
      "Epoch #93 - training loss: 0.029200858220357152 \t validation loss: 0.029631244018673897\n",
      "Epoch #94 - training loss: 0.02924423893135423 \t validation loss: 0.029609298333525658\n",
      "Best validation loss at epoch=95, saving model\n",
      "Epoch #95 - training loss: 0.029256421978976325 \t validation loss: 0.02954787202179432\n",
      "Epoch #96 - training loss: 0.029124038756016202 \t validation loss: 0.02958006039261818\n",
      "Best validation loss at epoch=97, saving model\n",
      "Epoch #97 - training loss: 0.029183455094392456 \t validation loss: 0.02951011434197426\n",
      "Best validation loss at epoch=98, saving model\n",
      "Epoch #98 - training loss: 0.029234744333689115 \t validation loss: 0.029464080929756165\n",
      "Epoch #99 - training loss: 0.029103111844091915 \t validation loss: 0.029477056115865707\n",
      "Best validation loss at epoch=100, saving model\n",
      "Epoch #100 - training loss: 0.02910782737707867 \t validation loss: 0.029451755806803703\n",
      "Epoch #101 - training loss: 0.029027528129518032 \t validation loss: 0.029572971165180206\n",
      "Epoch #102 - training loss: 0.029117568125822214 \t validation loss: 0.029508747160434723\n",
      "Epoch #103 - training loss: 0.029236581922298013 \t validation loss: 0.029532963410019875\n",
      "Epoch #104 - training loss: 0.029166541081210112 \t validation loss: 0.029480796307325363\n",
      "Epoch #105 - training loss: 0.029079317278273734 \t validation loss: 0.02949710562825203\n",
      "Best validation loss at epoch=106, saving model\n",
      "Epoch #106 - training loss: 0.029037359175119168 \t validation loss: 0.02942768670618534\n",
      "Epoch #107 - training loss: 0.029070046315117372 \t validation loss: 0.02947562374174595\n",
      "Epoch #108 - training loss: 0.02904328958597935 \t validation loss: 0.02951064333319664\n",
      "Epoch #109 - training loss: 0.02900458061329055 \t validation loss: 0.0294782891869545\n",
      "Epoch #110 - training loss: 0.029246575266620004 \t validation loss: 0.029535803943872452\n",
      "Best validation loss at epoch=111, saving model\n",
      "Epoch #111 - training loss: 0.029087415912568686 \t validation loss: 0.029377050697803497\n",
      "Epoch #112 - training loss: 0.02904910636827984 \t validation loss: 0.029417147859930992\n",
      "Epoch #113 - training loss: 0.02916100798931326 \t validation loss: 0.029590284451842308\n",
      "Best validation loss at epoch=114, saving model\n",
      "Epoch #114 - training loss: 0.02898155418864129 \t validation loss: 0.029367320239543915\n",
      "Epoch #115 - training loss: 0.029015476081332973 \t validation loss: 0.029458483681082726\n",
      "Epoch #116 - training loss: 0.02900916775616473 \t validation loss: 0.02944108471274376\n",
      "Epoch #117 - training loss: 0.029089989986385956 \t validation loss: 0.02941891923546791\n",
      "Epoch #118 - training loss: 0.029071328637318063 \t validation loss: 0.029392139986157417\n",
      "Best validation loss at epoch=119, saving model\n",
      "Epoch #119 - training loss: 0.02910862230773244 \t validation loss: 0.029363036155700684\n",
      "Best validation loss at epoch=120, saving model\n",
      "Epoch #120 - training loss: 0.02890622292193125 \t validation loss: 0.029349856078624725\n",
      "Epoch #121 - training loss: 0.029054611664611263 \t validation loss: 0.029403256252408028\n",
      "Epoch #122 - training loss: 0.028947763801579377 \t validation loss: 0.029444128274917603\n",
      "Epoch #123 - training loss: 0.02900520717811625 \t validation loss: 0.029500287026166916\n",
      "Best validation loss at epoch=124, saving model\n",
      "Epoch #124 - training loss: 0.02900249949733887 \t validation loss: 0.029182037338614464\n",
      "Epoch #125 - training loss: 0.028974823465451917 \t validation loss: 0.029296215623617172\n",
      "Epoch #126 - training loss: 0.028944026461732168 \t validation loss: 0.029330424964427948\n",
      "Epoch #127 - training loss: 0.02892432440652348 \t validation loss: 0.029358593747019768\n",
      "Epoch #128 - training loss: 0.029043244735681862 \t validation loss: 0.029311614111065865\n",
      "Epoch #129 - training loss: 0.02890031121408196 \t validation loss: 0.02928745746612549\n",
      "Epoch #130 - training loss: 0.028956904157553035 \t validation loss: 0.029269181191921234\n",
      "Epoch #131 - training loss: 0.029030205229769245 \t validation loss: 0.029316099360585213\n",
      "Epoch #132 - training loss: 0.028872692828720556 \t validation loss: 0.029222464188933372\n",
      "Epoch #133 - training loss: 0.029086853184771493 \t validation loss: 0.029356688261032104\n",
      "Epoch #134 - training loss: 0.028851335284586572 \t validation loss: 0.02946103923022747\n",
      "Epoch #135 - training loss: 0.028807085787434298 \t validation loss: 0.029416454955935478\n",
      "Epoch #136 - training loss: 0.028988900391756046 \t validation loss: 0.0292417723685503\n",
      "Epoch #137 - training loss: 0.028880753742649833 \t validation loss: 0.029363034293055534\n",
      "Epoch #138 - training loss: 0.028927150249978387 \t validation loss: 0.029285386204719543\n",
      "Epoch #139 - training loss: 0.028885557508092498 \t validation loss: 0.02924373373389244\n",
      "Epoch #140 - training loss: 0.028924403617580058 \t validation loss: 0.029203075915575027\n",
      "Epoch #141 - training loss: 0.028846094438831394 \t validation loss: 0.029387889429926872\n",
      "Epoch #142 - training loss: 0.02894957995986137 \t validation loss: 0.02924884669482708\n",
      "Epoch #143 - training loss: 0.02885266603375974 \t validation loss: 0.029452841728925705\n",
      "Epoch #144 - training loss: 0.02897710229830395 \t validation loss: 0.029242239892482758\n",
      "Epoch #145 - training loss: 0.028822125052777436 \t validation loss: 0.029279833659529686\n",
      "Epoch #146 - training loss: 0.028904909516614456 \t validation loss: 0.029188785701990128\n",
      "Best validation loss at epoch=147, saving model\n",
      "Epoch #147 - training loss: 0.028856016403673003 \t validation loss: 0.02916870079934597\n",
      "Epoch #148 - training loss: 0.028993496011106776 \t validation loss: 0.029204051941633224\n",
      "Epoch #149 - training loss: 0.02889863516040132 \t validation loss: 0.029297703877091408\n",
      "Epoch #150 - training loss: 0.028779325170921632 \t validation loss: 0.029262550175189972\n",
      "Epoch #151 - training loss: 0.02887297045471907 \t validation loss: 0.029316235333681107\n",
      "Epoch #152 - training loss: 0.02884875750259228 \t validation loss: 0.029257938265800476\n",
      "Epoch #153 - training loss: 0.028876952890926393 \t validation loss: 0.02922569215297699\n",
      "Epoch #154 - training loss: 0.028860202468067003 \t validation loss: 0.0292663536965847\n",
      "Epoch #155 - training loss: 0.028695491510056088 \t validation loss: 0.029234344139695168\n",
      "Epoch #156 - training loss: 0.028878454354950147 \t validation loss: 0.02921505644917488\n",
      "Best validation loss at epoch=157, saving model\n",
      "Epoch #157 - training loss: 0.028752170056392884 \t validation loss: 0.029108844697475433\n",
      "Epoch #158 - training loss: 0.0287592172229976 \t validation loss: 0.029246678575873375\n",
      "Epoch #159 - training loss: 0.028792726176807105 \t validation loss: 0.02919483371078968\n",
      "Epoch #160 - training loss: 0.028743954947329984 \t validation loss: 0.02916271984577179\n",
      "Epoch #161 - training loss: 0.028866044518572047 \t validation loss: 0.02926911599934101\n",
      "Epoch #162 - training loss: 0.028789637579242168 \t validation loss: 0.029319725930690765\n",
      "Epoch #163 - training loss: 0.02878385263439304 \t validation loss: 0.029159994795918465\n",
      "Epoch #164 - training loss: 0.02877577360831129 \t validation loss: 0.02918500080704689\n",
      "Epoch #165 - training loss: 0.02883554181098448 \t validation loss: 0.02927248738706112\n",
      "Epoch #166 - training loss: 0.028754222159935037 \t validation loss: 0.029275579378008842\n",
      "Epoch #167 - training loss: 0.028835185917861744 \t validation loss: 0.029183970764279366\n",
      "Epoch #168 - training loss: 0.028899218486279712 \t validation loss: 0.02925747074186802\n",
      "Epoch #169 - training loss: 0.02871807631577151 \t validation loss: 0.02919945865869522\n",
      "Epoch #170 - training loss: 0.0287117027538888 \t validation loss: 0.02921687252819538\n",
      "Epoch #171 - training loss: 0.028694655791256483 \t validation loss: 0.029294656589627266\n",
      "Epoch #172 - training loss: 0.028818952116927914 \t validation loss: 0.029230907559394836\n",
      "Epoch #173 - training loss: 0.02879472939221898 \t validation loss: 0.02938113734126091\n",
      "Best validation loss at epoch=174, saving model\n",
      "Epoch #174 - training loss: 0.028817427278869216 \t validation loss: 0.029054900631308556\n",
      "Epoch #175 - training loss: 0.028825058639625978 \t validation loss: 0.02917107753455639\n",
      "Epoch #176 - training loss: 0.02874401417368126 \t validation loss: 0.029167354106903076\n",
      "Epoch #177 - training loss: 0.028760223155859244 \t validation loss: 0.029387405142188072\n",
      "Epoch #178 - training loss: 0.028711179201986563 \t validation loss: 0.029148269444704056\n",
      "Epoch #179 - training loss: 0.02879904957246792 \t validation loss: 0.02913248911499977\n",
      "Epoch #180 - training loss: 0.028844927503354486 \t validation loss: 0.02919926866889\n",
      "Epoch #181 - training loss: 0.028705470236036312 \t validation loss: 0.029210343956947327\n",
      "Epoch #182 - training loss: 0.028737998488260075 \t validation loss: 0.02920684777200222\n",
      "Epoch #183 - training loss: 0.028712307401109537 \t validation loss: 0.029064800590276718\n",
      "Epoch #184 - training loss: 0.028679868369858634 \t validation loss: 0.02910086140036583\n",
      "Epoch #185 - training loss: 0.028684469663161547 \t validation loss: 0.02921943925321102\n",
      "Epoch #186 - training loss: 0.028685933724766313 \t validation loss: 0.029189782217144966\n",
      "Epoch #187 - training loss: 0.02871316719243602 \t validation loss: 0.02910970337688923\n",
      "Epoch #188 - training loss: 0.028559456720796902 \t validation loss: 0.029117774218320847\n",
      "Epoch #189 - training loss: 0.028703830275158587 \t validation loss: 0.02934424765408039\n",
      "Epoch #190 - training loss: 0.028810192176953277 \t validation loss: 0.02914408966898918\n",
      "Epoch #191 - training loss: 0.028713729938697537 \t validation loss: 0.02928352542221546\n",
      "Best validation loss at epoch=192, saving model\n",
      "Epoch #192 - training loss: 0.028786456890627598 \t validation loss: 0.02903052605688572\n",
      "Epoch #193 - training loss: 0.028779590811321018 \t validation loss: 0.029078176245093346\n",
      "Epoch #194 - training loss: 0.028717750921211344 \t validation loss: 0.029104912653565407\n",
      "Epoch #195 - training loss: 0.028680634243518285 \t validation loss: 0.029084423556923866\n",
      "Best validation loss at epoch=196, saving model\n",
      "Epoch #196 - training loss: 0.028577112259498185 \t validation loss: 0.029000932350754738\n",
      "Epoch #197 - training loss: 0.02863718024175794 \t validation loss: 0.02918984182178974\n",
      "Epoch #198 - training loss: 0.02868903971167292 \t validation loss: 0.029092319309711456\n",
      "Epoch #199 - training loss: 0.028533018624673036 \t validation loss: 0.02908703126013279\n",
      "Epoch #200 - training loss: 0.028797117895960116 \t validation loss: 0.02915840595960617\n",
      "Epoch #201 - training loss: 0.028547487473853897 \t validation loss: 0.02910529263317585\n",
      "Epoch #202 - training loss: 0.0287144682541259 \t validation loss: 0.02932528220117092\n",
      "Epoch #203 - training loss: 0.028694844260682105 \t validation loss: 0.02919587679207325\n",
      "Epoch #204 - training loss: 0.028504873593508968 \t validation loss: 0.029171153903007507\n",
      "Epoch #205 - training loss: 0.028682950193267782 \t validation loss: 0.029118934646248817\n",
      "Epoch #206 - training loss: 0.028559734179265436 \t validation loss: 0.029164792969822884\n",
      "Epoch #207 - training loss: 0.028570348951727786 \t validation loss: 0.029147658497095108\n",
      "Epoch #208 - training loss: 0.02859344309992094 \t validation loss: 0.02920861914753914\n",
      "Epoch #209 - training loss: 0.028680079129328177 \t validation loss: 0.029137447476387024\n",
      "Epoch #210 - training loss: 0.02873714779083918 \t validation loss: 0.02924460545182228\n",
      "Epoch #211 - training loss: 0.028760281335598496 \t validation loss: 0.02922951988875866\n",
      "Epoch #212 - training loss: 0.028740722688754418 \t validation loss: 0.02930392138659954\n",
      "Epoch #213 - training loss: 0.028629981273176362 \t validation loss: 0.029113145545125008\n",
      "Epoch #214 - training loss: 0.02876959316436079 \t validation loss: 0.029162626713514328\n",
      "Epoch #215 - training loss: 0.028656872450527185 \t validation loss: 0.029120568186044693\n",
      "Epoch #216 - training loss: 0.02865497801701468 \t validation loss: 0.02913493663072586\n",
      "Epoch #217 - training loss: 0.028610087295262796 \t validation loss: 0.029108528047800064\n",
      "Epoch #218 - training loss: 0.0285788391026843 \t validation loss: 0.02909216843545437\n",
      "Epoch #219 - training loss: 0.02859502501436284 \t validation loss: 0.02904609777033329\n",
      "Epoch #220 - training loss: 0.028767411746589082 \t validation loss: 0.029051562771201134\n",
      "Epoch #221 - training loss: 0.028645366782455828 \t validation loss: 0.029089095070958138\n",
      "Best validation loss at epoch=222, saving model\n",
      "Epoch #222 - training loss: 0.028636365176882863 \t validation loss: 0.028987180441617966\n",
      "Epoch #223 - training loss: 0.028604800206729765 \t validation loss: 0.02910437434911728\n",
      "Epoch #224 - training loss: 0.028590858562685534 \t validation loss: 0.02913765050470829\n",
      "Epoch #225 - training loss: 0.028538351986607684 \t validation loss: 0.029098626226186752\n",
      "Epoch #226 - training loss: 0.02865798383090119 \t validation loss: 0.029262445867061615\n",
      "Epoch #227 - training loss: 0.028522801471106723 \t validation loss: 0.029054967686533928\n",
      "Epoch #228 - training loss: 0.02859851095746172 \t validation loss: 0.029157832264900208\n",
      "Epoch #229 - training loss: 0.0287189923778061 \t validation loss: 0.029195047914981842\n",
      "Epoch #230 - training loss: 0.028697364563680824 \t validation loss: 0.02904275804758072\n",
      "Epoch #231 - training loss: 0.02856490112718634 \t validation loss: 0.02922283671796322\n",
      "Epoch #232 - training loss: 0.028703027267938542 \t validation loss: 0.029147857800126076\n",
      "Epoch #233 - training loss: 0.028652474270661577 \t validation loss: 0.02905506081879139\n",
      "Epoch #234 - training loss: 0.028722467155842476 \t validation loss: 0.02915935032069683\n",
      "Epoch #235 - training loss: 0.02863416851838651 \t validation loss: 0.029124712571501732\n",
      "Epoch #236 - training loss: 0.028699181411898114 \t validation loss: 0.029163358733057976\n",
      "Best validation loss at epoch=237, saving model\n",
      "Epoch #237 - training loss: 0.028486440416670457 \t validation loss: 0.02898690104484558\n",
      "Epoch #238 - training loss: 0.02866925813646796 \t validation loss: 0.029103009030222893\n",
      "Epoch #239 - training loss: 0.028750400019294405 \t validation loss: 0.02905249036848545\n",
      "Epoch #240 - training loss: 0.028763520400755006 \t validation loss: 0.029339520260691643\n",
      "Epoch #241 - training loss: 0.02849783244717513 \t validation loss: 0.02907736599445343\n",
      "Epoch #242 - training loss: 0.028625914232527958 \t validation loss: 0.029058145359158516\n",
      "Epoch #243 - training loss: 0.028608677914182385 \t validation loss: 0.02915039099752903\n",
      "Epoch #244 - training loss: 0.028510269701725856 \t validation loss: 0.029006529599428177\n",
      "Best validation loss at epoch=245, saving model\n",
      "Epoch #245 - training loss: 0.02861694443063014 \t validation loss: 0.02891581691801548\n",
      "Epoch #246 - training loss: 0.02851977775402987 \t validation loss: 0.02893035300076008\n",
      "Epoch #247 - training loss: 0.028555578052298646 \t validation loss: 0.029041403904557228\n",
      "Epoch #248 - training loss: 0.028568441054119046 \t validation loss: 0.02894267439842224\n",
      "Epoch #249 - training loss: 0.028511444091386156 \t validation loss: 0.02897494286298752\n",
      "Epoch #250 - training loss: 0.028608485146622016 \t validation loss: 0.028977328911423683\n",
      "Epoch #251 - training loss: 0.02856720424738192 \t validation loss: 0.028984395787119865\n",
      "Epoch #252 - training loss: 0.028552089760177254 \t validation loss: 0.029276998713612556\n",
      "Epoch #253 - training loss: 0.028689164202660322 \t validation loss: 0.02915693074464798\n",
      "Epoch #254 - training loss: 0.028638654717242407 \t validation loss: 0.02903398498892784\n",
      "Epoch #255 - training loss: 0.028720601850028945 \t validation loss: 0.029071636497974396\n",
      "Epoch #256 - training loss: 0.02860633175949813 \t validation loss: 0.029031120240688324\n",
      "Epoch #257 - training loss: 0.02855347719843314 \t validation loss: 0.028921112418174744\n",
      "Epoch #258 - training loss: 0.028487890641740028 \t validation loss: 0.029054220765829086\n",
      "Best validation loss at epoch=259, saving model\n",
      "Epoch #259 - training loss: 0.02858917398749341 \t validation loss: 0.028873169794678688\n",
      "Epoch #260 - training loss: 0.028501592234238204 \t validation loss: 0.028924992308020592\n",
      "Epoch #261 - training loss: 0.028653938033234336 \t validation loss: 0.028952402994036674\n",
      "Epoch #262 - training loss: 0.028587753245898843 \t validation loss: 0.028914175927639008\n",
      "Epoch #263 - training loss: 0.028705913323109223 \t validation loss: 0.028909003362059593\n",
      "Epoch #264 - training loss: 0.028593812468398642 \t validation loss: 0.029022160917520523\n",
      "Epoch #265 - training loss: 0.02862054683768859 \t validation loss: 0.02905033901333809\n",
      "Epoch #266 - training loss: 0.028558047412673093 \t validation loss: 0.02901410311460495\n",
      "Epoch #267 - training loss: 0.02863367931283218 \t validation loss: 0.029058322310447693\n",
      "Epoch #268 - training loss: 0.028624229570868742 \t validation loss: 0.028920456767082214\n",
      "Epoch #269 - training loss: 0.028508728782590514 \t validation loss: 0.02905534952878952\n",
      "Epoch #270 - training loss: 0.028423598677467563 \t validation loss: 0.02904035709798336\n",
      "Epoch #271 - training loss: 0.02851715700509931 \t validation loss: 0.029100405052304268\n",
      "Epoch #272 - training loss: 0.02854302736644127 \t validation loss: 0.02908203937113285\n",
      "Epoch #273 - training loss: 0.028482718486879274 \t validation loss: 0.02909473516047001\n",
      "Epoch #274 - training loss: 0.02843217156211372 \t validation loss: 0.02905121259391308\n",
      "Epoch #275 - training loss: 0.028511159221967772 \t validation loss: 0.02906699851155281\n",
      "Epoch #276 - training loss: 0.028444573842979808 \t validation loss: 0.02901989221572876\n",
      "Epoch #277 - training loss: 0.02864456104161488 \t validation loss: 0.02897622063755989\n",
      "Epoch #278 - training loss: 0.028516870578372676 \t validation loss: 0.02893582172691822\n",
      "Epoch #279 - training loss: 0.028512065800544543 \t validation loss: 0.029271138831973076\n",
      "Epoch #280 - training loss: 0.02862095743835664 \t validation loss: 0.02908148244023323\n",
      "Epoch #281 - training loss: 0.02860127642977716 \t validation loss: 0.02893122471868992\n",
      "Epoch #282 - training loss: 0.028582638340355463 \t validation loss: 0.02924480475485325\n",
      "Epoch #283 - training loss: 0.02853000210688181 \t validation loss: 0.028976432979106903\n",
      "Epoch #284 - training loss: 0.02857324759326371 \t validation loss: 0.029078533872961998\n",
      "Epoch #285 - training loss: 0.02859965973845332 \t validation loss: 0.029041850939393044\n",
      "Epoch #286 - training loss: 0.028636741018682948 \t validation loss: 0.028998568654060364\n",
      "Epoch #287 - training loss: 0.028579062568762373 \t validation loss: 0.02912146784365177\n",
      "Epoch #288 - training loss: 0.028482375753866924 \t validation loss: 0.029003886505961418\n",
      "Epoch #289 - training loss: 0.028509811665754715 \t validation loss: 0.029227999970316887\n",
      "Epoch #290 - training loss: 0.02857809869313119 \t validation loss: 0.029003214091062546\n",
      "Epoch #291 - training loss: 0.02846312324914584 \t validation loss: 0.02907954901456833\n",
      "Epoch #292 - training loss: 0.02843296867426002 \t validation loss: 0.029025008901953697\n",
      "Epoch #293 - training loss: 0.028520630299684734 \t validation loss: 0.029149480164051056\n",
      "Epoch #294 - training loss: 0.02857809744746473 \t validation loss: 0.029055600985884666\n",
      "Epoch #295 - training loss: 0.02874436123094862 \t validation loss: 0.029006095603108406\n",
      "Epoch #296 - training loss: 0.028670572202897683 \t validation loss: 0.029079115018248558\n",
      "Epoch #297 - training loss: 0.02851509052910617 \t validation loss: 0.029003208503127098\n",
      "Epoch #298 - training loss: 0.02862449417456379 \t validation loss: 0.02913004159927368\n",
      "Epoch #299 - training loss: 0.028417035960727435 \t validation loss: 0.028968412429094315\n",
      "Epoch #300 - training loss: 0.028596438869214265 \t validation loss: 0.028949126601219177\n",
      "Epoch #301 - training loss: 0.028487731351353213 \t validation loss: 0.028921136632561684\n",
      "Epoch #302 - training loss: 0.028642923665742753 \t validation loss: 0.029100783169269562\n",
      "Epoch #303 - training loss: 0.028432750575446183 \t validation loss: 0.029144635424017906\n",
      "Epoch #304 - training loss: 0.02851010829109403 \t validation loss: 0.028972400352358818\n",
      "Epoch #305 - training loss: 0.028398576920772804 \t validation loss: 0.028931349515914917\n",
      "Epoch #306 - training loss: 0.02848909262610992 \t validation loss: 0.028950480744242668\n",
      "Epoch #307 - training loss: 0.02856021049401833 \t validation loss: 0.02887568436563015\n",
      "Best validation loss at epoch=308, saving model\n",
      "Epoch #308 - training loss: 0.028597090872476593 \t validation loss: 0.02885579690337181\n",
      "Epoch #309 - training loss: 0.028491817546262618 \t validation loss: 0.029036657884716988\n",
      "Epoch #310 - training loss: 0.02848527147984078 \t validation loss: 0.028964152559638023\n",
      "Epoch #311 - training loss: 0.02857011213909467 \t validation loss: 0.02900393307209015\n",
      "Epoch #312 - training loss: 0.02844647122132709 \t validation loss: 0.02905452251434326\n",
      "Epoch #313 - training loss: 0.028509989999616573 \t validation loss: 0.028980957344174385\n",
      "Epoch #314 - training loss: 0.02851865485281732 \t validation loss: 0.02895391173660755\n",
      "Epoch #315 - training loss: 0.02845430328299938 \t validation loss: 0.028926851227879524\n",
      "Epoch #316 - training loss: 0.028457068730095254 \t validation loss: 0.029208490625023842\n",
      "Epoch #317 - training loss: 0.02853728033335631 \t validation loss: 0.028974268585443497\n",
      "Epoch #318 - training loss: 0.028677469447283957 \t validation loss: 0.029106060042977333\n",
      "Epoch #319 - training loss: 0.028476085922747446 \t validation loss: 0.029027385637164116\n",
      "Epoch #320 - training loss: 0.02849060043915962 \t validation loss: 0.029116950929164886\n",
      "Epoch #321 - training loss: 0.028490047276783843 \t validation loss: 0.02906486578285694\n",
      "Epoch #322 - training loss: 0.028430391164276772 \t validation loss: 0.028981657698750496\n",
      "Epoch #323 - training loss: 0.02864271578625767 \t validation loss: 0.02920580469071865\n",
      "Epoch #324 - training loss: 0.028433980401317144 \t validation loss: 0.02904345467686653\n",
      "Epoch #325 - training loss: 0.02851547420338315 \t validation loss: 0.029149072244763374\n",
      "Epoch #326 - training loss: 0.028537984112389312 \t validation loss: 0.029008284211158752\n",
      "Epoch #327 - training loss: 0.02845690707447335 \t validation loss: 0.02905556932091713\n",
      "Epoch #328 - training loss: 0.02856906177746258 \t validation loss: 0.029107442125678062\n",
      "Epoch #329 - training loss: 0.02849551069512387 \t validation loss: 0.029112370684742928\n",
      "Epoch #330 - training loss: 0.02848925671632072 \t validation loss: 0.029027722775936127\n",
      "Epoch #331 - training loss: 0.02841423827311748 \t validation loss: 0.028982602059841156\n",
      "Epoch #332 - training loss: 0.02849290440240929 \t validation loss: 0.029068797826766968\n",
      "Epoch #333 - training loss: 0.028476801509941915 \t validation loss: 0.029068034142255783\n",
      "Epoch #334 - training loss: 0.028382411651760052 \t validation loss: 0.028997644782066345\n",
      "Best validation loss at epoch=335, saving model\n",
      "Epoch #335 - training loss: 0.02843245192533593 \t validation loss: 0.028821902349591255\n",
      "Epoch #336 - training loss: 0.028488178152007337 \t validation loss: 0.029051491990685463\n",
      "Epoch #337 - training loss: 0.028511799509840615 \t validation loss: 0.028878450393676758\n",
      "Epoch #338 - training loss: 0.028449483430175185 \t validation loss: 0.029103614389896393\n",
      "Epoch #339 - training loss: 0.028547944750535938 \t validation loss: 0.029005281627178192\n",
      "Epoch #340 - training loss: 0.028540573425129834 \t validation loss: 0.029332440346479416\n",
      "Epoch #341 - training loss: 0.028422174732988012 \t validation loss: 0.029125459492206573\n",
      "Epoch #342 - training loss: 0.02841878310116134 \t validation loss: 0.029086550697684288\n",
      "Epoch #343 - training loss: 0.028478126773859302 \t validation loss: 0.02899789623916149\n",
      "Epoch #344 - training loss: 0.028403410988876044 \t validation loss: 0.02904614247381687\n",
      "Epoch #345 - training loss: 0.028574392702106235 \t validation loss: 0.029086925089359283\n",
      "Epoch #346 - training loss: 0.028485933109772044 \t validation loss: 0.028998613357543945\n",
      "Epoch #347 - training loss: 0.028476849894581553 \t validation loss: 0.028892163187265396\n",
      "Epoch #348 - training loss: 0.028530377860413413 \t validation loss: 0.029160697013139725\n",
      "Epoch #349 - training loss: 0.028528064612760362 \t validation loss: 0.02908829227089882\n",
      "Epoch #350 - training loss: 0.028441932894977626 \t validation loss: 0.02895846590399742\n",
      "Epoch #351 - training loss: 0.028474241546472278 \t validation loss: 0.028951486572623253\n",
      "Epoch #352 - training loss: 0.028361173292596752 \t validation loss: 0.028898874297738075\n",
      "Epoch #353 - training loss: 0.028536687629401855 \t validation loss: 0.029045095667243004\n",
      "Epoch #354 - training loss: 0.02848099745950541 \t validation loss: 0.029117373749613762\n",
      "Epoch #355 - training loss: 0.028396923097689965 \t validation loss: 0.02906479313969612\n",
      "Epoch #356 - training loss: 0.02847688697995456 \t validation loss: 0.02920396625995636\n",
      "Epoch #357 - training loss: 0.028503351698033294 \t validation loss: 0.029225464910268784\n",
      "Epoch #358 - training loss: 0.028396815153440492 \t validation loss: 0.02895696647465229\n",
      "Epoch #359 - training loss: 0.028440224021183222 \t validation loss: 0.029169166460633278\n",
      "Epoch #360 - training loss: 0.02841033852016067 \t validation loss: 0.02902427315711975\n",
      "Epoch #361 - training loss: 0.028491785693949726 \t validation loss: 0.029134012758731842\n",
      "Epoch #362 - training loss: 0.028507701002378413 \t validation loss: 0.02903568185865879\n",
      "Epoch #363 - training loss: 0.028300741312726325 \t validation loss: 0.029064523056149483\n",
      "Epoch #364 - training loss: 0.028590500232722155 \t validation loss: 0.029110148549079895\n",
      "Epoch #365 - training loss: 0.028482222230654622 \t validation loss: 0.029047202318906784\n",
      "Epoch #366 - training loss: 0.028406929325607806 \t validation loss: 0.02898939698934555\n",
      "Epoch #367 - training loss: 0.02838955325483411 \t validation loss: 0.029226502403616905\n",
      "Epoch #368 - training loss: 0.028433665364793352 \t validation loss: 0.0289847943931818\n",
      "Epoch #369 - training loss: 0.02853169044969708 \t validation loss: 0.029125437140464783\n",
      "Epoch #370 - training loss: 0.02853595918112901 \t validation loss: 0.028985533863306046\n",
      "Epoch #371 - training loss: 0.02860140377327771 \t validation loss: 0.02924143150448799\n",
      "Epoch #372 - training loss: 0.02834257851057547 \t validation loss: 0.029127905145287514\n",
      "Epoch #373 - training loss: 0.028562197535581584 \t validation loss: 0.02899806760251522\n",
      "Epoch #374 - training loss: 0.028562941153423872 \t validation loss: 0.029009101912379265\n",
      "Epoch #375 - training loss: 0.028421743729690405 \t validation loss: 0.029033372178673744\n",
      "Epoch #376 - training loss: 0.028470547966176294 \t validation loss: 0.02931276150047779\n",
      "Epoch #377 - training loss: 0.028537257700596487 \t validation loss: 0.02901986427605152\n",
      "Epoch #378 - training loss: 0.02851347451780993 \t validation loss: 0.029128266498446465\n",
      "Epoch #379 - training loss: 0.028459140383305184 \t validation loss: 0.02906426601111889\n",
      "Epoch #380 - training loss: 0.02835773265464826 \t validation loss: 0.029020026326179504\n",
      "Epoch #381 - training loss: 0.028548780878252807 \t validation loss: 0.02910039573907852\n",
      "Epoch #382 - training loss: 0.028322197919318767 \t validation loss: 0.02903888374567032\n",
      "Epoch #383 - training loss: 0.028471172608910723 \t validation loss: 0.029016483575105667\n",
      "Epoch #384 - training loss: 0.028443439867224984 \t validation loss: 0.029072878882288933\n",
      "Epoch #385 - training loss: 0.02848027133323411 \t validation loss: 0.02903888188302517\n",
      "Epoch #386 - training loss: 0.028385667837399636 \t validation loss: 0.02894657664000988\n",
      "Epoch #387 - training loss: 0.0284334205224303 \t validation loss: 0.029042238369584084\n",
      "Epoch #388 - training loss: 0.02853234458131071 \t validation loss: 0.02911556512117386\n",
      "Epoch #389 - training loss: 0.028487003016568417 \t validation loss: 0.029204992577433586\n",
      "Epoch #390 - training loss: 0.028396088082336134 \t validation loss: 0.029141977429389954\n",
      "Epoch #391 - training loss: 0.0285354257260433 \t validation loss: 0.029020709916949272\n",
      "Epoch #392 - training loss: 0.028486665462574254 \t validation loss: 0.028988782316446304\n",
      "Epoch #393 - training loss: 0.02852278053526347 \t validation loss: 0.029063573107123375\n",
      "Epoch #394 - training loss: 0.028463223965287325 \t validation loss: 0.028990505263209343\n",
      "Epoch #395 - training loss: 0.02841962719375783 \t validation loss: 0.029109172523021698\n",
      "Epoch #396 - training loss: 0.028514570128749084 \t validation loss: 0.028971556574106216\n",
      "Epoch #397 - training loss: 0.02847685289886245 \t validation loss: 0.029122697189450264\n",
      "Epoch #398 - training loss: 0.028477043831248983 \t validation loss: 0.02919297106564045\n",
      "Epoch #399 - training loss: 0.028417938650086724 \t validation loss: 0.02910810522735119\n",
      "Epoch #400 - training loss: 0.02860074353042262 \t validation loss: 0.028923144564032555\n",
      "Epoch #401 - training loss: 0.02839346501410613 \t validation loss: 0.029115524142980576\n",
      "Epoch #402 - training loss: 0.028410540395587453 \t validation loss: 0.029015960171818733\n",
      "Epoch #403 - training loss: 0.028385478731179952 \t validation loss: 0.02905857190489769\n",
      "Epoch #404 - training loss: 0.028549312691364574 \t validation loss: 0.028969325125217438\n",
      "Epoch #405 - training loss: 0.02843629667982171 \t validation loss: 0.028884245082736015\n",
      "Epoch #406 - training loss: 0.028389786680839683 \t validation loss: 0.028936749324202538\n",
      "Epoch #407 - training loss: 0.02828800520946126 \t validation loss: 0.02910619042813778\n",
      "Epoch #408 - training loss: 0.028487166310561463 \t validation loss: 0.028872050344944\n",
      "Epoch #409 - training loss: 0.028408747625751584 \t validation loss: 0.02899952046573162\n",
      "Epoch #410 - training loss: 0.02837096862098638 \t validation loss: 0.028960013762116432\n",
      "Epoch #411 - training loss: 0.028509487627646966 \t validation loss: 0.02909623458981514\n",
      "Epoch #412 - training loss: 0.02844375680152118 \t validation loss: 0.028919154778122902\n",
      "Epoch #413 - training loss: 0.02851752829927108 \t validation loss: 0.02902531996369362\n",
      "Epoch #414 - training loss: 0.02843056018671428 \t validation loss: 0.029225464910268784\n",
      "Epoch #415 - training loss: 0.028387036966250008 \t validation loss: 0.02914932370185852\n",
      "Epoch #416 - training loss: 0.028371387597252714 \t validation loss: 0.02910722605884075\n",
      "Epoch #417 - training loss: 0.028515089673442295 \t validation loss: 0.028967197984457016\n",
      "Epoch #418 - training loss: 0.02847672362471399 \t validation loss: 0.028884228318929672\n",
      "Epoch #419 - training loss: 0.028373072441753786 \t validation loss: 0.028945406898856163\n",
      "Epoch #420 - training loss: 0.028433145650768765 \t validation loss: 0.028968602418899536\n",
      "Epoch #421 - training loss: 0.028429641808983032 \t validation loss: 0.029037941247224808\n",
      "Epoch #422 - training loss: 0.028444444200445534 \t validation loss: 0.029224619269371033\n",
      "Epoch #423 - training loss: 0.028446305841315588 \t validation loss: 0.02905343659222126\n",
      "Epoch #424 - training loss: 0.028461417953377477 \t validation loss: 0.028934938833117485\n",
      "Epoch #425 - training loss: 0.028663660677031 \t validation loss: 0.029081115499138832\n",
      "Epoch #426 - training loss: 0.02839135952254242 \t validation loss: 0.029023481532931328\n",
      "Epoch #427 - training loss: 0.02835110010762971 \t validation loss: 0.028889724984765053\n",
      "Epoch #428 - training loss: 0.028393493664434732 \t validation loss: 0.028873644769191742\n",
      "Epoch #429 - training loss: 0.028354061355839556 \t validation loss: 0.028972938656806946\n",
      "Best validation loss at epoch=430, saving model\n",
      "Epoch #430 - training loss: 0.028408750786303723 \t validation loss: 0.028819330036640167\n",
      "Epoch #431 - training loss: 0.02850565207650168 \t validation loss: 0.028989389538764954\n",
      "Epoch #432 - training loss: 0.028399677934553797 \t validation loss: 0.028857432305812836\n",
      "Epoch #433 - training loss: 0.028454422903007592 \t validation loss: 0.029062261804938316\n",
      "Epoch #434 - training loss: 0.0284913778845473 \t validation loss: 0.02904604934155941\n",
      "Epoch #435 - training loss: 0.028511692218597784 \t validation loss: 0.02907242253422737\n",
      "Epoch #436 - training loss: 0.028311127786861456 \t validation loss: 0.02902062050998211\n",
      "Epoch #437 - training loss: 0.028432215987731716 \t validation loss: 0.028911324217915535\n",
      "Epoch #438 - training loss: 0.0284530770446118 \t validation loss: 0.029172077775001526\n",
      "Epoch #439 - training loss: 0.02849098659306044 \t validation loss: 0.029152868315577507\n",
      "Epoch #440 - training loss: 0.028462731764008736 \t validation loss: 0.028883734717965126\n",
      "Epoch #441 - training loss: 0.028500841398195614 \t validation loss: 0.029159139841794968\n",
      "Epoch #442 - training loss: 0.02843398201446872 \t validation loss: 0.029049135744571686\n",
      "Epoch #443 - training loss: 0.028487552624786673 \t validation loss: 0.028983622789382935\n",
      "Epoch #444 - training loss: 0.02845149225603998 \t validation loss: 0.028934819623827934\n",
      "Epoch #445 - training loss: 0.028392088115827777 \t validation loss: 0.02909249998629093\n",
      "Epoch #446 - training loss: 0.028375552560975577 \t validation loss: 0.029102854430675507\n",
      "Epoch #447 - training loss: 0.028452135220789702 \t validation loss: 0.029049580916762352\n",
      "Epoch #448 - training loss: 0.028488679331451713 \t validation loss: 0.02894318848848343\n",
      "Epoch #449 - training loss: 0.02833846282408477 \t validation loss: 0.029028648510575294\n",
      "Epoch #450 - training loss: 0.028451482227659374 \t validation loss: 0.02892821840941906\n",
      "Epoch #451 - training loss: 0.028402033935677497 \t validation loss: 0.028976833447813988\n",
      "Epoch #452 - training loss: 0.02835122377898119 \t validation loss: 0.02905907668173313\n",
      "Epoch #453 - training loss: 0.028407925048147688 \t validation loss: 0.02913336083292961\n",
      "Epoch #454 - training loss: 0.0283822176565472 \t validation loss: 0.028856897726655006\n",
      "Epoch #455 - training loss: 0.028438028866853888 \t validation loss: 0.02905271202325821\n",
      "Epoch #456 - training loss: 0.02850449320498126 \t validation loss: 0.02886236645281315\n",
      "Epoch #457 - training loss: 0.02846406178181447 \t validation loss: 0.02891119010746479\n",
      "Epoch #458 - training loss: 0.0283911917241546 \t validation loss: 0.029071763157844543\n",
      "Epoch #459 - training loss: 0.028405499436516077 \t validation loss: 0.028915569186210632\n",
      "Epoch #460 - training loss: 0.02840845579933555 \t validation loss: 0.028839120641350746\n",
      "Epoch #461 - training loss: 0.028428436621728153 \t validation loss: 0.02888508141040802\n",
      "Epoch #462 - training loss: 0.028494280636872614 \t validation loss: 0.028982901945710182\n",
      "Epoch #463 - training loss: 0.02834943639802633 \t validation loss: 0.029010601341724396\n",
      "Epoch #464 - training loss: 0.028492072071137926 \t validation loss: 0.029075974598526955\n",
      "Epoch #465 - training loss: 0.02851422572494043 \t validation loss: 0.028999801725149155\n",
      "Epoch #466 - training loss: 0.02847017416626042 \t validation loss: 0.028922421857714653\n",
      "Epoch #467 - training loss: 0.028469337891729646 \t validation loss: 0.02912724018096924\n",
      "Epoch #468 - training loss: 0.028388064629371264 \t validation loss: 0.028901034966111183\n",
      "Best validation loss at epoch=469, saving model\n",
      "Epoch #469 - training loss: 0.028356581267934992 \t validation loss: 0.028776532039046288\n",
      "Epoch #470 - training loss: 0.028301974490097574 \t validation loss: 0.028845973312854767\n",
      "Epoch #471 - training loss: 0.028442086076374728 \t validation loss: 0.028884101659059525\n",
      "Epoch #472 - training loss: 0.02843797868081662 \t validation loss: 0.02887963131070137\n",
      "Epoch #473 - training loss: 0.02848787145055038 \t validation loss: 0.02904149889945984\n",
      "Epoch #474 - training loss: 0.028624224872823727 \t validation loss: 0.028856799006462097\n",
      "Epoch #475 - training loss: 0.02840449145726672 \t validation loss: 0.02891051024198532\n",
      "Epoch #476 - training loss: 0.028412644015499023 \t validation loss: 0.02889924868941307\n",
      "Epoch #477 - training loss: 0.028388480300973646 \t validation loss: 0.028829503804445267\n",
      "Epoch #478 - training loss: 0.028341076791653954 \t validation loss: 0.028855355456471443\n",
      "Epoch #479 - training loss: 0.028398412458123044 \t validation loss: 0.028780071064829826\n",
      "Epoch #480 - training loss: 0.02828137970770884 \t validation loss: 0.028809763491153717\n",
      "Epoch #481 - training loss: 0.028388721990890844 \t validation loss: 0.028879337012767792\n",
      "Epoch #482 - training loss: 0.02837024238753192 \t validation loss: 0.02878596819937229\n",
      "Epoch #483 - training loss: 0.028454180079110585 \t validation loss: 0.028894584625959396\n",
      "Epoch #484 - training loss: 0.028296632899378553 \t validation loss: 0.028896212577819824\n",
      "Epoch #485 - training loss: 0.028363774445023193 \t validation loss: 0.028934940695762634\n",
      "Epoch #486 - training loss: 0.028450574227779874 \t validation loss: 0.028827378526329994\n",
      "Epoch #487 - training loss: 0.028461903681333684 \t validation loss: 0.028887197375297546\n",
      "Epoch #488 - training loss: 0.02838476624148871 \t validation loss: 0.028896089643239975\n",
      "Epoch #489 - training loss: 0.02844425384946009 \t validation loss: 0.029059382155537605\n",
      "Epoch #490 - training loss: 0.028314221164885352 \t validation loss: 0.02881372906267643\n",
      "Epoch #491 - training loss: 0.028350740908942323 \t validation loss: 0.02904733084142208\n",
      "Epoch #492 - training loss: 0.028373067676156355 \t validation loss: 0.02886572852730751\n",
      "Epoch #493 - training loss: 0.028316966874968093 \t validation loss: 0.028818557038903236\n",
      "Epoch #494 - training loss: 0.028360521576657343 \t validation loss: 0.028890302404761314\n",
      "Best validation loss at epoch=495, saving model\n",
      "Epoch #495 - training loss: 0.028373554989342475 \t validation loss: 0.028750531375408173\n",
      "Epoch #496 - training loss: 0.02829386370447491 \t validation loss: 0.02892645075917244\n",
      "Epoch #497 - training loss: 0.028416523014553743 \t validation loss: 0.028846681118011475\n",
      "Epoch #498 - training loss: 0.028379127579584457 \t validation loss: 0.028886981308460236\n",
      "Epoch #499 - training loss: 0.028372912884760123 \t validation loss: 0.02918313257396221\n",
      "Epoch #500 - training loss: 0.028419944090224674 \t validation loss: 0.02923857793211937\n",
      "             productid  predictedyQuantity\n",
      "0  440880169_redorange           10.919061\n",
      "1      441066672_peach            3.616292\n",
      "2      441066672_beige            4.160666\n",
      "3    460997795_mustard            4.678298\n",
      "4     469228238_indigo          128.074997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172410/1884702822.py:36: FutureWarning: Passing 'use_legacy_dataset=True' to get the legacy behaviour is deprecated as of pyarrow 8.0.0, and the legacy implementation will be removed in a future version.\n",
      "  pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=path, filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)\n"
     ]
    }
   ],
   "source": [
    "experiment3(\"menShirts\", 2, epochs=500, lr=0.0005)\n",
    "experiment3(\"womenKurtas\", 2, epochs=500, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30740, 34)\n",
      "(97077, 33)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30740 entries, 0 to 30739\n",
      "Data columns (total 34 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   similargrouplevel       30740 non-null  object \n",
      " 1   productid               30740 non-null  object \n",
      " 2   0_sales                 30740 non-null  float64\n",
      " 3   1_sales                 30740 non-null  float64\n",
      " 4   2_sales                 30740 non-null  float64\n",
      " 5   3_sales                 30740 non-null  float64\n",
      " 6   4_sales                 30740 non-null  float64\n",
      " 7   5_sales                 30740 non-null  float64\n",
      " 8   6_sales                 30740 non-null  float64\n",
      " 9   7_sales                 30740 non-null  float64\n",
      " 10  8_sales                 30740 non-null  float64\n",
      " 11  9_sales                 30740 non-null  float64\n",
      " 12  sales_avg_10_months     30740 non-null  float64\n",
      " 13  sales_std_10_months     30740 non-null  float64\n",
      " 14  totalPLPViews           30740 non-null  float64\n",
      " 15  totalPLPClicks          30740 non-null  float64\n",
      " 16  totalPDPCount           30740 non-null  float64\n",
      " 17  totalATC                30740 non-null  float64\n",
      " 18  totalReturns            30740 non-null  float64\n",
      " 19  totalUsers              30740 non-null  float64\n",
      " 20  totalAvailableQuantity  30740 non-null  float64\n",
      " 21  totalWishlist           30740 non-null  float64\n",
      " 22  pricebucket             30740 non-null  int32  \n",
      " 23  normalized_color_0      30740 non-null  float64\n",
      " 24  normalized_color_1      30740 non-null  float64\n",
      " 25  normalized_pattern_0    30740 non-null  float64\n",
      " 26  normalized_pattern_1    30740 non-null  float64\n",
      " 27  normalized_brand_0      30740 non-null  float64\n",
      " 28  normalized_brand_1      30740 non-null  float64\n",
      " 29  normalized_style_0      30740 non-null  float64\n",
      " 30  normalized_style_1      30740 non-null  float64\n",
      " 31  normalized_sleeve_0     30740 non-null  float64\n",
      " 32  normalized_sleeve_1     30740 non-null  float64\n",
      " 33  yQuantity               30740 non-null  float64\n",
      "dtypes: float64(31), int32(1), object(2)\n",
      "memory usage: 7.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97077 entries, 0 to 97076\n",
      "Data columns (total 33 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   similargrouplevel       97077 non-null  object \n",
      " 1   productid               97077 non-null  object \n",
      " 2   0_sales                 97077 non-null  float64\n",
      " 3   1_sales                 97077 non-null  float64\n",
      " 4   2_sales                 97077 non-null  float64\n",
      " 5   3_sales                 97077 non-null  float64\n",
      " 6   4_sales                 97077 non-null  float64\n",
      " 7   5_sales                 97077 non-null  float64\n",
      " 8   6_sales                 97077 non-null  float64\n",
      " 9   7_sales                 97077 non-null  float64\n",
      " 10  8_sales                 97077 non-null  float64\n",
      " 11  9_sales                 97077 non-null  float64\n",
      " 12  sales_avg_10_months     97077 non-null  float64\n",
      " 13  sales_std_10_months     97077 non-null  float64\n",
      " 14  totalPLPViews           97077 non-null  float64\n",
      " 15  totalPLPClicks          97077 non-null  float64\n",
      " 16  totalPDPCount           97077 non-null  float64\n",
      " 17  totalATC                97077 non-null  float64\n",
      " 18  totalReturns            97077 non-null  float64\n",
      " 19  totalUsers              97077 non-null  float64\n",
      " 20  totalAvailableQuantity  97077 non-null  float64\n",
      " 21  totalWishlist           97077 non-null  float64\n",
      " 22  pricebucket             97077 non-null  int32  \n",
      " 23  normalized_color_0      97077 non-null  float64\n",
      " 24  normalized_color_1      97077 non-null  float64\n",
      " 25  normalized_pattern_0    97077 non-null  float64\n",
      " 26  normalized_pattern_1    97077 non-null  float64\n",
      " 27  normalized_brand_0      97077 non-null  float64\n",
      " 28  normalized_brand_1      97077 non-null  float64\n",
      " 29  normalized_style_0      97077 non-null  float64\n",
      " 30  normalized_style_1      97077 non-null  float64\n",
      " 31  normalized_sleeve_0     97077 non-null  float64\n",
      " 32  normalized_sleeve_1     97077 non-null  float64\n",
      "dtypes: float64(30), int32(1), object(2)\n",
      "memory usage: 24.1+ MB\n",
      "['0_sales', '1_sales', '2_sales', '3_sales', '4_sales', '5_sales', '6_sales', '7_sales', '8_sales', '9_sales', 'sales_avg_10_months', 'sales_std_10_months', 'totalPLPViews', 'totalPLPClicks', 'totalPDPCount', 'totalATC', 'totalReturns', 'totalUsers', 'totalAvailableQuantity', 'totalWishlist', 'pricebucket']\n",
      "[1.] [7795.]\n",
      "(25149, 34) (6148, 34)\n",
      "cuda:0\n",
      "Best validation loss at epoch=1, saving model\n",
      "Epoch #1 - training loss: 0.08505622547202904 \t validation loss: 0.02453690767288208\n",
      "Best validation loss at epoch=2, saving model\n",
      "Epoch #2 - training loss: 0.026126773814985137 \t validation loss: 0.02225470170378685\n",
      "Best validation loss at epoch=3, saving model\n",
      "Epoch #3 - training loss: 0.02516779698324716 \t validation loss: 0.0211508646607399\n",
      "Epoch #4 - training loss: 0.024379980812074653 \t validation loss: 0.021463148295879364\n",
      "Epoch #5 - training loss: 0.023919370052090307 \t validation loss: 0.02140866033732891\n",
      "Best validation loss at epoch=6, saving model\n",
      "Epoch #6 - training loss: 0.023772190548598197 \t validation loss: 0.020672811195254326\n",
      "Epoch #7 - training loss: 0.023616398604005384 \t validation loss: 0.020691324025392532\n",
      "Epoch #8 - training loss: 0.023026402042526158 \t validation loss: 0.020954463630914688\n",
      "Best validation loss at epoch=9, saving model\n",
      "Epoch #9 - training loss: 0.022864388281206606 \t validation loss: 0.02046610787510872\n",
      "Best validation loss at epoch=10, saving model\n",
      "Epoch #10 - training loss: 0.02274336846260603 \t validation loss: 0.02036377787590027\n",
      "Epoch #11 - training loss: 0.02247440124369541 \t validation loss: 0.020399324595928192\n",
      "Best validation loss at epoch=12, saving model\n",
      "Epoch #12 - training loss: 0.0223333370812522 \t validation loss: 0.020186712965369225\n",
      "Best validation loss at epoch=13, saving model\n",
      "Epoch #13 - training loss: 0.02200193068154963 \t validation loss: 0.019976483657956123\n",
      "Epoch #14 - training loss: 0.021793436435804624 \t validation loss: 0.020039431750774384\n",
      "Epoch #15 - training loss: 0.021602296097215954 \t validation loss: 0.020124351605772972\n",
      "Epoch #16 - training loss: 0.021372280195414737 \t validation loss: 0.020045028999447823\n",
      "Best validation loss at epoch=17, saving model\n",
      "Epoch #17 - training loss: 0.021222894502340987 \t validation loss: 0.019833751022815704\n",
      "Epoch #18 - training loss: 0.021110108539248537 \t validation loss: 0.019910749047994614\n",
      "Epoch #19 - training loss: 0.021128207916144733 \t validation loss: 0.01985371857881546\n",
      "Best validation loss at epoch=20, saving model\n",
      "Epoch #20 - training loss: 0.020892019900285705 \t validation loss: 0.019782761111855507\n",
      "Best validation loss at epoch=21, saving model\n",
      "Epoch #21 - training loss: 0.02097264363602468 \t validation loss: 0.019677141681313515\n",
      "Best validation loss at epoch=22, saving model\n",
      "Epoch #22 - training loss: 0.020760625080579215 \t validation loss: 0.019615763798356056\n",
      "Epoch #23 - training loss: 0.02066502442500394 \t validation loss: 0.019638333469629288\n",
      "Best validation loss at epoch=24, saving model\n",
      "Epoch #24 - training loss: 0.02059908968746472 \t validation loss: 0.019615672528743744\n",
      "Epoch #25 - training loss: 0.020506951604987597 \t validation loss: 0.01961631327867508\n",
      "Best validation loss at epoch=26, saving model\n",
      "Epoch #26 - training loss: 0.02050727251650184 \t validation loss: 0.019569985568523407\n",
      "Epoch #27 - training loss: 0.020391926242358936 \t validation loss: 0.01960652694106102\n",
      "Epoch #28 - training loss: 0.02031195127007783 \t validation loss: 0.019616125151515007\n",
      "Epoch #29 - training loss: 0.020227796640721666 \t validation loss: 0.019570987671613693\n",
      "Best validation loss at epoch=30, saving model\n",
      "Epoch #30 - training loss: 0.020169334696119378 \t validation loss: 0.019499769434332848\n",
      "Epoch #31 - training loss: 0.020109366345557437 \t validation loss: 0.019505998119711876\n",
      "Best validation loss at epoch=32, saving model\n",
      "Epoch #32 - training loss: 0.020050273360209956 \t validation loss: 0.01944088004529476\n",
      "Epoch #33 - training loss: 0.02008137174104904 \t validation loss: 0.019444022327661514\n",
      "Epoch #34 - training loss: 0.019969062212925808 \t validation loss: 0.01952030137181282\n",
      "Epoch #35 - training loss: 0.019948536836820994 \t validation loss: 0.01951984129846096\n",
      "Epoch #36 - training loss: 0.019811797962126555 \t validation loss: 0.01944609172642231\n",
      "Epoch #37 - training loss: 0.019820423675760344 \t validation loss: 0.019622741267085075\n",
      "Epoch #38 - training loss: 0.01983238254217016 \t validation loss: 0.019452592357993126\n",
      "Best validation loss at epoch=39, saving model\n",
      "Epoch #39 - training loss: 0.019826674829148183 \t validation loss: 0.019412025809288025\n",
      "Epoch #40 - training loss: 0.01979592411903439 \t validation loss: 0.01960572972893715\n",
      "Epoch #41 - training loss: 0.019603976272758405 \t validation loss: 0.019412482157349586\n",
      "Best validation loss at epoch=42, saving model\n",
      "Epoch #42 - training loss: 0.019665773321465133 \t validation loss: 0.019377419725060463\n",
      "Epoch #43 - training loss: 0.019645435775327645 \t validation loss: 0.019398896023631096\n",
      "Epoch #44 - training loss: 0.01976889570534324 \t validation loss: 0.019413957372307777\n",
      "Best validation loss at epoch=45, saving model\n",
      "Epoch #45 - training loss: 0.019721546067980825 \t validation loss: 0.019339142367243767\n",
      "Epoch #46 - training loss: 0.019685039459282804 \t validation loss: 0.019455209374427795\n",
      "Epoch #47 - training loss: 0.019665396720123518 \t validation loss: 0.019386664032936096\n",
      "Best validation loss at epoch=48, saving model\n",
      "Epoch #48 - training loss: 0.019670553806469225 \t validation loss: 0.019322561100125313\n",
      "Epoch #49 - training loss: 0.019594379854358874 \t validation loss: 0.019395112991333008\n",
      "Best validation loss at epoch=50, saving model\n",
      "Epoch #50 - training loss: 0.019613626344236218 \t validation loss: 0.019315026700496674\n",
      "Epoch #51 - training loss: 0.019650324769792664 \t validation loss: 0.019399460405111313\n",
      "Best validation loss at epoch=52, saving model\n",
      "Epoch #52 - training loss: 0.019566615258646048 \t validation loss: 0.019303428009152412\n",
      "Epoch #53 - training loss: 0.019643477474452015 \t validation loss: 0.01937646046280861\n",
      "Epoch #54 - training loss: 0.019508922507001716 \t validation loss: 0.01937556453049183\n",
      "Epoch #55 - training loss: 0.01962136210737287 \t validation loss: 0.0194700974971056\n",
      "Epoch #56 - training loss: 0.019419889991130136 \t validation loss: 0.019389303401112556\n",
      "Epoch #57 - training loss: 0.019407060776783783 \t validation loss: 0.0193511713296175\n",
      "Epoch #58 - training loss: 0.019555987777176556 \t validation loss: 0.019328875467181206\n",
      "Epoch #59 - training loss: 0.019384780649250005 \t validation loss: 0.019468124955892563\n",
      "Epoch #60 - training loss: 0.01940497519199256 \t validation loss: 0.019341088831424713\n",
      "Epoch #61 - training loss: 0.019419761855084046 \t validation loss: 0.01939319260418415\n",
      "Epoch #62 - training loss: 0.019360940608605267 \t validation loss: 0.019366007298231125\n",
      "Epoch #63 - training loss: 0.01949668513005896 \t validation loss: 0.019360220059752464\n",
      "Epoch #64 - training loss: 0.01934852311387658 \t validation loss: 0.01937362365424633\n",
      "Epoch #65 - training loss: 0.019332852562782683 \t validation loss: 0.019335031509399414\n",
      "Epoch #66 - training loss: 0.019423965164193303 \t validation loss: 0.019433090463280678\n",
      "Epoch #67 - training loss: 0.01934430072184913 \t validation loss: 0.019392920657992363\n",
      "Epoch #68 - training loss: 0.01936533157802691 \t validation loss: 0.019330982118844986\n",
      "Epoch #69 - training loss: 0.019285515841144095 \t validation loss: 0.01937982626259327\n",
      "Epoch #70 - training loss: 0.019416247272662297 \t validation loss: 0.019343571737408638\n",
      "Epoch #71 - training loss: 0.01926357012099711 \t validation loss: 0.01934995874762535\n",
      "Epoch #72 - training loss: 0.019302153022319173 \t validation loss: 0.019360028207302094\n",
      "Epoch #73 - training loss: 0.01927251035116233 \t validation loss: 0.019460268318653107\n",
      "Epoch #74 - training loss: 0.019439788943061687 \t validation loss: 0.019327813759446144\n",
      "Epoch #75 - training loss: 0.019421646692997712 \t validation loss: 0.01937709003686905\n",
      "Epoch #76 - training loss: 0.019319798835926945 \t validation loss: 0.019440846517682076\n",
      "Best validation loss at epoch=77, saving model\n",
      "Epoch #77 - training loss: 0.019283347151532865 \t validation loss: 0.019257398322224617\n",
      "Epoch #78 - training loss: 0.019300142278788006 \t validation loss: 0.019326424226164818\n",
      "Epoch #79 - training loss: 0.019380819662266475 \t validation loss: 0.019294070079922676\n",
      "Epoch #80 - training loss: 0.019327512083895458 \t validation loss: 0.019303062930703163\n",
      "Epoch #81 - training loss: 0.019375765188389522 \t validation loss: 0.019289495423436165\n",
      "Best validation loss at epoch=82, saving model\n",
      "Epoch #82 - training loss: 0.019251065017584307 \t validation loss: 0.01922362856566906\n",
      "Epoch #83 - training loss: 0.019314600268058528 \t validation loss: 0.01939348317682743\n",
      "Epoch #84 - training loss: 0.01929914630403754 \t validation loss: 0.01922474429011345\n",
      "Epoch #85 - training loss: 0.019287044681300214 \t validation loss: 0.01934361457824707\n",
      "Epoch #86 - training loss: 0.019300211635032656 \t validation loss: 0.019331105053424835\n",
      "Epoch #87 - training loss: 0.019111684514921467 \t validation loss: 0.019364485517144203\n",
      "Epoch #88 - training loss: 0.01924250386299411 \t validation loss: 0.01940300315618515\n",
      "Epoch #89 - training loss: 0.019184527554472162 \t validation loss: 0.01933741196990013\n",
      "Epoch #90 - training loss: 0.01930647784055086 \t validation loss: 0.019290069118142128\n",
      "Epoch #91 - training loss: 0.019274001637960125 \t validation loss: 0.01928369142115116\n",
      "Epoch #92 - training loss: 0.019278876348784207 \t validation loss: 0.019245753064751625\n",
      "Epoch #93 - training loss: 0.01917442002802328 \t validation loss: 0.019255541265010834\n",
      "Epoch #94 - training loss: 0.019156977296421292 \t validation loss: 0.019304309040308\n",
      "Epoch #95 - training loss: 0.019242034408198612 \t validation loss: 0.01931883580982685\n",
      "Epoch #96 - training loss: 0.0192339099842794 \t validation loss: 0.019287927076220512\n",
      "Epoch #97 - training loss: 0.019177471650586385 \t validation loss: 0.01935468427836895\n",
      "Epoch #98 - training loss: 0.019184832815911358 \t validation loss: 0.01923530548810959\n",
      "Epoch #99 - training loss: 0.019259620634957578 \t validation loss: 0.01923259161412716\n",
      "Epoch #100 - training loss: 0.019118629734381845 \t validation loss: 0.019237708300352097\n",
      "Epoch #101 - training loss: 0.019224206959105032 \t validation loss: 0.019281134009361267\n",
      "Epoch #102 - training loss: 0.019161909857207234 \t validation loss: 0.019289135932922363\n",
      "Best validation loss at epoch=103, saving model\n",
      "Epoch #103 - training loss: 0.019076495475260316 \t validation loss: 0.01921687461435795\n",
      "Epoch #104 - training loss: 0.019171987907593228 \t validation loss: 0.019297916442155838\n",
      "Epoch #105 - training loss: 0.019130251006739346 \t validation loss: 0.01927107200026512\n",
      "Best validation loss at epoch=106, saving model\n",
      "Epoch #106 - training loss: 0.019025884736566596 \t validation loss: 0.019178595393896103\n",
      "Epoch #107 - training loss: 0.01914134730056972 \t validation loss: 0.019262369722127914\n",
      "Epoch #108 - training loss: 0.01906233825691187 \t validation loss: 0.019295096397399902\n",
      "Epoch #109 - training loss: 0.01911020884616597 \t validation loss: 0.019236037507653236\n",
      "Best validation loss at epoch=110, saving model\n",
      "Epoch #110 - training loss: 0.019156337752167112 \t validation loss: 0.019126569852232933\n",
      "Epoch #111 - training loss: 0.019058759277686476 \t validation loss: 0.01930285431444645\n",
      "Epoch #112 - training loss: 0.019189146619622306 \t validation loss: 0.01927708275616169\n",
      "Epoch #113 - training loss: 0.019035889331702215 \t validation loss: 0.019202353432774544\n",
      "Epoch #114 - training loss: 0.019113614992090282 \t validation loss: 0.01914863847196102\n",
      "Epoch #115 - training loss: 0.019070552627906013 \t validation loss: 0.019218014553189278\n",
      "Epoch #116 - training loss: 0.019181864918061314 \t validation loss: 0.01927195116877556\n",
      "Epoch #117 - training loss: 0.019108418867015725 \t validation loss: 0.019210875034332275\n",
      "Epoch #118 - training loss: 0.019223338036922513 \t validation loss: 0.01930057257413864\n",
      "Epoch #119 - training loss: 0.01914308221156525 \t validation loss: 0.0192857813090086\n",
      "Epoch #120 - training loss: 0.019073034048222814 \t validation loss: 0.019215958192944527\n",
      "Best validation loss at epoch=121, saving model\n",
      "Epoch #121 - training loss: 0.019043139606168505 \t validation loss: 0.01912623643875122\n",
      "Epoch #122 - training loss: 0.0190192146048804 \t validation loss: 0.019157197326421738\n",
      "Epoch #123 - training loss: 0.01913338368995839 \t validation loss: 0.01919635199010372\n",
      "Epoch #124 - training loss: 0.019133574860229804 \t validation loss: 0.019284455105662346\n",
      "Epoch #125 - training loss: 0.01903084860580742 \t validation loss: 0.019218243658542633\n",
      "Epoch #126 - training loss: 0.01903021165412987 \t validation loss: 0.019136767834424973\n",
      "Epoch #127 - training loss: 0.019015884494919116 \t validation loss: 0.019200600683689117\n",
      "Epoch #128 - training loss: 0.018990651540054828 \t validation loss: 0.019180607050657272\n",
      "Epoch #129 - training loss: 0.019082383983798183 \t validation loss: 0.019238704815506935\n",
      "Best validation loss at epoch=130, saving model\n",
      "Epoch #130 - training loss: 0.018925373535010087 \t validation loss: 0.019095078110694885\n",
      "Epoch #131 - training loss: 0.018928580636834832 \t validation loss: 0.019133010879158974\n",
      "Epoch #132 - training loss: 0.018998394756738073 \t validation loss: 0.019095949828624725\n",
      "Epoch #133 - training loss: 0.018942051193718414 \t validation loss: 0.019163060933351517\n",
      "Epoch #134 - training loss: 0.019124705094701735 \t validation loss: 0.019123099744319916\n",
      "Epoch #135 - training loss: 0.018982799780691506 \t validation loss: 0.01915539801120758\n",
      "Epoch #136 - training loss: 0.0190203038595949 \t validation loss: 0.019183648750185966\n",
      "Epoch #137 - training loss: 0.01894865253596169 \t validation loss: 0.019169099628925323\n",
      "Epoch #138 - training loss: 0.018974878759402188 \t validation loss: 0.01911277323961258\n",
      "Epoch #139 - training loss: 0.01901745384760723 \t validation loss: 0.0191159937530756\n",
      "Best validation loss at epoch=140, saving model\n",
      "Epoch #140 - training loss: 0.01894065341289351 \t validation loss: 0.0190616175532341\n",
      "Best validation loss at epoch=141, saving model\n",
      "Epoch #141 - training loss: 0.018968060593992756 \t validation loss: 0.019030792638659477\n",
      "Epoch #142 - training loss: 0.018997306560587353 \t validation loss: 0.01905079372227192\n",
      "Epoch #143 - training loss: 0.019066077984489834 \t validation loss: 0.01917927712202072\n",
      "Epoch #144 - training loss: 0.01892932548935104 \t validation loss: 0.019261518493294716\n",
      "Epoch #145 - training loss: 0.01900672016926347 \t validation loss: 0.01914367824792862\n",
      "Epoch #146 - training loss: 0.0190995198013083 \t validation loss: 0.019177941605448723\n",
      "Epoch #147 - training loss: 0.019003125079640536 \t validation loss: 0.01920437626540661\n",
      "Epoch #148 - training loss: 0.01901986301597801 \t validation loss: 0.019228778779506683\n",
      "Epoch #149 - training loss: 0.019037079306876963 \t validation loss: 0.01913117617368698\n",
      "Epoch #150 - training loss: 0.018953188469894466 \t validation loss: 0.019214950501918793\n",
      "Epoch #151 - training loss: 0.019025739887432686 \t validation loss: 0.019084380939602852\n",
      "Epoch #152 - training loss: 0.019043007035662605 \t validation loss: 0.01925612986087799\n",
      "Epoch #153 - training loss: 0.01892851541637425 \t validation loss: 0.019169075414538383\n",
      "Best validation loss at epoch=154, saving model\n",
      "Epoch #154 - training loss: 0.018902631044079354 \t validation loss: 0.019015882164239883\n",
      "Epoch #155 - training loss: 0.018883789477537678 \t validation loss: 0.01918678544461727\n",
      "Epoch #156 - training loss: 0.018978445758971913 \t validation loss: 0.019111838191747665\n",
      "Epoch #157 - training loss: 0.01893293973129646 \t validation loss: 0.01916404999792576\n",
      "Epoch #158 - training loss: 0.019053747610539008 \t validation loss: 0.01909606158733368\n",
      "Epoch #159 - training loss: 0.01882914363163387 \t validation loss: 0.01914888434112072\n",
      "Epoch #160 - training loss: 0.01899778804783324 \t validation loss: 0.019292790442705154\n",
      "Epoch #161 - training loss: 0.01902380361144615 \t validation loss: 0.019202932715415955\n",
      "Epoch #162 - training loss: 0.018908208488207904 \t validation loss: 0.01907648704946041\n",
      "Epoch #163 - training loss: 0.01894533838008999 \t validation loss: 0.019071968272328377\n",
      "Epoch #164 - training loss: 0.018821619468248764 \t validation loss: 0.019030887633562088\n",
      "Epoch #165 - training loss: 0.01895218579229086 \t validation loss: 0.019096417352557182\n",
      "Epoch #166 - training loss: 0.018915169681333433 \t validation loss: 0.01912515237927437\n",
      "Epoch #167 - training loss: 0.01899913261929278 \t validation loss: 0.019128913059830666\n",
      "Epoch #168 - training loss: 0.018929698848563017 \t validation loss: 0.019046436995267868\n",
      "Epoch #169 - training loss: 0.018914023494929266 \t validation loss: 0.019073788076639175\n",
      "Epoch #170 - training loss: 0.018802507787017496 \t validation loss: 0.019016560167074203\n",
      "Epoch #171 - training loss: 0.018918443646780244 \t validation loss: 0.019103825092315674\n",
      "Epoch #172 - training loss: 0.018891853807126262 \t validation loss: 0.019088350236415863\n",
      "Epoch #173 - training loss: 0.01891103609684546 \t validation loss: 0.01916862279176712\n",
      "Epoch #174 - training loss: 0.019034714846735357 \t validation loss: 0.01932600885629654\n",
      "Epoch #175 - training loss: 0.018992221902960994 \t validation loss: 0.019121816381812096\n",
      "Best validation loss at epoch=176, saving model\n",
      "Epoch #176 - training loss: 0.018893847689269834 \t validation loss: 0.01900632679462433\n",
      "Epoch #177 - training loss: 0.018799831804813472 \t validation loss: 0.01908908598124981\n",
      "Epoch #178 - training loss: 0.018975233712239536 \t validation loss: 0.01912815496325493\n",
      "Epoch #179 - training loss: 0.01896499279281421 \t validation loss: 0.019080750644207\n",
      "Epoch #180 - training loss: 0.01890138993692246 \t validation loss: 0.019132280722260475\n",
      "Epoch #181 - training loss: 0.01882267663118898 \t validation loss: 0.019234927371144295\n",
      "Epoch #182 - training loss: 0.01898860554685401 \t validation loss: 0.019029753282666206\n",
      "Epoch #183 - training loss: 0.018904045625192344 \t validation loss: 0.019111674278974533\n",
      "Epoch #184 - training loss: 0.018832218372351994 \t validation loss: 0.0191195085644722\n",
      "Epoch #185 - training loss: 0.018852163436614975 \t validation loss: 0.01911412924528122\n",
      "Epoch #186 - training loss: 0.0188319609364032 \t validation loss: 0.019108129665255547\n",
      "Epoch #187 - training loss: 0.018882269253301772 \t validation loss: 0.01913120597600937\n",
      "Epoch #188 - training loss: 0.01892681489698589 \t validation loss: 0.01913284882903099\n",
      "Epoch #189 - training loss: 0.018981833905456172 \t validation loss: 0.019025195389986038\n",
      "Epoch #190 - training loss: 0.0188496277029893 \t validation loss: 0.019142504781484604\n",
      "Epoch #191 - training loss: 0.019051395467583353 \t validation loss: 0.019079772755503654\n",
      "Best validation loss at epoch=192, saving model\n",
      "Epoch #192 - training loss: 0.01885884976678878 \t validation loss: 0.01897682435810566\n",
      "Epoch #193 - training loss: 0.018752112652169195 \t validation loss: 0.019062530249357224\n",
      "Best validation loss at epoch=194, saving model\n",
      "Epoch #194 - training loss: 0.018829639565057246 \t validation loss: 0.018871143460273743\n",
      "Epoch #195 - training loss: 0.01889776247820467 \t validation loss: 0.01894625835120678\n",
      "Epoch #196 - training loss: 0.018953804401265588 \t validation loss: 0.019015885889530182\n",
      "Epoch #197 - training loss: 0.01895255294248794 \t validation loss: 0.019226565957069397\n",
      "Epoch #198 - training loss: 0.019005040825599697 \t validation loss: 0.01916840672492981\n",
      "Epoch #199 - training loss: 0.01888318250317625 \t validation loss: 0.019042743369936943\n",
      "Epoch #200 - training loss: 0.018903757446404 \t validation loss: 0.019088763743638992\n",
      "Epoch #201 - training loss: 0.018910885954596054 \t validation loss: 0.019216597080230713\n",
      "Epoch #202 - training loss: 0.01877886151989839 \t validation loss: 0.019109735265374184\n",
      "Epoch #203 - training loss: 0.01893339947660922 \t validation loss: 0.01919940672814846\n",
      "Epoch #204 - training loss: 0.018934988051913917 \t validation loss: 0.01908443309366703\n",
      "Epoch #205 - training loss: 0.018866093282937813 \t validation loss: 0.019183313474059105\n",
      "Epoch #206 - training loss: 0.018759494479508348 \t validation loss: 0.01915445365011692\n",
      "Epoch #207 - training loss: 0.018820474871024964 \t validation loss: 0.019173182547092438\n",
      "Epoch #208 - training loss: 0.01898024170392543 \t validation loss: 0.019065946340560913\n",
      "Epoch #209 - training loss: 0.018855012097295114 \t validation loss: 0.0191772673279047\n",
      "Epoch #210 - training loss: 0.018801537100640926 \t validation loss: 0.019171325489878654\n",
      "Epoch #211 - training loss: 0.018991786859644828 \t validation loss: 0.019008800387382507\n",
      "Epoch #212 - training loss: 0.018923746502311652 \t validation loss: 0.019148966297507286\n",
      "Epoch #213 - training loss: 0.01882445006008456 \t validation loss: 0.0191476009786129\n",
      "Epoch #214 - training loss: 0.018852730930907426 \t validation loss: 0.01911787874996662\n",
      "Epoch #215 - training loss: 0.01890165716006308 \t validation loss: 0.019160514697432518\n",
      "Epoch #216 - training loss: 0.018923426734485254 \t validation loss: 0.019113393500447273\n",
      "Epoch #217 - training loss: 0.018947442160373567 \t validation loss: 0.019142935052514076\n",
      "Epoch #218 - training loss: 0.018827636241556922 \t validation loss: 0.019175071269273758\n",
      "Epoch #219 - training loss: 0.018943813151329945 \t validation loss: 0.019134782254695892\n",
      "Epoch #220 - training loss: 0.018849855085692493 \t validation loss: 0.01926877535879612\n",
      "Epoch #221 - training loss: 0.018903740149015073 \t validation loss: 0.01903262920677662\n",
      "Epoch #222 - training loss: 0.018848150052652238 \t validation loss: 0.019065361469984055\n",
      "Epoch #223 - training loss: 0.01890020843114775 \t validation loss: 0.01911899447441101\n",
      "Epoch #224 - training loss: 0.018829680375375187 \t validation loss: 0.019020745530724525\n",
      "Epoch #225 - training loss: 0.01876390852051888 \t validation loss: 0.01914028450846672\n",
      "Epoch #226 - training loss: 0.01879112719777663 \t validation loss: 0.019031167030334473\n",
      "Epoch #227 - training loss: 0.018809426967707123 \t validation loss: 0.018980368971824646\n",
      "Epoch #228 - training loss: 0.018802978949336583 \t validation loss: 0.0191152673214674\n",
      "Epoch #229 - training loss: 0.01874396635136407 \t validation loss: 0.019124256446957588\n",
      "Epoch #230 - training loss: 0.018763239647315185 \t validation loss: 0.019024355337023735\n",
      "Epoch #231 - training loss: 0.018881747361486127 \t validation loss: 0.01903420500457287\n",
      "Epoch #232 - training loss: 0.018815864605093553 \t validation loss: 0.01905680261552334\n",
      "Epoch #233 - training loss: 0.018801284105677135 \t validation loss: 0.019142189994454384\n",
      "Epoch #234 - training loss: 0.018891756256128763 \t validation loss: 0.019094716757535934\n",
      "Epoch #235 - training loss: 0.01882079737082978 \t validation loss: 0.01902586594223976\n",
      "Epoch #236 - training loss: 0.01868884394147024 \t validation loss: 0.019024135544896126\n",
      "Epoch #237 - training loss: 0.01886577584015526 \t validation loss: 0.019035032019019127\n",
      "Epoch #238 - training loss: 0.018721485325390367 \t validation loss: 0.019043441861867905\n",
      "Epoch #239 - training loss: 0.018752171646708705 \t validation loss: 0.018990036100149155\n",
      "Epoch #240 - training loss: 0.018842292355171816 \t validation loss: 0.019073685631155968\n",
      "Epoch #241 - training loss: 0.018803300800478193 \t validation loss: 0.018957216292619705\n",
      "Epoch #242 - training loss: 0.018802872805553636 \t validation loss: 0.01897832751274109\n",
      "Epoch #243 - training loss: 0.018875368732579386 \t validation loss: 0.019164253026247025\n",
      "Epoch #244 - training loss: 0.018894431536578257 \t validation loss: 0.019083213061094284\n",
      "Epoch #245 - training loss: 0.018947018518047348 \t validation loss: 0.019046088680624962\n",
      "Epoch #246 - training loss: 0.01881346072894847 \t validation loss: 0.01901472546160221\n",
      "Epoch #247 - training loss: 0.018850459961315534 \t validation loss: 0.01905621774494648\n",
      "Epoch #248 - training loss: 0.01888285727495221 \t validation loss: 0.019081396982073784\n",
      "Epoch #249 - training loss: 0.018901358558766686 \t validation loss: 0.019147275015711784\n",
      "Epoch #250 - training loss: 0.018872111687543474 \t validation loss: 0.018980924040079117\n",
      "Epoch #251 - training loss: 0.018758167060103956 \t validation loss: 0.019084520637989044\n",
      "Epoch #252 - training loss: 0.018769036427104643 \t validation loss: 0.01910342276096344\n",
      "Epoch #253 - training loss: 0.01883055350215287 \t validation loss: 0.01908484473824501\n",
      "Epoch #254 - training loss: 0.018774194276304383 \t validation loss: 0.018997497856616974\n",
      "Epoch #255 - training loss: 0.01902625807205061 \t validation loss: 0.0191511083394289\n",
      "Epoch #256 - training loss: 0.018807374623123627 \t validation loss: 0.019174426794052124\n",
      "Epoch #257 - training loss: 0.018826827579504175 \t validation loss: 0.01911742612719536\n",
      "Epoch #258 - training loss: 0.018710076768591904 \t validation loss: 0.019195279106497765\n",
      "Epoch #259 - training loss: 0.018848997318025702 \t validation loss: 0.019075416028499603\n",
      "Epoch #260 - training loss: 0.01888106397833604 \t validation loss: 0.01909484900534153\n",
      "Epoch #261 - training loss: 0.01888025582198552 \t validation loss: 0.01900535635650158\n",
      "Epoch #262 - training loss: 0.018801609887949124 \t validation loss: 0.019175956025719643\n",
      "Epoch #263 - training loss: 0.01880532606344694 \t validation loss: 0.01908595860004425\n",
      "Epoch #264 - training loss: 0.018808535491501924 \t validation loss: 0.019038870930671692\n",
      "Epoch #265 - training loss: 0.01875366010305703 \t validation loss: 0.019155150279402733\n",
      "Epoch #266 - training loss: 0.018800041082190574 \t validation loss: 0.019095787778496742\n",
      "Epoch #267 - training loss: 0.01884547351313767 \t validation loss: 0.019099431112408638\n",
      "Epoch #268 - training loss: 0.018793134392494228 \t validation loss: 0.01910230703651905\n",
      "Epoch #269 - training loss: 0.01884545551873981 \t validation loss: 0.01903855986893177\n",
      "Epoch #270 - training loss: 0.018844906418958002 \t validation loss: 0.019019192084670067\n",
      "Epoch #271 - training loss: 0.01882140991829668 \t validation loss: 0.018939990550279617\n",
      "Epoch #272 - training loss: 0.018729090520957854 \t validation loss: 0.018989354372024536\n",
      "Epoch #273 - training loss: 0.01878039997521859 \t validation loss: 0.018897008150815964\n",
      "Epoch #274 - training loss: 0.01881738400359632 \t validation loss: 0.01894180104136467\n",
      "Epoch #275 - training loss: 0.018804159031433475 \t validation loss: 0.019143279641866684\n",
      "Epoch #276 - training loss: 0.01884139246740349 \t validation loss: 0.019146472215652466\n",
      "Epoch #277 - training loss: 0.018830205788420644 \t validation loss: 0.019020533189177513\n",
      "Epoch #278 - training loss: 0.01883956626662688 \t validation loss: 0.01909002475440502\n",
      "Epoch #279 - training loss: 0.01876958579590224 \t validation loss: 0.01897352747619152\n",
      "Epoch #280 - training loss: 0.018794489053973726 \t validation loss: 0.018929066136479378\n",
      "Epoch #281 - training loss: 0.01873332440770071 \t validation loss: 0.019116235896945\n",
      "Epoch #282 - training loss: 0.018754064834196192 \t validation loss: 0.01909084990620613\n",
      "Epoch #283 - training loss: 0.018751279927885077 \t validation loss: 0.01915912516415119\n",
      "Epoch #284 - training loss: 0.018857666447011243 \t validation loss: 0.019083980470895767\n",
      "Epoch #285 - training loss: 0.01861713930258563 \t validation loss: 0.018957260996103287\n",
      "Epoch #286 - training loss: 0.01885630931278133 \t validation loss: 0.01901227980852127\n",
      "Epoch #287 - training loss: 0.018717442057243766 \t validation loss: 0.01898130588233471\n",
      "Epoch #288 - training loss: 0.018767377915999788 \t validation loss: 0.019048409536480904\n",
      "Epoch #289 - training loss: 0.018689027592943164 \t validation loss: 0.018965577706694603\n",
      "Epoch #290 - training loss: 0.01883704749226665 \t validation loss: 0.019128326326608658\n",
      "Epoch #291 - training loss: 0.01877554597149182 \t validation loss: 0.01907162368297577\n",
      "Epoch #292 - training loss: 0.01870873919243266 \t validation loss: 0.01900293119251728\n",
      "Epoch #293 - training loss: 0.01869370260625888 \t validation loss: 0.018946973606944084\n",
      "Epoch #294 - training loss: 0.018850527950532877 \t validation loss: 0.019071761518716812\n",
      "Epoch #295 - training loss: 0.018701315838487664 \t validation loss: 0.019129078835248947\n",
      "Epoch #296 - training loss: 0.018737074825912713 \t validation loss: 0.0189707949757576\n",
      "Epoch #297 - training loss: 0.018791500550166818 \t validation loss: 0.019168034195899963\n",
      "Epoch #298 - training loss: 0.018679496895664248 \t validation loss: 0.018987540155649185\n",
      "Epoch #299 - training loss: 0.018948197647169897 \t validation loss: 0.019223446026444435\n",
      "Epoch #300 - training loss: 0.018800496253345136 \t validation loss: 0.01903090998530388\n",
      "Epoch #301 - training loss: 0.018782048641591314 \t validation loss: 0.019067537039518356\n",
      "Epoch #302 - training loss: 0.01876452934578251 \t validation loss: 0.01907842792570591\n",
      "Epoch #303 - training loss: 0.01883112458470663 \t validation loss: 0.019003378227353096\n",
      "Epoch #304 - training loss: 0.018742309275801016 \t validation loss: 0.019041569903492928\n",
      "Epoch #305 - training loss: 0.018782693231893573 \t validation loss: 0.019037362188100815\n",
      "Epoch #306 - training loss: 0.018792716589323275 \t validation loss: 0.019157780334353447\n",
      "Epoch #307 - training loss: 0.018844936746558186 \t validation loss: 0.019150452688336372\n",
      "Epoch #308 - training loss: 0.018704583544497656 \t validation loss: 0.019001759588718414\n",
      "Epoch #309 - training loss: 0.01882173252458073 \t validation loss: 0.019000576809048653\n",
      "Epoch #310 - training loss: 0.018776398997718265 \t validation loss: 0.01917298696935177\n",
      "Epoch #311 - training loss: 0.018771808248630183 \t validation loss: 0.019143860787153244\n",
      "Epoch #312 - training loss: 0.01871553094891511 \t validation loss: 0.019125906750559807\n",
      "Epoch #313 - training loss: 0.018836353136119758 \t validation loss: 0.01911965385079384\n",
      "Epoch #314 - training loss: 0.018745683079621024 \t validation loss: 0.019001413136720657\n",
      "Epoch #315 - training loss: 0.018794015711351374 \t validation loss: 0.019113482907414436\n",
      "Epoch #316 - training loss: 0.018850323936907918 \t validation loss: 0.019122129306197166\n",
      "Epoch #317 - training loss: 0.018718414744184276 \t validation loss: 0.019036613404750824\n",
      "Epoch #318 - training loss: 0.018705199854626396 \t validation loss: 0.019145416095852852\n",
      "Epoch #319 - training loss: 0.018857906255159217 \t validation loss: 0.01913493499159813\n",
      "Epoch #320 - training loss: 0.018808060324495765 \t validation loss: 0.01912659779191017\n",
      "Epoch #321 - training loss: 0.018689704155193488 \t validation loss: 0.01912604086101055\n",
      "Epoch #322 - training loss: 0.018698873322243525 \t validation loss: 0.019139986485242844\n",
      "Epoch #323 - training loss: 0.018728078135651104 \t validation loss: 0.01917072758078575\n",
      "Epoch #324 - training loss: 0.018794978707198314 \t validation loss: 0.019027844071388245\n",
      "Epoch #325 - training loss: 0.018817296432580347 \t validation loss: 0.01907569356262684\n",
      "Epoch #326 - training loss: 0.018722623748004815 \t validation loss: 0.019012004137039185\n",
      "Epoch #327 - training loss: 0.018593890343905444 \t validation loss: 0.019101925194263458\n",
      "Epoch #328 - training loss: 0.018771492266896995 \t validation loss: 0.0191104244440794\n",
      "Epoch #329 - training loss: 0.01887018694067789 \t validation loss: 0.019115781411528587\n",
      "Epoch #330 - training loss: 0.018730554798214585 \t validation loss: 0.019158871844410896\n",
      "Epoch #331 - training loss: 0.018855617351082574 \t validation loss: 0.01912512630224228\n",
      "Epoch #332 - training loss: 0.018846434035937592 \t validation loss: 0.019107075408101082\n",
      "Epoch #333 - training loss: 0.018865449992927965 \t validation loss: 0.019167304039001465\n",
      "Epoch #334 - training loss: 0.01882465587377216 \t validation loss: 0.019168488681316376\n",
      "Epoch #335 - training loss: 0.018763327372562925 \t validation loss: 0.019069552421569824\n",
      "Epoch #336 - training loss: 0.018713678353448296 \t validation loss: 0.019032834097743034\n",
      "Epoch #337 - training loss: 0.018748772707856763 \t validation loss: 0.01903623528778553\n",
      "Epoch #338 - training loss: 0.018805794651281967 \t validation loss: 0.01903754658997059\n",
      "Epoch #339 - training loss: 0.018768035963320048 \t validation loss: 0.019039293751120567\n",
      "Epoch #340 - training loss: 0.01874918971970013 \t validation loss: 0.01902819238603115\n",
      "Epoch #341 - training loss: 0.018761276811432498 \t validation loss: 0.01895112358033657\n",
      "Epoch #342 - training loss: 0.018769550834823946 \t validation loss: 0.019081711769104004\n",
      "Epoch #343 - training loss: 0.018776967030042297 \t validation loss: 0.0192903783172369\n",
      "Epoch #344 - training loss: 0.018763564666139945 \t validation loss: 0.019212543964385986\n",
      "Epoch #345 - training loss: 0.01886165148109957 \t validation loss: 0.019033540040254593\n",
      "Epoch #346 - training loss: 0.018640789705488808 \t validation loss: 0.019094213843345642\n",
      "Epoch #347 - training loss: 0.018751134218019286 \t validation loss: 0.01913868449628353\n",
      "Epoch #348 - training loss: 0.018669360079416993 \t validation loss: 0.019198307767510414\n",
      "Epoch #349 - training loss: 0.018579431028538353 \t validation loss: 0.01911485753953457\n",
      "Epoch #350 - training loss: 0.018688212425869172 \t validation loss: 0.01922723464667797\n",
      "Epoch #351 - training loss: 0.018823568163451494 \t validation loss: 0.019160659983754158\n",
      "Epoch #352 - training loss: 0.018756860913412207 \t validation loss: 0.019186342135071754\n",
      "Epoch #353 - training loss: 0.018883877064866626 \t validation loss: 0.019163507968187332\n",
      "Epoch #354 - training loss: 0.018641752093010076 \t validation loss: 0.019132912158966064\n",
      "Epoch #355 - training loss: 0.01873300338763103 \t validation loss: 0.018999619409441948\n",
      "Epoch #356 - training loss: 0.018834057656727778 \t validation loss: 0.019209973514080048\n",
      "Epoch #357 - training loss: 0.018762166790106588 \t validation loss: 0.019211819395422935\n",
      "Epoch #358 - training loss: 0.01866334080013927 \t validation loss: 0.01914665661752224\n",
      "Epoch #359 - training loss: 0.018792922543302462 \t validation loss: 0.019298840314149857\n",
      "Epoch #360 - training loss: 0.01868074081043244 \t validation loss: 0.019117649644613266\n",
      "Epoch #361 - training loss: 0.01869574175127278 \t validation loss: 0.019226321950554848\n",
      "Epoch #362 - training loss: 0.018688360668695086 \t validation loss: 0.019212618470191956\n",
      "Epoch #363 - training loss: 0.018767947719023105 \t validation loss: 0.019225111231207848\n",
      "Epoch #364 - training loss: 0.01871324900751281 \t validation loss: 0.019135983660817146\n",
      "Epoch #365 - training loss: 0.0188379492589931 \t validation loss: 0.019217899069190025\n",
      "Epoch #366 - training loss: 0.018762347490710627 \t validation loss: 0.019030550494790077\n",
      "Epoch #367 - training loss: 0.018912219085321307 \t validation loss: 0.0190728846937418\n",
      "Epoch #368 - training loss: 0.01867501750042674 \t validation loss: 0.01904931478202343\n",
      "Epoch #369 - training loss: 0.018811829497290267 \t validation loss: 0.01911533623933792\n",
      "Epoch #370 - training loss: 0.018687934563406238 \t validation loss: 0.019164005294442177\n",
      "Epoch #371 - training loss: 0.018830121953372553 \t validation loss: 0.019208718091249466\n",
      "Epoch #372 - training loss: 0.01874206887297095 \t validation loss: 0.019137753173708916\n",
      "Epoch #373 - training loss: 0.018693237865913162 \t validation loss: 0.019138766452670097\n",
      "Epoch #374 - training loss: 0.018660682762504384 \t validation loss: 0.01914939098060131\n",
      "Epoch #375 - training loss: 0.018701061605813966 \t validation loss: 0.01908140443265438\n",
      "Epoch #376 - training loss: 0.018769169892451375 \t validation loss: 0.01896214671432972\n",
      "Epoch #377 - training loss: 0.018750940899477358 \t validation loss: 0.01912703365087509\n",
      "Epoch #378 - training loss: 0.018727780908203807 \t validation loss: 0.01907554641366005\n",
      "Epoch #379 - training loss: 0.018704176649664808 \t validation loss: 0.01904568262398243\n",
      "Epoch #380 - training loss: 0.01862061306846085 \t validation loss: 0.018981624394655228\n",
      "Epoch #381 - training loss: 0.01868153454225724 \t validation loss: 0.019031977280974388\n",
      "Epoch #382 - training loss: 0.01882292956357027 \t validation loss: 0.01917543075978756\n",
      "Epoch #383 - training loss: 0.018767501269378196 \t validation loss: 0.019019771367311478\n",
      "Epoch #384 - training loss: 0.018776537671946227 \t validation loss: 0.019057821482419968\n",
      "Epoch #385 - training loss: 0.01874767845010112 \t validation loss: 0.019023260101675987\n",
      "Epoch #386 - training loss: 0.018674229273192917 \t validation loss: 0.019031722098588943\n",
      "Epoch #387 - training loss: 0.01869748883678987 \t validation loss: 0.019080735743045807\n",
      "Epoch #388 - training loss: 0.01869358543490481 \t validation loss: 0.019129762426018715\n"
     ]
    }
   ],
   "source": [
    "experiment3(\"menShirts\", 3, epochs=500, lr=0.0005)\n",
    "experiment3(\"womenKurtas\", 3, epochs=500, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment3(\"menShirts\", 4, epochs=500, lr=0.0005)\n",
    "experiment3(\"womenKurtas\", 4, epochs=500, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment3(\"menShirts\", 5, epochs=500, lr=0.0005)\n",
    "experiment3(\"womenKurtas\", 5, epochs=500, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment3(\"menShirts\", 6, epochs=500, lr=0.0005)\n",
    "experiment3(\"womenKurtas\", 6, epochs=500, lr=0.0005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_env",
   "language": "python",
   "name": "api_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
