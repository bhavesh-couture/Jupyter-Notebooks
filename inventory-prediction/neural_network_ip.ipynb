{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "fs = pa.hdfs.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_percentage_error as mape, r2_score\n",
    "def metrics(actual, predicted):    \n",
    "    y_true = actual\n",
    "    y_pred = predicted\n",
    "    print(f\"rmse: {mse(y_true, y_pred)**(0.5)}\")\n",
    "    print(f\"mape: {mape(y_true, y_pred)}\")\n",
    "    print(f\"r2_score: {r2_score(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate = 0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.fc1(x)\n",
    "      x = self.relu1(x)\n",
    "      x = self.dropout1(x)\n",
    "      x = self.fc2(x)\n",
    "      x = self.relu2(x)\n",
    "      x = self.dropout2(x)\n",
    "      x = self.fc3(x)\n",
    "      x = self.sigmoid(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "  class _Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "      self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "      self.x = torch.tensor(x).float().to(self.device)\n",
    "      self.y = torch.tensor(y).float().reshape(-1, 1).to(self.device)\n",
    "    \n",
    "    def __len__(self, ):\n",
    "      return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "      x = self.x[idx]\n",
    "      y = self.y[idx]\n",
    "      return x, y\n",
    "\n",
    "  def __init__(self, neural_network, neural_network_layers_info):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self._neural_network = neural_network\n",
    "    self._neural_network_layers_info = neural_network_layers_info\n",
    "    self.model = self._neural_network(**self._neural_network_layers_info).to(self.device)\n",
    "    self._training_done = False\n",
    "    self._compile_done = False\n",
    "\n",
    "  def __clear(self, ):\n",
    "    self.model = self._neural_network(**self._neural_network_layers_info)\n",
    "    self._training_done = False\n",
    "    self._compile_done = False\n",
    "    self._validation_loss_history = []\n",
    "\n",
    "  def compile(self, criterion, optimizer, learning_rate):\n",
    "    if self._training_done:\n",
    "      raise Exception(\"Cannot compile an already trained model\")\n",
    "    \n",
    "    if self._compile_done:\n",
    "      print(\"Warn: Previously trained Model will be replaced.\")\n",
    "      self.__clear()\n",
    "\n",
    "    self.criterion = criterion\n",
    "    self._optimizer = optimizer\n",
    "    self._lr = learning_rate\n",
    "    self.optimizer = self._optimizer(self.model.parameters(), lr = self._lr)\n",
    "    self._compile_done = True\n",
    "\n",
    "  def _get_dataset_loader(self, x, y, batch_size):\n",
    "    dataset = self._Dataset(x, y)\n",
    "    dataset_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "    return dataset_loader\n",
    "  \n",
    "  def fit(self, x_train, y_train, num_epochs, batch_size = 32, x_val = None, y_val = None, early_stopping = None):\n",
    "    if not self._compile_done:\n",
    "      raise Exception(\"compile before training\")\n",
    "    \n",
    "    if self._training_done:\n",
    "      print(\"Warn: Previously trained Model will be replaced.\")\n",
    "      self.__clear()\n",
    "      self.compile(self.criterion, self._optimizer, self._lr)\n",
    "    \n",
    "    do_validation = (x_val is not None) and (y_val is not None)\n",
    "\n",
    "    train_data_loader = self._get_dataset_loader(x_train, y_train, batch_size)\n",
    "    if do_validation:\n",
    "      validation_data = self._Dataset(x_val, y_val)\n",
    "      if not early_stopping:\n",
    "        early_stopping = np.inf\n",
    "      self._fit_with_validation(train_data_loader, validation_data, epochs = num_epochs, early_stopping = early_stopping)\n",
    "    else:\n",
    "      self._fit_without_validation(train_data_loader, epochs = num_epochs)\n",
    "\n",
    "    self._training_done = True\n",
    "\n",
    "  def predict(self, x_test):\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "      x_test = torch.tensor(x_test).float().to(self.device)\n",
    "      y_test = self.model(x_test)\n",
    "    \n",
    "      return y_test.cpu().detach().numpy()\n",
    "    \n",
    "  def _fit_with_validation(self, train_data_loader, validation_data, epochs, early_stopping):\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    val_loss_history = []\n",
    "    best_model = None\n",
    "    for epoch in range(epochs):\n",
    "      if epoch - best_epoch > early_stopping:\n",
    "        print(\"Early stopping at epoch:\", epoch+1)\n",
    "        break\n",
    "      \n",
    "      self.model.train()\n",
    "      train_loss = []\n",
    "\n",
    "      for x, y in train_data_loader:\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        train_loss.append(float(loss.item()))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "      self.model.eval()\n",
    "      with torch.no_grad():\n",
    "        y_pred = self.model(validation_data.x)\n",
    "        val_loss = float(self.criterion(y_pred, validation_data.y))\n",
    "        val_loss_history.append(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "          print(f\"Best validation loss at epoch={epoch+1}, saving model\")\n",
    "          best_val_loss = val_loss\n",
    "          best_epoch = epoch + 1\n",
    "          best_model = self.model.state_dict().copy()\n",
    "      \n",
    "        print(f\"Epoch #{epoch + 1} - training loss: {sum(train_loss)/len(train_loss)} \\t validation loss: {val_loss}\")\n",
    "    self.model.load_state_dict(best_model)\n",
    "    self._validation_loss_history = val_loss_history\n",
    "\n",
    "  def _fit_without_validation(self, train_data_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "      self.model.train()\n",
    "      train_loss = []\n",
    "\n",
    "      for x, y in train_data_loader:\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        train_loss.append(float(loss.item()))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "      print(f\"Epoch #{epoch + 1} - training loss: {sum(train_loss)/len(train_loss)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train, val_size = 0.2):\n",
    "    train = train.sort_values(by=[\"yQuantity\"]).reset_index(drop=True)\n",
    "    val_indexes = np.array([])\n",
    "    for i in range(0, train.shape[0], 50):\n",
    "        left = i\n",
    "        right = min(i+50, train.shape[0])\n",
    "        cnt = (right - left) * val_size\n",
    "\n",
    "        # get cnt random numbers between [left, right)\n",
    "        idx = np.random.randint(left, right, int(cnt))\n",
    "        val_indexes = np.concatenate((val_indexes, idx))\n",
    "    \n",
    "    validation_data = train.iloc[val_indexes.astype(int)]\n",
    "    train_data = train[~train.index.isin(val_indexes.astype(int))]\n",
    "    return train_data, validation_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with baseline model's features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/data/Archive/bhavesh/inventoryPrediction\"\n",
    "prediction_start_date = \"2023-04-16\"\n",
    "train_path = {\n",
    "    \"menShirts\": f\"{base_dir}/TransformedDataset/date_when_prediction_is_made={prediction_start_date}/menShirts/data/train\",\n",
    "    \"womenKurtas\": f\"{base_dir}/TransformedDataset/date_when_prediction_is_made={prediction_start_date}/womenKurtas/data/train\",\n",
    "}\n",
    "test_path = {\n",
    "    \"menShirts\": f\"{base_dir}/TransformedDataset/date_when_prediction_is_made={prediction_start_date}/menShirts/data/test\",\n",
    "    \"womenKurtas\": f\"{base_dir}/TransformedDataset/date_when_prediction_is_made={prediction_start_date}/womenKurtas/data/test\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(of):\n",
    "    train = pq.ParquetDataset(train_path[of], fs).read().to_pandas()\n",
    "    test = pq.ParquetDataset(test_path[of], fs).read().to_pandas()\n",
    "\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    train.info()\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(of, epochs = 200, lr = 0.001):\n",
    "    train, test = fetch_data(of)\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    cols = list(filter(lambda x: not (x.endswith(\"0\") or x.endswith(\"1\")), train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]).columns))\n",
    "    print(cols)\n",
    "    train.loc[:, cols] = scaler_x.fit_transform(train[cols])\n",
    "    train[\"yQuantity\"] = scaler_y.fit_transform(train[[\"yQuantity\"]])\n",
    "    print(scaler_y.data_min_, scaler_y.data_max_)\n",
    "\n",
    "    train, val = train_val_split(train)\n",
    "    print(train.shape, val.shape)\n",
    "\n",
    "    x_train, y_train = train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), train[\"yQuantity\"]\n",
    "    x_val, y_val = val.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), val[\"yQuantity\"]\n",
    "\n",
    "    neural_network = Model(NeuralNetwork, {\"input_size\": 33, \"output_size\": 1, \"hidden_size1\": 66, \"hidden_size2\": 33})\n",
    "    print(neural_network.device)\n",
    "    neural_network.compile(nn.BCELoss(), optim.Adam, lr)\n",
    "    neural_network.fit(x_train.values, y_train.values, epochs, x_val=x_val.values, y_val=y_val.values)\n",
    "\n",
    "    x_test = test.drop(columns=[\"productid\", \"similargrouplevel\"])\n",
    "    x_test.loc[:, cols] = scaler_x.fit_transform(x_test[cols])\n",
    "    y_pred = neural_network.predict(x_test.values).reshape(-1)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    predictions = pd.DataFrame({\"productid\": test[\"productid\"], \"predictedyQuantity\": y_pred})\n",
    "    print(predictions.head())\n",
    "\n",
    "    pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=f\"{base_dir}/experiments/exp_neural_network_new/predictions/ModelForEachBrickProductLevel/{of}/predictions\", filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(\"menShirts\", epochs = 500, lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(\"womenKurtas\", epochs = 500, lr = 0.0005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with 0-4 months sales, wishlist, availableQuantity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/data/Archive/bhavesh/inventoryPrediction/temp/local\"\n",
    "train_path = {\n",
    "    \"menShirts\": f\"{base_dir}/menShirts/data/train\",\n",
    "    \"womenKurtas\": f\"{base_dir}/womenKurtas/data/train\",\n",
    "}\n",
    "test_path = {\n",
    "    \"menShirts\": f\"{base_dir}/menShirts/data/test\",\n",
    "    \"womenKurtas\": f\"{base_dir}/womenKurtas/data/test\",\n",
    "}\n",
    "\n",
    "def fetch_data2(of):\n",
    "    cols_to_read = [f\"{i}_sales\" for i in range(4)] + [f\"{i}_wishlist\" for i in range(4)] + [f\"{i}_availableQuantity\" for i in range(4)]\n",
    "    cols_to_read = [\"productid\", \"similargrouplevel\"] + cols_to_read\n",
    "    train = pq.ParquetDataset(train_path[of], fs).read(columns=cols_to_read + [\"yQuantity\"]).to_pandas()\n",
    "    test = pq.ParquetDataset(test_path[of], fs).read(columns=cols_to_read).to_pandas()\n",
    "\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    train.info()\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment2(of, epochs = 200, lr = 0.001):\n",
    "    train, test = fetch_data2(of)\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    cols = train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]).columns\n",
    "    print(cols)\n",
    "    train.loc[:, cols] = scaler_x.fit_transform(train[cols])\n",
    "    train[\"yQuantity\"] = scaler_y.fit_transform(train[[\"yQuantity\"]])\n",
    "    print(scaler_y.data_min_, scaler_y.data_max_)\n",
    "\n",
    "    train, val = train_val_split(train)\n",
    "    print(train.shape, val.shape)\n",
    "\n",
    "    x_train, y_train = train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), train[\"yQuantity\"]\n",
    "    x_val, y_val = val.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), val[\"yQuantity\"]\n",
    "\n",
    "    neural_network = Model(NeuralNetwork, {\"input_size\": 12, \"output_size\": 1, \"hidden_size1\": 24, \"hidden_size2\": 6})\n",
    "    print(neural_network.device)\n",
    "    neural_network.compile(nn.BCELoss(), optim.Adam, lr)\n",
    "    neural_network.fit(x_train.values, y_train.values, epochs, x_val=x_val.values, y_val=y_val.values)\n",
    "\n",
    "    x_test = test.drop(columns=[\"productid\", \"similargrouplevel\"])\n",
    "    x_test.loc[:, cols] = scaler_x.fit_transform(x_test[cols])\n",
    "    y_pred = neural_network.predict(x_test.values).reshape(-1)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    predictions = pd.DataFrame({\"productid\": test[\"productid\"], \"predictedyQuantity\": y_pred})\n",
    "    print(predictions.head())\n",
    "\n",
    "    pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=f\"{base_dir}/experiments/exp_neural_network_new/predictions/ModelForEachBrickProductLevel/{of}/predictions\", filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2(\"menShirts\", epochs = 200, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2(\"womenKurtas\", epochs = 200, lr = 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_env",
   "language": "python",
   "name": "api_env"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
