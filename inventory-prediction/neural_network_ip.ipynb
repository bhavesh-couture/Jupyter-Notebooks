{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jioapp/anaconda3/envs/api_env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/tmp/ipykernel_38732/75781886.py:11: FutureWarning: pyarrow.hdfs.connect is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  fs = pa.hdfs.connect()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "fs = pa.hdfs.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_percentage_error as mape, r2_score\n",
    "def metrics(actual, predicted):    \n",
    "    y_true = actual\n",
    "    y_pred = predicted\n",
    "    print(f\"rmse: {mse(y_true, y_pred)**(0.5)}\")\n",
    "    print(f\"mape: {mape(y_true, y_pred)}\")\n",
    "    print(f\"r2_score: {r2_score(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate = 0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.fc1(x)\n",
    "      x = self.relu1(x)\n",
    "      x = self.dropout1(x)\n",
    "      x = self.fc2(x)\n",
    "      x = self.relu2(x)\n",
    "      x = self.dropout2(x)\n",
    "      x = self.fc3(x)\n",
    "      x = self.sigmoid(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "  class _Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "      self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "      self.x = torch.tensor(x).float().to(self.device)\n",
    "      self.y = torch.tensor(y).float().reshape(-1, 1).to(self.device)\n",
    "    \n",
    "    def __len__(self, ):\n",
    "      return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "      x = self.x[idx]\n",
    "      y = self.y[idx]\n",
    "      return x, y\n",
    "\n",
    "  def __init__(self, neural_network, neural_network_layers_info):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self._neural_network = neural_network\n",
    "    self._neural_network_layers_info = neural_network_layers_info\n",
    "    self.model = self._neural_network(**self._neural_network_layers_info).to(self.device)\n",
    "    self._training_done = False\n",
    "    self._compile_done = False\n",
    "\n",
    "  def __clear(self, ):\n",
    "    self.model = self._neural_network(**self._neural_network_layers_info)\n",
    "    self._training_done = False\n",
    "    self._compile_done = False\n",
    "    self._validation_loss_history = []\n",
    "\n",
    "  def compile(self, criterion, optimizer, learning_rate):\n",
    "    if self._training_done:\n",
    "      raise Exception(\"Cannot compile an already trained model\")\n",
    "    \n",
    "    if self._compile_done:\n",
    "      print(\"Warn: Previously trained Model will be replaced.\")\n",
    "      self.__clear()\n",
    "\n",
    "    self.criterion = criterion\n",
    "    self._optimizer = optimizer\n",
    "    self._lr = learning_rate\n",
    "    self.optimizer = self._optimizer(self.model.parameters(), lr = self._lr)\n",
    "    self._compile_done = True\n",
    "\n",
    "  def _get_dataset_loader(self, x, y, batch_size):\n",
    "    dataset = self._Dataset(x, y)\n",
    "    dataset_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "    return dataset_loader\n",
    "  \n",
    "  def fit(self, x_train, y_train, num_epochs, batch_size = 32, x_val = None, y_val = None, early_stopping = None):\n",
    "    if not self._compile_done:\n",
    "      raise Exception(\"compile before training\")\n",
    "    \n",
    "    if self._training_done:\n",
    "      print(\"Warn: Previously trained Model will be replaced.\")\n",
    "      self.__clear()\n",
    "      self.compile(self.criterion, self._optimizer, self._lr)\n",
    "    \n",
    "    do_validation = (x_val is not None) and (y_val is not None)\n",
    "\n",
    "    train_data_loader = self._get_dataset_loader(x_train, y_train, batch_size)\n",
    "    if do_validation:\n",
    "      validation_data = self._Dataset(x_val, y_val)\n",
    "      if not early_stopping:\n",
    "        early_stopping = np.inf\n",
    "      self._fit_with_validation(train_data_loader, validation_data, epochs = num_epochs, early_stopping = early_stopping)\n",
    "    else:\n",
    "      self._fit_without_validation(train_data_loader, epochs = num_epochs)\n",
    "\n",
    "    self._training_done = True\n",
    "\n",
    "  def predict(self, x_test):\n",
    "    self.model.eval()\n",
    "    with torch.no_grad():\n",
    "      x_test = torch.tensor(x_test).float().to(self.device)\n",
    "      y_test = self.model(x_test)\n",
    "    \n",
    "      return y_test.cpu().detach().numpy()\n",
    "    \n",
    "  def _fit_with_validation(self, train_data_loader, validation_data, epochs, early_stopping):\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    val_loss_history = []\n",
    "    best_model = None\n",
    "    for epoch in range(epochs):\n",
    "      if epoch - best_epoch > early_stopping:\n",
    "        print(\"Early stopping at epoch:\", epoch+1)\n",
    "        break\n",
    "      \n",
    "      self.model.train()\n",
    "      train_loss = []\n",
    "\n",
    "      for x, y in train_data_loader:\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        train_loss.append(float(loss.item()))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "      self.model.eval()\n",
    "      with torch.no_grad():\n",
    "        y_pred = self.model(validation_data.x)\n",
    "        val_loss = float(self.criterion(y_pred, validation_data.y))\n",
    "        val_loss_history.append(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "          print(f\"Best validation loss at epoch={epoch+1}, saving model\")\n",
    "          best_val_loss = val_loss\n",
    "          best_epoch = epoch + 1\n",
    "          best_model = self.model.state_dict().copy()\n",
    "      \n",
    "        print(f\"Epoch #{epoch + 1} - training loss: {sum(train_loss)/len(train_loss)} \\t validation loss: {val_loss}\")\n",
    "    self.model.load_state_dict(best_model)\n",
    "    self._validation_loss_history = val_loss_history\n",
    "\n",
    "  def _fit_without_validation(self, train_data_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "      self.model.train()\n",
    "      train_loss = []\n",
    "\n",
    "      for x, y in train_data_loader:\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        train_loss.append(float(loss.item()))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "      print(f\"Epoch #{epoch + 1} - training loss: {sum(train_loss)/len(train_loss)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train, val_size = 0.2):\n",
    "    train = train.sort_values(by=[\"yQuantity\"]).reset_index(drop=True)\n",
    "    val_indexes = np.array([])\n",
    "    for i in range(0, train.shape[0], 50):\n",
    "        left = i\n",
    "        right = min(i+50, train.shape[0])\n",
    "        cnt = (right - left) * val_size\n",
    "\n",
    "        # get cnt random numbers between [left, right)\n",
    "        idx = np.random.randint(left, right, int(cnt))\n",
    "        val_indexes = np.concatenate((val_indexes, idx))\n",
    "    \n",
    "    validation_data = train.iloc[val_indexes.astype(int)]\n",
    "    train_data = train[~train.index.isin(val_indexes.astype(int))]\n",
    "    return train_data, validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with baseline model's features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/data/Archive/bhavesh/InventoryPrediction\"\n",
    "run_date = \"2023-04-30\"\n",
    "run_dir = f\"{base_dir}/{run_date}\"\n",
    "train_path = {\n",
    "    \"menShirts\": f\"{run_dir}/TransformedDataset/baseline/menShirts/data/train\",\n",
    "    \"womenKurtas\": f\"{run_dir}/TransformedDataset/baseline/womenKurtas/data/train\",\n",
    "}\n",
    "test_path = {\n",
    "    \"menShirts\": f\"{run_dir}/TransformedDataset/baseline/menShirts/data/test\",\n",
    "    \"womenKurtas\": f\"{run_dir}/TransformedDataset/baseline/womenKurtas/data/test\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(of):\n",
    "    train = pq.ParquetDataset(train_path[of], fs).read().to_pandas()\n",
    "    test = pq.ParquetDataset(test_path[of], fs).read().to_pandas()\n",
    "\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    train.info()\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(of, epochs = 200, lr = 0.001):\n",
    "    train, test = fetch_data(of)\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    cols = list(filter(lambda x: not (x.endswith(\"0\") or x.endswith(\"1\")), train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]).columns))\n",
    "    print(cols)\n",
    "    train.loc[:, cols] = scaler_x.fit_transform(train[cols])\n",
    "    train[\"yQuantity\"] = scaler_y.fit_transform(train[[\"yQuantity\"]])\n",
    "    print(scaler_y.data_min_, scaler_y.data_max_)\n",
    "\n",
    "    train, val = train_val_split(train)\n",
    "    print(train.shape, val.shape)\n",
    "\n",
    "    x_train, y_train = train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), train[\"yQuantity\"]\n",
    "    x_val, y_val = val.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), val[\"yQuantity\"]\n",
    "\n",
    "    neural_network = Model(NeuralNetwork, {\"input_size\": 33, \"output_size\": 1, \"hidden_size1\": 66, \"hidden_size2\": 33})\n",
    "    print(neural_network.device)\n",
    "    neural_network.compile(nn.BCELoss(), optim.Adam, lr)\n",
    "    neural_network.fit(x_train.values, y_train.values, epochs, x_val=x_val.values, y_val=y_val.values)\n",
    "\n",
    "    x_test = test.drop(columns=[\"productid\", \"similargrouplevel\"])\n",
    "    x_test.loc[:, cols] = scaler_x.fit_transform(x_test[cols])\n",
    "    y_pred = neural_network.predict(x_test.values).reshape(-1)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    predictions = pd.DataFrame({\"productid\": test[\"productid\"], \"predictedyQuantity\": y_pred})\n",
    "    print(predictions.head())\n",
    "\n",
    "    pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=f\"{run_dir}/experiments/exp_neural_network/predictions/ModelForEachBrickProductLevel/{of}/predictions\", filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37698, 36)\n",
      "(97077, 35)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37698 entries, 0 to 37697\n",
      "Data columns (total 36 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   similargrouplevel       37698 non-null  object \n",
      " 1   productid               37698 non-null  object \n",
      " 2   0_monthSales            37698 non-null  float64\n",
      " 3   1_monthSales            37698 non-null  float64\n",
      " 4   2_monthSales            37698 non-null  float64\n",
      " 5   3_monthSales            37698 non-null  float64\n",
      " 6   4_monthSales            37698 non-null  float64\n",
      " 7   5_monthSales            37698 non-null  float64\n",
      " 8   6_monthSales            37698 non-null  float64\n",
      " 9   7_monthSales            37698 non-null  float64\n",
      " 10  8_monthSales            37698 non-null  float64\n",
      " 11  9_monthSales            37698 non-null  float64\n",
      " 12  10_monthSales           37698 non-null  float64\n",
      " 13  11_monthSales           37698 non-null  float64\n",
      " 14  avgSales                37698 non-null  float64\n",
      " 15  stddevSales             37698 non-null  float64\n",
      " 16  totalReturn             37698 non-null  float64\n",
      " 17  totalUsers              37698 non-null  float64\n",
      " 18  totalPLPViews           37698 non-null  float64\n",
      " 19  totalPLPClicks          37698 non-null  float64\n",
      " 20  totalPDPCount           37698 non-null  float64\n",
      " 21  totalATC                37698 non-null  float64\n",
      " 22  totalWishList           37698 non-null  float64\n",
      " 23  totalAvailableQuantity  37698 non-null  float64\n",
      " 24  yQuantity               37698 non-null  float64\n",
      " 25  pricebucket             37698 non-null  int32  \n",
      " 26  normalized_color_0      37698 non-null  float64\n",
      " 27  normalized_color_1      37698 non-null  float64\n",
      " 28  normalized_pattern_0    37698 non-null  float64\n",
      " 29  normalized_pattern_1    37698 non-null  float64\n",
      " 30  normalized_brand_0      37698 non-null  float64\n",
      " 31  normalized_brand_1      37698 non-null  float64\n",
      " 32  normalized_style_0      37698 non-null  float64\n",
      " 33  normalized_style_1      37698 non-null  float64\n",
      " 34  normalized_sleeve_0     37698 non-null  float64\n",
      " 35  normalized_sleeve_1     37698 non-null  float64\n",
      "dtypes: float64(33), int32(1), object(2)\n",
      "memory usage: 10.2+ MB\n",
      "['0_monthSales', '1_monthSales', '2_monthSales', '3_monthSales', '4_monthSales', '5_monthSales', '6_monthSales', '7_monthSales', '8_monthSales', '9_monthSales', '10_monthSales', '11_monthSales', 'avgSales', 'stddevSales', 'totalReturn', 'totalUsers', 'totalPLPViews', 'totalPLPClicks', 'totalPDPCount', 'totalATC', 'totalWishList', 'totalAvailableQuantity', 'pricebucket']\n",
      "[1.] [7795.]\n",
      "(30805, 36) (7539, 36)\n",
      "cuda:0\n",
      "Best validation loss at epoch=1, saving model\n",
      "Epoch #1 - training loss: 0.07677584366530195 \t validation loss: 0.021853599697351456\n",
      "Best validation loss at epoch=2, saving model\n",
      "Epoch #2 - training loss: 0.024446211882582294 \t validation loss: 0.01946597546339035\n",
      "Best validation loss at epoch=3, saving model\n",
      "Epoch #3 - training loss: 0.02250942856019165 \t validation loss: 0.018716929480433464\n",
      "Best validation loss at epoch=4, saving model\n",
      "Epoch #4 - training loss: 0.021719462854696704 \t validation loss: 0.018434351310133934\n",
      "Best validation loss at epoch=5, saving model\n",
      "Epoch #5 - training loss: 0.021385676126342398 \t validation loss: 0.01806054636836052\n",
      "Epoch #6 - training loss: 0.020898391055562796 \t validation loss: 0.018138334155082703\n",
      "Best validation loss at epoch=7, saving model\n",
      "Epoch #7 - training loss: 0.02041250007157359 \t validation loss: 0.01771908812224865\n",
      "Epoch #8 - training loss: 0.020169013489634107 \t validation loss: 0.017738599330186844\n",
      "Best validation loss at epoch=9, saving model\n",
      "Epoch #9 - training loss: 0.019780182833889822 \t validation loss: 0.0175476111471653\n",
      "Best validation loss at epoch=10, saving model\n",
      "Epoch #10 - training loss: 0.019645816834329814 \t validation loss: 0.0173538438975811\n",
      "Best validation loss at epoch=11, saving model\n",
      "Epoch #11 - training loss: 0.019401515137961834 \t validation loss: 0.01714433543384075\n",
      "Best validation loss at epoch=12, saving model\n",
      "Epoch #12 - training loss: 0.01912793581492099 \t validation loss: 0.017002861946821213\n",
      "Epoch #13 - training loss: 0.018919166766606627 \t validation loss: 0.017132364213466644\n",
      "Epoch #14 - training loss: 0.018672760038466237 \t validation loss: 0.01702231913805008\n",
      "Best validation loss at epoch=15, saving model\n",
      "Epoch #15 - training loss: 0.018497587679720748 \t validation loss: 0.016871778294444084\n",
      "Epoch #16 - training loss: 0.01845610734939831 \t validation loss: 0.016894957050681114\n",
      "Best validation loss at epoch=17, saving model\n",
      "Epoch #17 - training loss: 0.01827175086320585 \t validation loss: 0.016814256086945534\n",
      "Best validation loss at epoch=18, saving model\n",
      "Epoch #18 - training loss: 0.018251650752025325 \t validation loss: 0.016772808507084846\n",
      "Epoch #19 - training loss: 0.018173928934066188 \t validation loss: 0.01683826744556427\n",
      "Epoch #20 - training loss: 0.018144452444368707 \t validation loss: 0.01708243228495121\n",
      "Best validation loss at epoch=21, saving model\n",
      "Epoch #21 - training loss: 0.017979748978370534 \t validation loss: 0.016760626807808876\n",
      "Best validation loss at epoch=22, saving model\n",
      "Epoch #22 - training loss: 0.01796449873530598 \t validation loss: 0.01673433929681778\n",
      "Best validation loss at epoch=23, saving model\n",
      "Epoch #23 - training loss: 0.01787053767190917 \t validation loss: 0.01672091707587242\n",
      "Best validation loss at epoch=24, saving model\n",
      "Epoch #24 - training loss: 0.017814075011806464 \t validation loss: 0.016720671206712723\n",
      "Best validation loss at epoch=25, saving model\n",
      "Epoch #25 - training loss: 0.01775529744876103 \t validation loss: 0.01667873188853264\n",
      "Epoch #26 - training loss: 0.01772040368750955 \t validation loss: 0.016722291707992554\n",
      "Epoch #27 - training loss: 0.017701739635078425 \t validation loss: 0.01669345423579216\n",
      "Epoch #28 - training loss: 0.017601735480158764 \t validation loss: 0.016719788312911987\n",
      "Epoch #29 - training loss: 0.017620565355929648 \t validation loss: 0.016688937321305275\n",
      "Epoch #30 - training loss: 0.017577349428056492 \t validation loss: 0.0167537871748209\n",
      "Best validation loss at epoch=31, saving model\n",
      "Epoch #31 - training loss: 0.01751421459925336 \t validation loss: 0.01667359657585621\n",
      "Epoch #32 - training loss: 0.01762836906340616 \t validation loss: 0.01669378951191902\n",
      "Epoch #33 - training loss: 0.017509751659120953 \t validation loss: 0.016678474843502045\n",
      "Best validation loss at epoch=34, saving model\n",
      "Epoch #34 - training loss: 0.017548761239300506 \t validation loss: 0.016655558720231056\n",
      "Best validation loss at epoch=35, saving model\n",
      "Epoch #35 - training loss: 0.01747199292817461 \t validation loss: 0.016593636944890022\n",
      "Epoch #36 - training loss: 0.017524474341346807 \t validation loss: 0.01661797985434532\n",
      "Epoch #37 - training loss: 0.01743891761733099 \t validation loss: 0.01667030155658722\n",
      "Epoch #38 - training loss: 0.0174506478901786 \t validation loss: 0.016660528257489204\n",
      "Epoch #39 - training loss: 0.017369859741290107 \t validation loss: 0.01661955937743187\n",
      "Epoch #40 - training loss: 0.01747382974848461 \t validation loss: 0.01674213446676731\n",
      "Epoch #41 - training loss: 0.017355927904891606 \t validation loss: 0.01671798713505268\n",
      "Epoch #42 - training loss: 0.017421583948548487 \t validation loss: 0.01665746234357357\n",
      "Epoch #43 - training loss: 0.017377536727421435 \t validation loss: 0.0166330523788929\n",
      "Epoch #44 - training loss: 0.017469970974829163 \t validation loss: 0.016603443771600723\n",
      "Epoch #45 - training loss: 0.01735931325978803 \t validation loss: 0.016596030443906784\n",
      "Epoch #46 - training loss: 0.017332345393453404 \t validation loss: 0.016622822731733322\n",
      "Epoch #47 - training loss: 0.01733873564950355 \t validation loss: 0.016611259430646896\n",
      "Epoch #48 - training loss: 0.017401054734122123 \t validation loss: 0.01668950915336609\n",
      "Epoch #49 - training loss: 0.017392117358025806 \t validation loss: 0.016641877591609955\n",
      "Epoch #50 - training loss: 0.01726046410338903 \t validation loss: 0.016610274091362953\n",
      "Epoch #51 - training loss: 0.017384553103070712 \t validation loss: 0.016620179638266563\n",
      "Epoch #52 - training loss: 0.017315494691422538 \t validation loss: 0.016630802303552628\n",
      "Epoch #53 - training loss: 0.017190707184804175 \t validation loss: 0.016649629920721054\n",
      "Epoch #54 - training loss: 0.017234171875036106 \t validation loss: 0.016610896214842796\n",
      "Best validation loss at epoch=55, saving model\n",
      "Epoch #55 - training loss: 0.017304831605332974 \t validation loss: 0.016590850427746773\n",
      "Epoch #56 - training loss: 0.01738304989518919 \t validation loss: 0.016629310324788094\n",
      "Epoch #57 - training loss: 0.01729791267947323 \t validation loss: 0.016626499593257904\n",
      "Best validation loss at epoch=58, saving model\n",
      "Epoch #58 - training loss: 0.017211492047035242 \t validation loss: 0.01658342406153679\n",
      "Best validation loss at epoch=59, saving model\n",
      "Epoch #59 - training loss: 0.01726729208758399 \t validation loss: 0.01658335141837597\n",
      "Epoch #60 - training loss: 0.017283955165630364 \t validation loss: 0.01664183847606182\n",
      "Epoch #61 - training loss: 0.01726847608610523 \t validation loss: 0.016627611592411995\n",
      "Epoch #62 - training loss: 0.01732418578739613 \t validation loss: 0.016660751774907112\n",
      "Epoch #63 - training loss: 0.01724109466075355 \t validation loss: 0.016609709709882736\n",
      "Epoch #64 - training loss: 0.01740373749011961 \t validation loss: 0.016655562445521355\n",
      "Epoch #65 - training loss: 0.01728911845162588 \t validation loss: 0.016629964113235474\n",
      "Epoch #66 - training loss: 0.017244145647369 \t validation loss: 0.016596917062997818\n",
      "Epoch #67 - training loss: 0.01726142118714959 \t validation loss: 0.016695307567715645\n",
      "Epoch #68 - training loss: 0.01723589698141228 \t validation loss: 0.016638411208987236\n",
      "Epoch #69 - training loss: 0.017240814510306922 \t validation loss: 0.0166900884360075\n",
      "Epoch #70 - training loss: 0.017201938220346248 \t validation loss: 0.016602041199803352\n",
      "Epoch #71 - training loss: 0.017295157017714873 \t validation loss: 0.01665310189127922\n",
      "Epoch #72 - training loss: 0.01729387878183771 \t validation loss: 0.016612699255347252\n",
      "Best validation loss at epoch=73, saving model\n",
      "Epoch #73 - training loss: 0.0171642996542148 \t validation loss: 0.016564419493079185\n",
      "Epoch #74 - training loss: 0.01717323328420644 \t validation loss: 0.016644073650240898\n",
      "Epoch #75 - training loss: 0.017360805018549388 \t validation loss: 0.016634875908493996\n",
      "Epoch #76 - training loss: 0.017348232208083146 \t validation loss: 0.016672957688570023\n",
      "Epoch #77 - training loss: 0.01725037932481129 \t validation loss: 0.01658947765827179\n",
      "Epoch #78 - training loss: 0.017215621056714776 \t validation loss: 0.01657499559223652\n",
      "Epoch #79 - training loss: 0.017199612407521065 \t validation loss: 0.016582515090703964\n",
      "Epoch #80 - training loss: 0.017170096520313184 \t validation loss: 0.01665032096207142\n",
      "Best validation loss at epoch=81, saving model\n",
      "Epoch #81 - training loss: 0.01716523129771814 \t validation loss: 0.016557546332478523\n",
      "Best validation loss at epoch=82, saving model\n",
      "Epoch #82 - training loss: 0.017312791630753516 \t validation loss: 0.01655041240155697\n",
      "Epoch #83 - training loss: 0.017246810995040388 \t validation loss: 0.016589617356657982\n",
      "Epoch #84 - training loss: 0.0172126681363334 \t validation loss: 0.016607845202088356\n",
      "Epoch #85 - training loss: 0.017268044531602886 \t validation loss: 0.01660032384097576\n",
      "Epoch #86 - training loss: 0.01719297976194341 \t validation loss: 0.0166460108011961\n",
      "Epoch #87 - training loss: 0.017237983105497012 \t validation loss: 0.01660747453570366\n",
      "Best validation loss at epoch=88, saving model\n",
      "Epoch #88 - training loss: 0.017223118930733656 \t validation loss: 0.01654483750462532\n",
      "Epoch #89 - training loss: 0.017190216295759963 \t validation loss: 0.016583502292633057\n",
      "Epoch #90 - training loss: 0.017161948914473136 \t validation loss: 0.01657593995332718\n",
      "Epoch #91 - training loss: 0.0171183345828732 \t validation loss: 0.01658584736287594\n",
      "Epoch #92 - training loss: 0.017194936744890053 \t validation loss: 0.016592208296060562\n",
      "Epoch #93 - training loss: 0.017199109913490853 \t validation loss: 0.016584789380431175\n",
      "Epoch #94 - training loss: 0.01727919186719087 \t validation loss: 0.01663452759385109\n",
      "Epoch #95 - training loss: 0.01727937978499699 \t validation loss: 0.016634877771139145\n",
      "Epoch #96 - training loss: 0.017199973278802873 \t validation loss: 0.01657264307141304\n",
      "Epoch #97 - training loss: 0.017201393862785443 \t validation loss: 0.016562368720769882\n",
      "Epoch #98 - training loss: 0.017279374522104735 \t validation loss: 0.01655377820134163\n",
      "Epoch #99 - training loss: 0.01722769318981069 \t validation loss: 0.016644127666950226\n",
      "Epoch #100 - training loss: 0.017166210911836934 \t validation loss: 0.016649002209305763\n",
      "Epoch #101 - training loss: 0.017171294729974654 \t validation loss: 0.01656295545399189\n",
      "Epoch #102 - training loss: 0.017198114705043545 \t validation loss: 0.016574610024690628\n",
      "Best validation loss at epoch=103, saving model\n",
      "Epoch #103 - training loss: 0.01715221895140222 \t validation loss: 0.01654355227947235\n",
      "Epoch #104 - training loss: 0.01725634838950098 \t validation loss: 0.016608070582151413\n",
      "Epoch #105 - training loss: 0.017185846982105575 \t validation loss: 0.016548164188861847\n",
      "Epoch #106 - training loss: 0.017181837028028005 \t validation loss: 0.016550332307815552\n",
      "Epoch #107 - training loss: 0.017187086719121173 \t validation loss: 0.016609909012913704\n",
      "Epoch #108 - training loss: 0.01708125174887819 \t validation loss: 0.01655755378305912\n",
      "Epoch #109 - training loss: 0.017249392080769376 \t validation loss: 0.016554201021790504\n",
      "Epoch #110 - training loss: 0.01712723330085373 \t validation loss: 0.016544850543141365\n",
      "Epoch #111 - training loss: 0.017219130220637112 \t validation loss: 0.016597077250480652\n",
      "Epoch #112 - training loss: 0.017246053074418784 \t validation loss: 0.01661909557878971\n",
      "Epoch #113 - training loss: 0.01712026537600285 \t validation loss: 0.016586709767580032\n",
      "Epoch #114 - training loss: 0.01715325022923708 \t validation loss: 0.016587093472480774\n",
      "Epoch #115 - training loss: 0.017121099238380743 \t validation loss: 0.016584474593400955\n",
      "Epoch #116 - training loss: 0.017159855455497044 \t validation loss: 0.01661592349410057\n",
      "Epoch #117 - training loss: 0.017165737892665636 \t validation loss: 0.016544369980692863\n",
      "Epoch #118 - training loss: 0.017155528428437664 \t validation loss: 0.01660931296646595\n",
      "Epoch #119 - training loss: 0.01721865663270073 \t validation loss: 0.01654849946498871\n",
      "Epoch #120 - training loss: 0.017178577328344765 \t validation loss: 0.01660829596221447\n",
      "Epoch #121 - training loss: 0.017127938412726234 \t validation loss: 0.016576839610934258\n",
      "Epoch #122 - training loss: 0.017139676590924482 \t validation loss: 0.016593733802437782\n",
      "Best validation loss at epoch=123, saving model\n",
      "Epoch #123 - training loss: 0.017124009220144495 \t validation loss: 0.016522252932190895\n",
      "Epoch #124 - training loss: 0.017132431608982958 \t validation loss: 0.0165714081376791\n",
      "Best validation loss at epoch=125, saving model\n",
      "Epoch #125 - training loss: 0.017126354001648338 \t validation loss: 0.01649041660130024\n",
      "Epoch #126 - training loss: 0.017154270078524127 \t validation loss: 0.016615096479654312\n",
      "Epoch #127 - training loss: 0.017161441144965234 \t validation loss: 0.016558775678277016\n",
      "Epoch #128 - training loss: 0.017231754974845463 \t validation loss: 0.016582928597927094\n",
      "Epoch #129 - training loss: 0.017106923366666398 \t validation loss: 0.016532285138964653\n",
      "Epoch #130 - training loss: 0.017060802544935718 \t validation loss: 0.01651040092110634\n",
      "Epoch #131 - training loss: 0.017178889063405738 \t validation loss: 0.016504980623722076\n",
      "Epoch #132 - training loss: 0.017156025971791354 \t validation loss: 0.016537779942154884\n",
      "Epoch #133 - training loss: 0.017090992169723828 \t validation loss: 0.0165268387645483\n",
      "Epoch #134 - training loss: 0.01709637309283235 \t validation loss: 0.016502153128385544\n",
      "Epoch #135 - training loss: 0.017201551317049504 \t validation loss: 0.016537489369511604\n",
      "Epoch #136 - training loss: 0.01721422682882223 \t validation loss: 0.016547774896025658\n",
      "Epoch #137 - training loss: 0.017079476690163794 \t validation loss: 0.016571106389164925\n",
      "Epoch #138 - training loss: 0.017187481399493876 \t validation loss: 0.016568081453442574\n",
      "Epoch #139 - training loss: 0.01711480997129113 \t validation loss: 0.016533439978957176\n",
      "Epoch #140 - training loss: 0.017121427869637208 \t validation loss: 0.01660976931452751\n",
      "Epoch #141 - training loss: 0.017157387998625016 \t validation loss: 0.016513552516698837\n",
      "Epoch #142 - training loss: 0.01722213072898815 \t validation loss: 0.016557563096284866\n",
      "Epoch #143 - training loss: 0.017290950164667442 \t validation loss: 0.016520831733942032\n",
      "Epoch #144 - training loss: 0.017233663612517786 \t validation loss: 0.01649194397032261\n",
      "Epoch #145 - training loss: 0.017124694592480134 \t validation loss: 0.01650826819241047\n",
      "Epoch #146 - training loss: 0.01706605682414792 \t validation loss: 0.016559259966015816\n",
      "Epoch #147 - training loss: 0.017166929672861358 \t validation loss: 0.016543691977858543\n",
      "Epoch #148 - training loss: 0.017190235620461358 \t validation loss: 0.016550758853554726\n",
      "Epoch #149 - training loss: 0.01724009273143915 \t validation loss: 0.016518354415893555\n",
      "Epoch #150 - training loss: 0.01706753356146032 \t validation loss: 0.016496047377586365\n",
      "Epoch #151 - training loss: 0.017132215668000047 \t validation loss: 0.016526011750102043\n",
      "Epoch #152 - training loss: 0.01715627375296985 \t validation loss: 0.016555527225136757\n",
      "Epoch #153 - training loss: 0.01720148705433969 \t validation loss: 0.01653202436864376\n",
      "Best validation loss at epoch=154, saving model\n",
      "Epoch #154 - training loss: 0.017086880976671118 \t validation loss: 0.01647423394024372\n",
      "Epoch #155 - training loss: 0.017131767227709403 \t validation loss: 0.016646353527903557\n",
      "Epoch #156 - training loss: 0.017158816054002567 \t validation loss: 0.01651470735669136\n",
      "Epoch #157 - training loss: 0.017176860355793376 \t validation loss: 0.016616450622677803\n",
      "Epoch #158 - training loss: 0.017051540546529693 \t validation loss: 0.016534684225916862\n",
      "Epoch #159 - training loss: 0.017145553983509802 \t validation loss: 0.016566665843129158\n",
      "Epoch #160 - training loss: 0.01709624062792791 \t validation loss: 0.01668097823858261\n",
      "Epoch #161 - training loss: 0.01716592562580462 \t validation loss: 0.016583159565925598\n",
      "Epoch #162 - training loss: 0.016988302618284514 \t validation loss: 0.016573859378695488\n",
      "Epoch #163 - training loss: 0.017157318893425057 \t validation loss: 0.016536051407456398\n",
      "Epoch #164 - training loss: 0.017185584344541007 \t validation loss: 0.016497068107128143\n",
      "Epoch #165 - training loss: 0.01709777913675905 \t validation loss: 0.016542870551347733\n",
      "Epoch #166 - training loss: 0.017053647415292316 \t validation loss: 0.01655574142932892\n",
      "Epoch #167 - training loss: 0.01711485784001292 \t validation loss: 0.016583027318120003\n",
      "Epoch #168 - training loss: 0.01707300850652011 \t validation loss: 0.01657656393945217\n",
      "Epoch #169 - training loss: 0.017134212887567116 \t validation loss: 0.0165657140314579\n",
      "Epoch #170 - training loss: 0.017233791012992685 \t validation loss: 0.016629936173558235\n",
      "Epoch #171 - training loss: 0.01709393315547748 \t validation loss: 0.016609029844403267\n",
      "Epoch #172 - training loss: 0.017065024355536796 \t validation loss: 0.016603780910372734\n",
      "Epoch #173 - training loss: 0.01700319975383169 \t validation loss: 0.016537781804800034\n",
      "Epoch #174 - training loss: 0.01705843648726117 \t validation loss: 0.01656724140048027\n",
      "Epoch #175 - training loss: 0.01712865892820943 \t validation loss: 0.016585398465394974\n",
      "Epoch #176 - training loss: 0.01706619065771898 \t validation loss: 0.016561217606067657\n",
      "Epoch #177 - training loss: 0.017150172824330488 \t validation loss: 0.016588300466537476\n",
      "Epoch #178 - training loss: 0.01707930716476839 \t validation loss: 0.016601545736193657\n",
      "Epoch #179 - training loss: 0.017186590486589486 \t validation loss: 0.016516489908099174\n",
      "Epoch #180 - training loss: 0.017066518484959436 \t validation loss: 0.016520749777555466\n",
      "Best validation loss at epoch=181, saving model\n",
      "Epoch #181 - training loss: 0.017063606240962807 \t validation loss: 0.01647232659161091\n",
      "Epoch #182 - training loss: 0.016987351776176208 \t validation loss: 0.016491632908582687\n",
      "Epoch #183 - training loss: 0.017070961876998882 \t validation loss: 0.016547415405511856\n",
      "Epoch #184 - training loss: 0.017046014379938433 \t validation loss: 0.016504008322954178\n",
      "Epoch #185 - training loss: 0.017170128954686767 \t validation loss: 0.016522254794836044\n",
      "Epoch #186 - training loss: 0.017131135742598273 \t validation loss: 0.016537612304091454\n",
      "Epoch #187 - training loss: 0.01712208038792324 \t validation loss: 0.01657986082136631\n",
      "Epoch #188 - training loss: 0.0170552457574844 \t validation loss: 0.016573656350374222\n",
      "Epoch #189 - training loss: 0.0170878433274384 \t validation loss: 0.016516193747520447\n",
      "Epoch #190 - training loss: 0.017187634766636234 \t validation loss: 0.01654057949781418\n",
      "Best validation loss at epoch=191, saving model\n",
      "Epoch #191 - training loss: 0.01706245925331686 \t validation loss: 0.01645149104297161\n",
      "Epoch #192 - training loss: 0.01710907795433567 \t validation loss: 0.016528110951185226\n",
      "Epoch #193 - training loss: 0.017086579273174092 \t validation loss: 0.01660194993019104\n",
      "Epoch #194 - training loss: 0.01713986133356925 \t validation loss: 0.016532834619283676\n",
      "Epoch #195 - training loss: 0.017039969433757348 \t validation loss: 0.016519183292984962\n",
      "Epoch #196 - training loss: 0.01713288418238057 \t validation loss: 0.016512267291545868\n",
      "Epoch #197 - training loss: 0.01714098484767504 \t validation loss: 0.01649940200150013\n",
      "Epoch #198 - training loss: 0.017159425755168878 \t validation loss: 0.016460925340652466\n",
      "Epoch #199 - training loss: 0.017062801996966255 \t validation loss: 0.016537223011255264\n",
      "Epoch #200 - training loss: 0.017053692851151707 \t validation loss: 0.016533594578504562\n",
      "Epoch #201 - training loss: 0.017057567968695442 \t validation loss: 0.016543155536055565\n",
      "Epoch #202 - training loss: 0.017008885132292144 \t validation loss: 0.016537750139832497\n",
      "Epoch #203 - training loss: 0.01705622635713997 \t validation loss: 0.01657135970890522\n",
      "Epoch #204 - training loss: 0.017097562686791894 \t validation loss: 0.016541937366127968\n",
      "Epoch #205 - training loss: 0.017057407440195585 \t validation loss: 0.016504885628819466\n",
      "Epoch #206 - training loss: 0.017063365406221217 \t validation loss: 0.01646053045988083\n",
      "Epoch #207 - training loss: 0.01715589901616336 \t validation loss: 0.016520777717232704\n",
      "Epoch #208 - training loss: 0.01706255253684163 \t validation loss: 0.01647055335342884\n",
      "Epoch #209 - training loss: 0.017115669166707974 \t validation loss: 0.016521234065294266\n",
      "Epoch #210 - training loss: 0.017080975985909637 \t validation loss: 0.016460098326206207\n",
      "Epoch #211 - training loss: 0.016944026559728764 \t validation loss: 0.016478506848216057\n",
      "Epoch #212 - training loss: 0.017088973784834736 \t validation loss: 0.016496160998940468\n",
      "Epoch #213 - training loss: 0.01701504528220441 \t validation loss: 0.016455357894301414\n",
      "Epoch #214 - training loss: 0.017134617802807034 \t validation loss: 0.01647111028432846\n",
      "Epoch #215 - training loss: 0.017076586430026856 \t validation loss: 0.016473153606057167\n",
      "Epoch #216 - training loss: 0.017149683007468972 \t validation loss: 0.016516484320163727\n",
      "Epoch #217 - training loss: 0.017136795260157368 \t validation loss: 0.016493940725922585\n",
      "Epoch #218 - training loss: 0.016998075692296773 \t validation loss: 0.0164622962474823\n",
      "Epoch #219 - training loss: 0.017061109397814002 \t validation loss: 0.016574179753661156\n",
      "Epoch #220 - training loss: 0.017092804803480074 \t validation loss: 0.01645522378385067\n",
      "Epoch #221 - training loss: 0.0169906799002355 \t validation loss: 0.016480090096592903\n",
      "Epoch #222 - training loss: 0.01703346381012327 \t validation loss: 0.016486238688230515\n",
      "Best validation loss at epoch=223, saving model\n",
      "Epoch #223 - training loss: 0.017065961898949284 \t validation loss: 0.016420187428593636\n",
      "Epoch #224 - training loss: 0.017025588025104666 \t validation loss: 0.016424380242824554\n",
      "Epoch #225 - training loss: 0.017061813054131944 \t validation loss: 0.01649400219321251\n",
      "Epoch #226 - training loss: 0.016994756153685946 \t validation loss: 0.016436001285910606\n",
      "Epoch #227 - training loss: 0.01709533809019125 \t validation loss: 0.016443202272057533\n",
      "Epoch #228 - training loss: 0.016940285661064182 \t validation loss: 0.01646917685866356\n",
      "Epoch #229 - training loss: 0.017023252558520763 \t validation loss: 0.016542945057153702\n",
      "Epoch #230 - training loss: 0.01711733025942083 \t validation loss: 0.016491614282131195\n",
      "Epoch #231 - training loss: 0.01711465352652459 \t validation loss: 0.016483420506119728\n",
      "Epoch #232 - training loss: 0.01706400497239509 \t validation loss: 0.016486555337905884\n",
      "Epoch #233 - training loss: 0.017032430880449388 \t validation loss: 0.01651124469935894\n",
      "Epoch #234 - training loss: 0.017157949592672257 \t validation loss: 0.016427839174866676\n",
      "Epoch #235 - training loss: 0.017075644881056 \t validation loss: 0.016443295404314995\n",
      "Epoch #236 - training loss: 0.017039573009817118 \t validation loss: 0.016441892832517624\n",
      "Epoch #237 - training loss: 0.017041401320675352 \t validation loss: 0.016444949433207512\n",
      "Epoch #238 - training loss: 0.017090806172891538 \t validation loss: 0.01651642844080925\n",
      "Epoch #239 - training loss: 0.016999175450416078 \t validation loss: 0.01651286520063877\n",
      "Epoch #240 - training loss: 0.017026321792572335 \t validation loss: 0.016474243253469467\n",
      "Epoch #241 - training loss: 0.01705665671796172 \t validation loss: 0.01651769131422043\n",
      "Epoch #242 - training loss: 0.01708192932622782 \t validation loss: 0.01650366745889187\n",
      "Epoch #243 - training loss: 0.017198705064324495 \t validation loss: 0.01644764468073845\n",
      "Epoch #244 - training loss: 0.017053095993185087 \t validation loss: 0.016441337764263153\n",
      "Epoch #245 - training loss: 0.01702888055221447 \t validation loss: 0.01649390533566475\n",
      "Epoch #246 - training loss: 0.01711589058187331 \t validation loss: 0.0164882093667984\n",
      "Epoch #247 - training loss: 0.017090506498070663 \t validation loss: 0.016442477703094482\n",
      "Epoch #248 - training loss: 0.0169739750728956 \t validation loss: 0.016543719917535782\n",
      "Epoch #249 - training loss: 0.01712652412920865 \t validation loss: 0.016484998166561127\n",
      "Epoch #250 - training loss: 0.017026208409857538 \t validation loss: 0.016494881361722946\n",
      "Epoch #251 - training loss: 0.017085975621490043 \t validation loss: 0.016493424773216248\n",
      "Epoch #252 - training loss: 0.017110283902832722 \t validation loss: 0.016452891752123833\n",
      "Epoch #253 - training loss: 0.017065233811871642 \t validation loss: 0.01642456091940403\n",
      "Best validation loss at epoch=254, saving model\n",
      "Epoch #254 - training loss: 0.017092965152879436 \t validation loss: 0.016419807448983192\n",
      "Epoch #255 - training loss: 0.01707952452040318 \t validation loss: 0.016488660126924515\n",
      "Epoch #256 - training loss: 0.017078254012954413 \t validation loss: 0.016508333384990692\n",
      "Epoch #257 - training loss: 0.017147696667772924 \t validation loss: 0.016462257131934166\n",
      "Epoch #258 - training loss: 0.01711954543993396 \t validation loss: 0.016534777358174324\n",
      "Epoch #259 - training loss: 0.017122086089127648 \t validation loss: 0.01647995039820671\n",
      "Epoch #260 - training loss: 0.016998491447957197 \t validation loss: 0.01644476316869259\n",
      "Epoch #261 - training loss: 0.01706692496369827 \t validation loss: 0.016508163884282112\n",
      "Epoch #262 - training loss: 0.01702265373577331 \t validation loss: 0.016476351767778397\n",
      "Epoch #263 - training loss: 0.017101570299009477 \t validation loss: 0.016505926847457886\n",
      "Epoch #264 - training loss: 0.017071215459581256 \t validation loss: 0.016498249024152756\n",
      "Epoch #265 - training loss: 0.01703815640428582 \t validation loss: 0.01647607609629631\n",
      "Epoch #266 - training loss: 0.017072878651642347 \t validation loss: 0.016506221145391464\n",
      "Epoch #267 - training loss: 0.017058187022780044 \t validation loss: 0.016485074535012245\n",
      "Epoch #268 - training loss: 0.017089360460625445 \t validation loss: 0.016489507630467415\n",
      "Epoch #269 - training loss: 0.016986645947417667 \t validation loss: 0.016499875113368034\n",
      "Epoch #270 - training loss: 0.017109823962354068 \t validation loss: 0.016512522473931313\n",
      "Epoch #271 - training loss: 0.017152978169292274 \t validation loss: 0.016499027609825134\n",
      "Epoch #272 - training loss: 0.017011743897364984 \t validation loss: 0.016464795917272568\n",
      "Epoch #273 - training loss: 0.017116148924656415 \t validation loss: 0.016529392451047897\n",
      "Epoch #274 - training loss: 0.01713819095473775 \t validation loss: 0.0165157001465559\n",
      "Epoch #275 - training loss: 0.017240071989423567 \t validation loss: 0.016541795805096626\n",
      "Epoch #276 - training loss: 0.01702814343459943 \t validation loss: 0.016568627208471298\n",
      "Epoch #277 - training loss: 0.01706805424532406 \t validation loss: 0.016649236902594566\n",
      "Epoch #278 - training loss: 0.017005800659208717 \t validation loss: 0.0165395550429821\n",
      "Epoch #279 - training loss: 0.0170184360338344 \t validation loss: 0.01649969071149826\n",
      "Epoch #280 - training loss: 0.017134634709409718 \t validation loss: 0.016489675268530846\n",
      "Epoch #281 - training loss: 0.017044206668299443 \t validation loss: 0.016447966918349266\n",
      "Epoch #282 - training loss: 0.017093079841817706 \t validation loss: 0.01646774634718895\n",
      "Epoch #283 - training loss: 0.01708769412486661 \t validation loss: 0.01650053635239601\n",
      "Best validation loss at epoch=284, saving model\n",
      "Epoch #284 - training loss: 0.01710543101082271 \t validation loss: 0.016412172466516495\n",
      "Epoch #285 - training loss: 0.01707323950453005 \t validation loss: 0.016473622992634773\n",
      "Epoch #286 - training loss: 0.01697159791583091 \t validation loss: 0.016509201377630234\n",
      "Epoch #287 - training loss: 0.017018917586660232 \t validation loss: 0.016471728682518005\n",
      "Epoch #288 - training loss: 0.016948273809618955 \t validation loss: 0.016430607065558434\n",
      "Epoch #289 - training loss: 0.01700722489950369 \t validation loss: 0.016462884843349457\n",
      "Epoch #290 - training loss: 0.017096625848648114 \t validation loss: 0.016500871628522873\n",
      "Epoch #291 - training loss: 0.016984410343397403 \t validation loss: 0.01647314801812172\n",
      "Epoch #292 - training loss: 0.016955692253294335 \t validation loss: 0.016458170488476753\n",
      "Epoch #293 - training loss: 0.017088867026902037 \t validation loss: 0.0164723452180624\n",
      "Epoch #294 - training loss: 0.016995348415788643 \t validation loss: 0.016502633690834045\n",
      "Epoch #295 - training loss: 0.01698645314258831 \t validation loss: 0.01644127443432808\n",
      "Epoch #296 - training loss: 0.017054108053778848 \t validation loss: 0.016507871448993683\n",
      "Epoch #297 - training loss: 0.017076267974582848 \t validation loss: 0.016451910138130188\n",
      "Epoch #298 - training loss: 0.01713684309910975 \t validation loss: 0.016493570059537888\n",
      "Epoch #299 - training loss: 0.017054031590936925 \t validation loss: 0.016456108540296555\n",
      "Epoch #300 - training loss: 0.01698196626494865 \t validation loss: 0.016451213508844376\n",
      "Epoch #301 - training loss: 0.017036951165898023 \t validation loss: 0.016431840136647224\n",
      "Epoch #302 - training loss: 0.017091152056608265 \t validation loss: 0.01644144393503666\n",
      "Epoch #303 - training loss: 0.01706532839097067 \t validation loss: 0.01650492660701275\n",
      "Epoch #304 - training loss: 0.016993755062513455 \t validation loss: 0.016508033499121666\n",
      "Epoch #305 - training loss: 0.017059380225856593 \t validation loss: 0.016489749774336815\n",
      "Epoch #306 - training loss: 0.016999073672203227 \t validation loss: 0.016501467674970627\n",
      "Epoch #307 - training loss: 0.017067843542304548 \t validation loss: 0.01648898608982563\n",
      "Epoch #308 - training loss: 0.01710957318172633 \t validation loss: 0.01645689085125923\n",
      "Epoch #309 - training loss: 0.017133128171953182 \t validation loss: 0.016570424661040306\n",
      "Epoch #310 - training loss: 0.01710791927408969 \t validation loss: 0.016495628282427788\n",
      "Epoch #311 - training loss: 0.017189929326182993 \t validation loss: 0.016497978940606117\n",
      "Epoch #312 - training loss: 0.017169324643406515 \t validation loss: 0.016449864953756332\n",
      "Epoch #313 - training loss: 0.016979176072614836 \t validation loss: 0.01653386652469635\n",
      "Epoch #314 - training loss: 0.01696543956757281 \t validation loss: 0.016440914943814278\n",
      "Epoch #315 - training loss: 0.017117325823537217 \t validation loss: 0.016452429816126823\n",
      "Epoch #316 - training loss: 0.01704220423739202 \t validation loss: 0.0164170078933239\n",
      "Epoch #317 - training loss: 0.017009402782096686 \t validation loss: 0.01644226349890232\n",
      "Epoch #318 - training loss: 0.017033056127538675 \t validation loss: 0.01642744056880474\n",
      "Epoch #319 - training loss: 0.017016064905679616 \t validation loss: 0.01654844917356968\n",
      "Epoch #320 - training loss: 0.017055667091641195 \t validation loss: 0.01647045463323593\n",
      "Epoch #321 - training loss: 0.0170749618082255 \t validation loss: 0.016520829871296883\n",
      "Epoch #322 - training loss: 0.016995234399929393 \t validation loss: 0.01648767851293087\n",
      "Epoch #323 - training loss: 0.01706020306653523 \t validation loss: 0.016495779156684875\n",
      "Epoch #324 - training loss: 0.017101059023004312 \t validation loss: 0.016550924628973007\n",
      "Epoch #325 - training loss: 0.017032574123089162 \t validation loss: 0.01645922288298607\n",
      "Epoch #326 - training loss: 0.016962095479608714 \t validation loss: 0.0164793748408556\n",
      "Epoch #327 - training loss: 0.017000599170593534 \t validation loss: 0.016441090032458305\n",
      "Epoch #328 - training loss: 0.017025008843146776 \t validation loss: 0.016534890979528427\n",
      "Epoch #329 - training loss: 0.01700283594288149 \t validation loss: 0.016582021489739418\n",
      "Epoch #330 - training loss: 0.017081716629868206 \t validation loss: 0.016535921022295952\n",
      "Epoch #331 - training loss: 0.017141863222576662 \t validation loss: 0.016495851799845695\n",
      "Epoch #332 - training loss: 0.01706533161211728 \t validation loss: 0.01651013270020485\n",
      "Epoch #333 - training loss: 0.01708327091987324 \t validation loss: 0.016422152519226074\n",
      "Epoch #334 - training loss: 0.017006334802716693 \t validation loss: 0.016500452533364296\n",
      "Epoch #335 - training loss: 0.017050656777556808 \t validation loss: 0.016440855339169502\n",
      "Best validation loss at epoch=336, saving model\n",
      "Epoch #336 - training loss: 0.017009316769567958 \t validation loss: 0.016385776922106743\n",
      "Epoch #337 - training loss: 0.01697620843912779 \t validation loss: 0.016426241025328636\n",
      "Epoch #338 - training loss: 0.017016583277452774 \t validation loss: 0.016416985541582108\n",
      "Best validation loss at epoch=339, saving model\n",
      "Epoch #339 - training loss: 0.016979015658836107 \t validation loss: 0.016375601291656494\n",
      "Epoch #340 - training loss: 0.017052769166003068 \t validation loss: 0.01644306257367134\n",
      "Epoch #341 - training loss: 0.0169646395665638 \t validation loss: 0.016435476019978523\n",
      "Epoch #342 - training loss: 0.017048181560066704 \t validation loss: 0.016466334462165833\n",
      "Epoch #343 - training loss: 0.017035571423363087 \t validation loss: 0.016468960791826248\n",
      "Epoch #344 - training loss: 0.017125680330141665 \t validation loss: 0.016552696004509926\n",
      "Epoch #345 - training loss: 0.01705744196157399 \t validation loss: 0.01646181009709835\n",
      "Epoch #346 - training loss: 0.017078859499208395 \t validation loss: 0.016484452411532402\n",
      "Epoch #347 - training loss: 0.01697062349393228 \t validation loss: 0.01655506156384945\n",
      "Epoch #348 - training loss: 0.016974536503816175 \t validation loss: 0.016395818442106247\n",
      "Epoch #349 - training loss: 0.01709807107831098 \t validation loss: 0.016408458352088928\n",
      "Epoch #350 - training loss: 0.01698428476272875 \t validation loss: 0.016423964872956276\n",
      "Epoch #351 - training loss: 0.01704820120206641 \t validation loss: 0.016448860988020897\n",
      "Epoch #352 - training loss: 0.01696645242564783 \t validation loss: 0.016450271010398865\n",
      "Epoch #353 - training loss: 0.017050974982502153 \t validation loss: 0.016464294865727425\n",
      "Epoch #354 - training loss: 0.016937271663783556 \t validation loss: 0.016501594334840775\n",
      "Epoch #355 - training loss: 0.01698457226041015 \t validation loss: 0.016437772661447525\n",
      "Epoch #356 - training loss: 0.017011319334558725 \t validation loss: 0.01657986082136631\n",
      "Epoch #357 - training loss: 0.016995323936139346 \t validation loss: 0.016575230285525322\n",
      "Epoch #358 - training loss: 0.016946608440518734 \t validation loss: 0.016502749174833298\n",
      "Epoch #359 - training loss: 0.017065827053702454 \t validation loss: 0.016541069373488426\n",
      "Epoch #360 - training loss: 0.01695089795643158 \t validation loss: 0.016497274860739708\n",
      "Epoch #361 - training loss: 0.016976458003808377 \t validation loss: 0.016516517847776413\n",
      "Epoch #362 - training loss: 0.017100068045201134 \t validation loss: 0.01658511906862259\n",
      "Epoch #363 - training loss: 0.017026183912056167 \t validation loss: 0.016428591683506966\n",
      "Epoch #364 - training loss: 0.01699491359730079 \t validation loss: 0.016515547409653664\n",
      "Epoch #365 - training loss: 0.0171045391915246 \t validation loss: 0.01654881052672863\n",
      "Epoch #366 - training loss: 0.016959385717192785 \t validation loss: 0.016486693173646927\n",
      "Epoch #367 - training loss: 0.017091361014366056 \t validation loss: 0.016459081321954727\n",
      "Epoch #368 - training loss: 0.01697499481157599 \t validation loss: 0.016564631834626198\n",
      "Epoch #369 - training loss: 0.016945826833372597 \t validation loss: 0.016528036445379257\n",
      "Epoch #370 - training loss: 0.01695365510278782 \t validation loss: 0.01650175079703331\n",
      "Epoch #371 - training loss: 0.017098320924953836 \t validation loss: 0.016553746536374092\n",
      "Epoch #372 - training loss: 0.017034809555027734 \t validation loss: 0.01653101295232773\n",
      "Epoch #373 - training loss: 0.01699514866476441 \t validation loss: 0.016455912962555885\n",
      "Epoch #374 - training loss: 0.01701352922192614 \t validation loss: 0.016459668055176735\n",
      "Epoch #375 - training loss: 0.017097375592364003 \t validation loss: 0.01646266132593155\n",
      "Epoch #376 - training loss: 0.017039157990646973 \t validation loss: 0.016440894454717636\n",
      "Epoch #377 - training loss: 0.01695020656873958 \t validation loss: 0.016469046473503113\n",
      "Epoch #378 - training loss: 0.017077280415644144 \t validation loss: 0.0164622962474823\n",
      "Epoch #379 - training loss: 0.016998280824080746 \t validation loss: 0.01645924523472786\n",
      "Epoch #380 - training loss: 0.01700515422695113 \t validation loss: 0.01646348088979721\n",
      "Epoch #381 - training loss: 0.017008922504272755 \t validation loss: 0.01651076227426529\n",
      "Epoch #382 - training loss: 0.01708075977167746 \t validation loss: 0.016468187794089317\n",
      "Epoch #383 - training loss: 0.017097430558305954 \t validation loss: 0.01651516743004322\n",
      "Epoch #384 - training loss: 0.01702919925404267 \t validation loss: 0.016495632007718086\n",
      "Epoch #385 - training loss: 0.016919050949941276 \t validation loss: 0.016458598896861076\n",
      "Epoch #386 - training loss: 0.016955286909181338 \t validation loss: 0.01640489138662815\n",
      "Epoch #387 - training loss: 0.017083653921925554 \t validation loss: 0.01647893525660038\n",
      "Epoch #388 - training loss: 0.016967208556232592 \t validation loss: 0.016546020284295082\n",
      "Epoch #389 - training loss: 0.017033536024168985 \t validation loss: 0.01649888977408409\n",
      "Epoch #390 - training loss: 0.016988645582905196 \t validation loss: 0.016460828483104706\n",
      "Epoch #391 - training loss: 0.016896748844642588 \t validation loss: 0.01643487624824047\n",
      "Epoch #392 - training loss: 0.016918921835184206 \t validation loss: 0.016440479084849358\n",
      "Epoch #393 - training loss: 0.017062546201524656 \t validation loss: 0.01640971377491951\n",
      "Epoch #394 - training loss: 0.016982219225761874 \t validation loss: 0.016417721286416054\n",
      "Epoch #395 - training loss: 0.01701537906114794 \t validation loss: 0.016437679529190063\n",
      "Epoch #396 - training loss: 0.01703914232782462 \t validation loss: 0.016558201983571053\n",
      "Epoch #397 - training loss: 0.017063228255176525 \t validation loss: 0.016571976244449615\n",
      "Epoch #398 - training loss: 0.016941978119114288 \t validation loss: 0.016471827402710915\n",
      "Epoch #399 - training loss: 0.016982406702835556 \t validation loss: 0.016509316861629486\n",
      "Epoch #400 - training loss: 0.017038517982772554 \t validation loss: 0.016515564173460007\n",
      "Epoch #401 - training loss: 0.01705452114035784 \t validation loss: 0.016479508951306343\n",
      "Epoch #402 - training loss: 0.016967020097736603 \t validation loss: 0.01652100868523121\n",
      "Epoch #403 - training loss: 0.01701518591554359 \t validation loss: 0.016507085412740707\n",
      "Epoch #404 - training loss: 0.017076555241611852 \t validation loss: 0.01650744304060936\n",
      "Epoch #405 - training loss: 0.016910371100026612 \t validation loss: 0.01646549254655838\n",
      "Epoch #406 - training loss: 0.01694044853215011 \t validation loss: 0.01642540656030178\n",
      "Epoch #407 - training loss: 0.01712533879759886 \t validation loss: 0.01650473102927208\n",
      "Epoch #408 - training loss: 0.01695972369918929 \t validation loss: 0.016454139724373817\n",
      "Epoch #409 - training loss: 0.017024698926681913 \t validation loss: 0.016429640352725983\n",
      "Epoch #410 - training loss: 0.017039481321980948 \t validation loss: 0.01653529889881611\n",
      "Epoch #411 - training loss: 0.01696464610687823 \t validation loss: 0.01651212014257908\n",
      "Epoch #412 - training loss: 0.016936399375878493 \t validation loss: 0.016521556302905083\n",
      "Epoch #413 - training loss: 0.016917120528808192 \t validation loss: 0.016465723514556885\n",
      "Epoch #414 - training loss: 0.017047475046127757 \t validation loss: 0.016424382105469704\n",
      "Epoch #415 - training loss: 0.01709390476030412 \t validation loss: 0.016461936756968498\n",
      "Epoch #416 - training loss: 0.016974073240297254 \t validation loss: 0.016425708308815956\n",
      "Epoch #417 - training loss: 0.017038718167994474 \t validation loss: 0.016418129205703735\n",
      "Epoch #418 - training loss: 0.016997164599849162 \t validation loss: 0.01648271642625332\n",
      "Epoch #419 - training loss: 0.017051269876791973 \t validation loss: 0.016409382224082947\n",
      "Epoch #420 - training loss: 0.016972015945341538 \t validation loss: 0.016523493453860283\n",
      "Epoch #421 - training loss: 0.016964609264454217 \t validation loss: 0.016384458169341087\n",
      "Epoch #422 - training loss: 0.016938486255331665 \t validation loss: 0.016447538509964943\n",
      "Epoch #423 - training loss: 0.01698008869491951 \t validation loss: 0.016464555636048317\n",
      "Epoch #424 - training loss: 0.017093328978593333 \t validation loss: 0.016412030905485153\n",
      "Epoch #425 - training loss: 0.016946810710083176 \t validation loss: 0.016486123204231262\n",
      "Epoch #426 - training loss: 0.016934763520119592 \t validation loss: 0.016501205042004585\n",
      "Epoch #427 - training loss: 0.017030840953560295 \t validation loss: 0.016415663063526154\n",
      "Epoch #428 - training loss: 0.016930161745639872 \t validation loss: 0.016471125185489655\n",
      "Epoch #429 - training loss: 0.017015094584456245 \t validation loss: 0.016474073752760887\n",
      "Epoch #430 - training loss: 0.01692892697285435 \t validation loss: 0.016491249203681946\n",
      "Epoch #431 - training loss: 0.017012064573655134 \t validation loss: 0.01649981178343296\n",
      "Epoch #432 - training loss: 0.01688723765476268 \t validation loss: 0.01646394096314907\n",
      "Epoch #433 - training loss: 0.016920188197424624 \t validation loss: 0.01644383743405342\n",
      "Epoch #434 - training loss: 0.016955434444182304 \t validation loss: 0.01639973558485508\n",
      "Epoch #435 - training loss: 0.016995058044512405 \t validation loss: 0.016416965052485466\n",
      "Epoch #436 - training loss: 0.01690290508072498 \t validation loss: 0.016453148797154427\n",
      "Epoch #437 - training loss: 0.016924016174991962 \t validation loss: 0.016568761318922043\n",
      "Epoch #438 - training loss: 0.017026618197599035 \t validation loss: 0.016488928347826004\n",
      "Epoch #439 - training loss: 0.017066330680422282 \t validation loss: 0.016542863100767136\n",
      "Epoch #440 - training loss: 0.016979385984929863 \t validation loss: 0.01647947169840336\n",
      "Epoch #441 - training loss: 0.016924327110151398 \t validation loss: 0.01648123934864998\n",
      "Epoch #442 - training loss: 0.01697540349083064 \t validation loss: 0.016504250466823578\n",
      "Epoch #443 - training loss: 0.017012324526648494 \t validation loss: 0.016485875472426414\n",
      "Epoch #444 - training loss: 0.016924826233703914 \t validation loss: 0.016550574451684952\n",
      "Epoch #445 - training loss: 0.017030177669398284 \t validation loss: 0.01643228530883789\n",
      "Epoch #446 - training loss: 0.01691305722795396 \t validation loss: 0.01642640121281147\n",
      "Epoch #447 - training loss: 0.016961560235358775 \t validation loss: 0.016461849212646484\n",
      "Epoch #448 - training loss: 0.0170444156204906 \t validation loss: 0.016499074175953865\n",
      "Epoch #449 - training loss: 0.016954557702241548 \t validation loss: 0.016501788049936295\n",
      "Epoch #450 - training loss: 0.017008609226527246 \t validation loss: 0.016545699909329414\n",
      "Epoch #451 - training loss: 0.01705110883107893 \t validation loss: 0.016438907012343407\n",
      "Epoch #452 - training loss: 0.0169294324733526 \t validation loss: 0.016461387276649475\n",
      "Epoch #453 - training loss: 0.016920744127055666 \t validation loss: 0.01650306209921837\n",
      "Epoch #454 - training loss: 0.016975726226292426 \t validation loss: 0.016466444358229637\n",
      "Epoch #455 - training loss: 0.017028489530574446 \t validation loss: 0.016474483534693718\n",
      "Epoch #456 - training loss: 0.01699359367433122 \t validation loss: 0.016472341492772102\n",
      "Epoch #457 - training loss: 0.017005332182897275 \t validation loss: 0.016462089493870735\n",
      "Epoch #458 - training loss: 0.016960867866970552 \t validation loss: 0.01645052433013916\n",
      "Epoch #459 - training loss: 0.016956490850058653 \t validation loss: 0.016471218317747116\n",
      "Epoch #460 - training loss: 0.016952281471452214 \t validation loss: 0.016515273600816727\n",
      "Epoch #461 - training loss: 0.0170743469557417 \t validation loss: 0.016513483598828316\n",
      "Epoch #462 - training loss: 0.01696678365507161 \t validation loss: 0.016533849760890007\n",
      "Epoch #463 - training loss: 0.016966386952799534 \t validation loss: 0.016515476629137993\n",
      "Epoch #464 - training loss: 0.016888079488564772 \t validation loss: 0.0164489708840847\n",
      "Epoch #465 - training loss: 0.01707889163177344 \t validation loss: 0.016469476744532585\n",
      "Epoch #466 - training loss: 0.01717033632304868 \t validation loss: 0.016520632430911064\n",
      "Epoch #467 - training loss: 0.016974613652806614 \t validation loss: 0.016434693709015846\n",
      "Epoch #468 - training loss: 0.016990705349205715 \t validation loss: 0.016430022194981575\n",
      "Epoch #469 - training loss: 0.01705974228718858 \t validation loss: 0.01645795814692974\n",
      "Epoch #470 - training loss: 0.01704146109885399 \t validation loss: 0.016491200774908066\n",
      "Epoch #471 - training loss: 0.016994809549352206 \t validation loss: 0.016421325504779816\n",
      "Epoch #472 - training loss: 0.01700945380589152 \t validation loss: 0.016462888568639755\n",
      "Epoch #473 - training loss: 0.01699414217831052 \t validation loss: 0.016508469358086586\n",
      "Epoch #474 - training loss: 0.016996904365861647 \t validation loss: 0.016447199508547783\n",
      "Epoch #475 - training loss: 0.016986501774907483 \t validation loss: 0.01651935838162899\n",
      "Epoch #476 - training loss: 0.01701692927056602 \t validation loss: 0.016468683257699013\n",
      "Epoch #477 - training loss: 0.01697711668968619 \t validation loss: 0.016468223184347153\n",
      "Epoch #478 - training loss: 0.017042159050821157 \t validation loss: 0.016445394605398178\n",
      "Epoch #479 - training loss: 0.016885291865596976 \t validation loss: 0.01645021326839924\n",
      "Epoch #480 - training loss: 0.016969173121906957 \t validation loss: 0.016492174938321114\n",
      "Epoch #481 - training loss: 0.016931619108393168 \t validation loss: 0.01650090143084526\n",
      "Epoch #482 - training loss: 0.01701966862595038 \t validation loss: 0.016515711322426796\n",
      "Epoch #483 - training loss: 0.016869003565985856 \t validation loss: 0.016435351222753525\n",
      "Epoch #484 - training loss: 0.016902493308114348 \t validation loss: 0.01643519103527069\n",
      "Epoch #485 - training loss: 0.01687617313354944 \t validation loss: 0.016583824530243874\n",
      "Epoch #486 - training loss: 0.016867448198715548 \t validation loss: 0.01664132811129093\n",
      "Epoch #487 - training loss: 0.016999420082814246 \t validation loss: 0.01654135435819626\n",
      "Epoch #488 - training loss: 0.016987598498241507 \t validation loss: 0.016562042757868767\n",
      "Epoch #489 - training loss: 0.016914136075606403 \t validation loss: 0.01651160791516304\n",
      "Epoch #490 - training loss: 0.017033622382555288 \t validation loss: 0.01651620864868164\n",
      "Epoch #491 - training loss: 0.016968515027786863 \t validation loss: 0.016511792317032814\n",
      "Epoch #492 - training loss: 0.017106831364194223 \t validation loss: 0.016677720472216606\n",
      "Epoch #493 - training loss: 0.016969470304326616 \t validation loss: 0.016529515385627747\n",
      "Epoch #494 - training loss: 0.016919396717968764 \t validation loss: 0.016483142971992493\n",
      "Epoch #495 - training loss: 0.01699759915017033 \t validation loss: 0.016469648107886314\n",
      "Epoch #496 - training loss: 0.016952986286598762 \t validation loss: 0.01648227497935295\n",
      "Epoch #497 - training loss: 0.01700823814095058 \t validation loss: 0.016478294506669044\n",
      "Epoch #498 - training loss: 0.016922408126642165 \t validation loss: 0.016496378928422928\n",
      "Epoch #499 - training loss: 0.016980987628379723 \t validation loss: 0.0164350438863039\n",
      "Epoch #500 - training loss: 0.01696406663109872 \t validation loss: 0.016513844951987267\n",
      "        productid  predictedyQuantity\n",
      "0   410039287_650            1.545671\n",
      "1  410068505_1146            1.315302\n",
      "2   410081289_ugr            1.284053\n",
      "3  410084807_rqr0            1.802777\n",
      "4    410139520_0m            1.293410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38732/97459344.py:31: FutureWarning: Passing 'use_legacy_dataset=True' to get the legacy behaviour is deprecated as of pyarrow 8.0.0, and the legacy implementation will be removed in a future version.\n",
      "  pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=f\"{run_dir}/experiments/exp_neural_network/predictions/ModelForEachBrickProductLevel/{of}/predictions\", filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)\n"
     ]
    }
   ],
   "source": [
    "experiment(\"menShirts\", epochs = 500, lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22001, 36)\n",
      "(55630, 35)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22001 entries, 0 to 22000\n",
      "Data columns (total 36 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   similargrouplevel       22001 non-null  object \n",
      " 1   productid               22001 non-null  object \n",
      " 2   0_monthSales            22001 non-null  float64\n",
      " 3   1_monthSales            22001 non-null  float64\n",
      " 4   2_monthSales            22001 non-null  float64\n",
      " 5   3_monthSales            22001 non-null  float64\n",
      " 6   4_monthSales            22001 non-null  float64\n",
      " 7   5_monthSales            22001 non-null  float64\n",
      " 8   6_monthSales            22001 non-null  float64\n",
      " 9   7_monthSales            22001 non-null  float64\n",
      " 10  8_monthSales            22001 non-null  float64\n",
      " 11  9_monthSales            22001 non-null  float64\n",
      " 12  10_monthSales           22001 non-null  float64\n",
      " 13  11_monthSales           22001 non-null  float64\n",
      " 14  avgSales                22001 non-null  float64\n",
      " 15  stddevSales             22001 non-null  float64\n",
      " 16  totalReturn             22001 non-null  float64\n",
      " 17  totalUsers              22001 non-null  float64\n",
      " 18  totalPLPViews           22001 non-null  float64\n",
      " 19  totalPLPClicks          22001 non-null  float64\n",
      " 20  totalPDPCount           22001 non-null  float64\n",
      " 21  totalATC                22001 non-null  float64\n",
      " 22  totalWishList           22001 non-null  float64\n",
      " 23  totalAvailableQuantity  22001 non-null  float64\n",
      " 24  yQuantity               22001 non-null  float64\n",
      " 25  pricebucket             22001 non-null  int32  \n",
      " 26  normalized_color_0      22001 non-null  float64\n",
      " 27  normalized_color_1      22001 non-null  float64\n",
      " 28  normalized_pattern_0    22001 non-null  float64\n",
      " 29  normalized_pattern_1    22001 non-null  float64\n",
      " 30  normalized_brand_0      22001 non-null  float64\n",
      " 31  normalized_brand_1      22001 non-null  float64\n",
      " 32  normalized_style_0      22001 non-null  float64\n",
      " 33  normalized_style_1      22001 non-null  float64\n",
      " 34  normalized_sleeve_0     22001 non-null  float64\n",
      " 35  normalized_sleeve_1     22001 non-null  float64\n",
      "dtypes: float64(33), int32(1), object(2)\n",
      "memory usage: 6.0+ MB\n",
      "['0_monthSales', '1_monthSales', '2_monthSales', '3_monthSales', '4_monthSales', '5_monthSales', '6_monthSales', '7_monthSales', '8_monthSales', '9_monthSales', '10_monthSales', '11_monthSales', 'avgSales', 'stddevSales', 'totalReturn', 'totalUsers', 'totalPLPViews', 'totalPLPClicks', 'totalPDPCount', 'totalATC', 'totalWishList', 'totalAvailableQuantity', 'pricebucket']\n",
      "[1.] [6698.]\n",
      "(17974, 36) (4400, 36)\n",
      "cuda:0\n",
      "Best validation loss at epoch=1, saving model\n",
      "Epoch #1 - training loss: 0.1421708999643245 \t validation loss: 0.037063613533973694\n",
      "Best validation loss at epoch=2, saving model\n",
      "Epoch #2 - training loss: 0.039818442187565854 \t validation loss: 0.03309031203389168\n",
      "Best validation loss at epoch=3, saving model\n",
      "Epoch #3 - training loss: 0.036677126492142886 \t validation loss: 0.03232888504862785\n",
      "Best validation loss at epoch=4, saving model\n",
      "Epoch #4 - training loss: 0.03541229642369849 \t validation loss: 0.0316346175968647\n",
      "Best validation loss at epoch=5, saving model\n",
      "Epoch #5 - training loss: 0.034988616758363185 \t validation loss: 0.03121994435787201\n",
      "Best validation loss at epoch=6, saving model\n",
      "Epoch #6 - training loss: 0.03441498247464976 \t validation loss: 0.031003545969724655\n",
      "Best validation loss at epoch=7, saving model\n",
      "Epoch #7 - training loss: 0.033987827047891585 \t validation loss: 0.030049528926610947\n",
      "Epoch #8 - training loss: 0.03337772358325103 \t validation loss: 0.030254174023866653\n",
      "Best validation loss at epoch=9, saving model\n",
      "Epoch #9 - training loss: 0.03282979859699912 \t validation loss: 0.029863104224205017\n",
      "Epoch #10 - training loss: 0.032679866683966545 \t validation loss: 0.029863260686397552\n",
      "Best validation loss at epoch=11, saving model\n",
      "Epoch #11 - training loss: 0.03222527411731488 \t validation loss: 0.029522208496928215\n",
      "Best validation loss at epoch=12, saving model\n",
      "Epoch #12 - training loss: 0.03191089267890615 \t validation loss: 0.029326435178518295\n",
      "Best validation loss at epoch=13, saving model\n",
      "Epoch #13 - training loss: 0.03174799099919816 \t validation loss: 0.02913038805127144\n",
      "Best validation loss at epoch=14, saving model\n",
      "Epoch #14 - training loss: 0.03175359110575362 \t validation loss: 0.028999317437410355\n",
      "Epoch #15 - training loss: 0.03148366333983188 \t validation loss: 0.029021253809332848\n",
      "Best validation loss at epoch=16, saving model\n",
      "Epoch #16 - training loss: 0.031016193275504252 \t validation loss: 0.028739120811223984\n",
      "Best validation loss at epoch=17, saving model\n",
      "Epoch #17 - training loss: 0.031356777682063734 \t validation loss: 0.028565915301442146\n",
      "Epoch #18 - training loss: 0.031055376059815634 \t validation loss: 0.028590675443410873\n",
      "Epoch #19 - training loss: 0.030836900184014 \t validation loss: 0.028691967949271202\n",
      "Best validation loss at epoch=20, saving model\n",
      "Epoch #20 - training loss: 0.030666051141140772 \t validation loss: 0.02845783159136772\n",
      "Epoch #21 - training loss: 0.03055986882155884 \t validation loss: 0.02855146862566471\n",
      "Best validation loss at epoch=22, saving model\n",
      "Epoch #22 - training loss: 0.030710280562440555 \t validation loss: 0.028399137780070305\n",
      "Epoch #23 - training loss: 0.03026631569702331 \t validation loss: 0.02861083298921585\n",
      "Epoch #24 - training loss: 0.030346650006033732 \t validation loss: 0.0285064447671175\n",
      "Best validation loss at epoch=25, saving model\n",
      "Epoch #25 - training loss: 0.030108346571438613 \t validation loss: 0.028379779309034348\n",
      "Best validation loss at epoch=26, saving model\n",
      "Epoch #26 - training loss: 0.030019990410888153 \t validation loss: 0.028374159708619118\n",
      "Best validation loss at epoch=27, saving model\n",
      "Epoch #27 - training loss: 0.029965168105043245 \t validation loss: 0.02817322313785553\n",
      "Epoch #28 - training loss: 0.030057966875148116 \t validation loss: 0.02829767018556595\n",
      "Best validation loss at epoch=29, saving model\n",
      "Epoch #29 - training loss: 0.029789603114420294 \t validation loss: 0.028124475851655006\n",
      "Epoch #30 - training loss: 0.029780200737364152 \t validation loss: 0.028193574398756027\n",
      "Epoch #31 - training loss: 0.029709849479224754 \t validation loss: 0.028233367949724197\n",
      "Epoch #32 - training loss: 0.029519894426778197 \t validation loss: 0.028242843225598335\n",
      "Epoch #33 - training loss: 0.029670734962850224 \t validation loss: 0.0281984806060791\n",
      "Epoch #34 - training loss: 0.02943536794018963 \t validation loss: 0.028177358210086823\n",
      "Epoch #35 - training loss: 0.029439533516970495 \t validation loss: 0.028129566460847855\n",
      "Best validation loss at epoch=36, saving model\n",
      "Epoch #36 - training loss: 0.029468441425213415 \t validation loss: 0.028099024668335915\n",
      "Epoch #37 - training loss: 0.029433986993005198 \t validation loss: 0.028256524354219437\n",
      "Best validation loss at epoch=38, saving model\n",
      "Epoch #38 - training loss: 0.029384388886358914 \t validation loss: 0.028080951422452927\n",
      "Epoch #39 - training loss: 0.029267064884218673 \t validation loss: 0.02820449508726597\n",
      "Epoch #40 - training loss: 0.029172464944555054 \t validation loss: 0.028162796050310135\n",
      "Best validation loss at epoch=41, saving model\n",
      "Epoch #41 - training loss: 0.029340701016685435 \t validation loss: 0.028063325211405754\n",
      "Best validation loss at epoch=42, saving model\n",
      "Epoch #42 - training loss: 0.02916784446234956 \t validation loss: 0.028035087510943413\n",
      "Epoch #43 - training loss: 0.02927189260561836 \t validation loss: 0.028077825903892517\n",
      "Best validation loss at epoch=44, saving model\n",
      "Epoch #44 - training loss: 0.029145849505920834 \t validation loss: 0.02799694798886776\n",
      "Best validation loss at epoch=45, saving model\n",
      "Epoch #45 - training loss: 0.029266073107022176 \t validation loss: 0.027994021773338318\n",
      "Epoch #46 - training loss: 0.029050338121028134 \t validation loss: 0.028014549985527992\n",
      "Epoch #47 - training loss: 0.029127951754579145 \t validation loss: 0.028079325333237648\n",
      "Epoch #48 - training loss: 0.02907558968921529 \t validation loss: 0.02801436372101307\n",
      "Best validation loss at epoch=49, saving model\n",
      "Epoch #49 - training loss: 0.029068611083309864 \t validation loss: 0.027972446754574776\n",
      "Best validation loss at epoch=50, saving model\n",
      "Epoch #50 - training loss: 0.028997313723452853 \t validation loss: 0.027956143021583557\n",
      "Epoch #51 - training loss: 0.028975158119690397 \t validation loss: 0.027991579845547676\n",
      "Epoch #52 - training loss: 0.029048652709387043 \t validation loss: 0.027992699295282364\n",
      "Best validation loss at epoch=53, saving model\n",
      "Epoch #53 - training loss: 0.028984681929755497 \t validation loss: 0.027899378910660744\n",
      "Epoch #54 - training loss: 0.029013448891208347 \t validation loss: 0.027974272146821022\n",
      "Epoch #55 - training loss: 0.028842785608176134 \t validation loss: 0.027958305552601814\n",
      "Epoch #56 - training loss: 0.028730202279486916 \t validation loss: 0.027989087626338005\n",
      "Best validation loss at epoch=57, saving model\n",
      "Epoch #57 - training loss: 0.02891710876867361 \t validation loss: 0.027879299595952034\n",
      "Epoch #58 - training loss: 0.028828124514052148 \t validation loss: 0.0280555821955204\n",
      "Epoch #59 - training loss: 0.028932304043528655 \t validation loss: 0.027933616191148758\n",
      "Epoch #60 - training loss: 0.028966503984787884 \t validation loss: 0.027975276112556458\n",
      "Epoch #61 - training loss: 0.028833791850974747 \t validation loss: 0.02799924463033676\n",
      "Epoch #62 - training loss: 0.028721793836942226 \t validation loss: 0.027939962223172188\n",
      "Epoch #63 - training loss: 0.028875097664780316 \t validation loss: 0.027935121208429337\n",
      "Epoch #64 - training loss: 0.028867595493727083 \t validation loss: 0.02791142649948597\n",
      "Epoch #65 - training loss: 0.02880586539071432 \t validation loss: 0.027961932122707367\n",
      "Epoch #66 - training loss: 0.028767427636240732 \t validation loss: 0.02794325165450573\n",
      "Epoch #67 - training loss: 0.028781400105306703 \t validation loss: 0.02802075445652008\n",
      "Epoch #68 - training loss: 0.02882545791672619 \t validation loss: 0.027978260070085526\n",
      "Epoch #69 - training loss: 0.02886238774913903 \t validation loss: 0.02789420448243618\n",
      "Epoch #70 - training loss: 0.028853552930601566 \t validation loss: 0.02795654721558094\n",
      "Epoch #71 - training loss: 0.028809669879954074 \t validation loss: 0.02794749103486538\n",
      "Epoch #72 - training loss: 0.02865932439827425 \t validation loss: 0.02789662405848503\n",
      "Epoch #73 - training loss: 0.02873580179670278 \t validation loss: 0.027924848720431328\n",
      "Epoch #74 - training loss: 0.02864444832551607 \t validation loss: 0.027948476374149323\n",
      "Epoch #75 - training loss: 0.02870013635296793 \t validation loss: 0.027975335717201233\n",
      "Epoch #76 - training loss: 0.028804725592162413 \t validation loss: 0.028020363301038742\n",
      "Epoch #77 - training loss: 0.028717068318649463 \t validation loss: 0.027946704998612404\n",
      "Epoch #78 - training loss: 0.028766714091248372 \t validation loss: 0.02791495993733406\n",
      "Best validation loss at epoch=79, saving model\n",
      "Epoch #79 - training loss: 0.028908498299684966 \t validation loss: 0.027852846309542656\n",
      "Epoch #80 - training loss: 0.02878274269727409 \t validation loss: 0.027976924553513527\n",
      "Epoch #81 - training loss: 0.028753061009381704 \t validation loss: 0.02789352461695671\n",
      "Epoch #82 - training loss: 0.028844791006316796 \t validation loss: 0.02785559557378292\n",
      "Epoch #83 - training loss: 0.028811829889262904 \t validation loss: 0.027903610840439796\n",
      "Epoch #84 - training loss: 0.02860907697718119 \t validation loss: 0.02797621861100197\n",
      "Epoch #85 - training loss: 0.028619624395421484 \t validation loss: 0.027909966185688972\n",
      "Epoch #86 - training loss: 0.028724830950412786 \t validation loss: 0.027920009568333626\n",
      "Best validation loss at epoch=87, saving model\n",
      "Epoch #87 - training loss: 0.028619576667214673 \t validation loss: 0.027844857424497604\n",
      "Epoch #88 - training loss: 0.028699958970879808 \t validation loss: 0.027883388102054596\n",
      "Epoch #89 - training loss: 0.028775376982409208 \t validation loss: 0.027879860252141953\n",
      "Epoch #90 - training loss: 0.028733224013684163 \t validation loss: 0.02784799411892891\n",
      "Best validation loss at epoch=91, saving model\n",
      "Epoch #91 - training loss: 0.02875984856301052 \t validation loss: 0.027799416333436966\n",
      "Epoch #92 - training loss: 0.028648317604759 \t validation loss: 0.027897225692868233\n",
      "Epoch #93 - training loss: 0.02870657957207126 \t validation loss: 0.02784726396203041\n",
      "Epoch #94 - training loss: 0.02871882220989163 \t validation loss: 0.02783534862101078\n",
      "Epoch #95 - training loss: 0.028712699308480118 \t validation loss: 0.027960166335105896\n",
      "Epoch #96 - training loss: 0.02870576676319946 \t validation loss: 0.027843981981277466\n",
      "Best validation loss at epoch=97, saving model\n",
      "Epoch #97 - training loss: 0.028566483963519972 \t validation loss: 0.027795933187007904\n",
      "Epoch #98 - training loss: 0.028611543864748856 \t validation loss: 0.027835719287395477\n",
      "Epoch #99 - training loss: 0.02873215550032184 \t validation loss: 0.027818622067570686\n",
      "Best validation loss at epoch=100, saving model\n",
      "Epoch #100 - training loss: 0.028708928079575088 \t validation loss: 0.027789339423179626\n",
      "Best validation loss at epoch=101, saving model\n",
      "Epoch #101 - training loss: 0.02866250108728275 \t validation loss: 0.027785591781139374\n",
      "Epoch #102 - training loss: 0.02875835149849428 \t validation loss: 0.027891550213098526\n",
      "Epoch #103 - training loss: 0.028579232575116204 \t validation loss: 0.02780808135867119\n",
      "Best validation loss at epoch=104, saving model\n",
      "Epoch #104 - training loss: 0.02856181334511588 \t validation loss: 0.027781261131167412\n",
      "Epoch #105 - training loss: 0.028652500660024224 \t validation loss: 0.02790057472884655\n",
      "Epoch #106 - training loss: 0.028564674497553795 \t validation loss: 0.027885645627975464\n",
      "Epoch #107 - training loss: 0.028670915848211968 \t validation loss: 0.02788492478430271\n",
      "Epoch #108 - training loss: 0.028481920910484543 \t validation loss: 0.027824170887470245\n",
      "Best validation loss at epoch=109, saving model\n",
      "Epoch #109 - training loss: 0.028609357780898105 \t validation loss: 0.02778022363781929\n",
      "Epoch #110 - training loss: 0.028591490954944263 \t validation loss: 0.0278460830450058\n",
      "Epoch #111 - training loss: 0.02853692889150478 \t validation loss: 0.02786361053586006\n",
      "Epoch #112 - training loss: 0.028697524869685027 \t validation loss: 0.027786752209067345\n",
      "Epoch #113 - training loss: 0.028601167043720815 \t validation loss: 0.02781721204519272\n",
      "Epoch #114 - training loss: 0.028649275852267675 \t validation loss: 0.02779844030737877\n",
      "Best validation loss at epoch=115, saving model\n",
      "Epoch #115 - training loss: 0.028563484901881073 \t validation loss: 0.02777671441435814\n",
      "Epoch #116 - training loss: 0.02864731476291908 \t validation loss: 0.027809608727693558\n",
      "Epoch #117 - training loss: 0.02860555903730439 \t validation loss: 0.027837686240673065\n",
      "Best validation loss at epoch=118, saving model\n",
      "Epoch #118 - training loss: 0.028555279778592197 \t validation loss: 0.027763407677412033\n",
      "Epoch #119 - training loss: 0.028600696173833547 \t validation loss: 0.027791013941168785\n",
      "Epoch #120 - training loss: 0.028625261799702935 \t validation loss: 0.02781664952635765\n",
      "Best validation loss at epoch=121, saving model\n",
      "Epoch #121 - training loss: 0.028598001099784266 \t validation loss: 0.02774753049015999\n",
      "Epoch #122 - training loss: 0.028601969083449048 \t validation loss: 0.02776685357093811\n",
      "Best validation loss at epoch=123, saving model\n",
      "Epoch #123 - training loss: 0.028565994385835746 \t validation loss: 0.027731878682971\n",
      "Epoch #124 - training loss: 0.028530686869619685 \t validation loss: 0.027852511033415794\n",
      "Best validation loss at epoch=125, saving model\n",
      "Epoch #125 - training loss: 0.02854423376992212 \t validation loss: 0.027711467817425728\n",
      "Epoch #126 - training loss: 0.028550449508921296 \t validation loss: 0.027762528508901596\n",
      "Epoch #127 - training loss: 0.02850961890121434 \t validation loss: 0.027753861621022224\n",
      "Best validation loss at epoch=128, saving model\n",
      "Epoch #128 - training loss: 0.028691476445857587 \t validation loss: 0.027710972353816032\n",
      "Epoch #129 - training loss: 0.028699057776839496 \t validation loss: 0.027728544548153877\n",
      "Best validation loss at epoch=130, saving model\n",
      "Epoch #130 - training loss: 0.028453309698206326 \t validation loss: 0.027693375945091248\n",
      "Epoch #131 - training loss: 0.028545456999869694 \t validation loss: 0.027780024334788322\n",
      "Best validation loss at epoch=132, saving model\n",
      "Epoch #132 - training loss: 0.02855082460365353 \t validation loss: 0.027676446363329887\n",
      "Epoch #133 - training loss: 0.028411629534228548 \t validation loss: 0.02775532752275467\n",
      "Epoch #134 - training loss: 0.028541043031389646 \t validation loss: 0.02767895720899105\n",
      "Epoch #135 - training loss: 0.028574373368871083 \t validation loss: 0.02775156870484352\n",
      "Epoch #136 - training loss: 0.028534782791440498 \t validation loss: 0.027706842869520187\n",
      "Epoch #137 - training loss: 0.028489215515301616 \t validation loss: 0.0278002992272377\n",
      "Epoch #138 - training loss: 0.028475336976176702 \t validation loss: 0.02770991250872612\n",
      "Epoch #139 - training loss: 0.028516537263332045 \t validation loss: 0.0277144443243742\n",
      "Epoch #140 - training loss: 0.028602536198302917 \t validation loss: 0.027752604335546494\n",
      "Epoch #141 - training loss: 0.028716924969677992 \t validation loss: 0.02771272137761116\n",
      "Epoch #142 - training loss: 0.028609346282135517 \t validation loss: 0.027702555060386658\n",
      "Epoch #143 - training loss: 0.028575884334331315 \t validation loss: 0.02772456407546997\n",
      "Epoch #144 - training loss: 0.0283795104755517 \t validation loss: 0.027810385450720787\n",
      "Best validation loss at epoch=145, saving model\n",
      "Epoch #145 - training loss: 0.028506818501388326 \t validation loss: 0.02763931080698967\n",
      "Epoch #146 - training loss: 0.0284883681321168 \t validation loss: 0.027694042772054672\n",
      "Epoch #147 - training loss: 0.02848744789161678 \t validation loss: 0.02764820121228695\n",
      "Epoch #148 - training loss: 0.028435878549435186 \t validation loss: 0.027681663632392883\n",
      "Epoch #149 - training loss: 0.02860658367599382 \t validation loss: 0.027723392471671104\n",
      "Epoch #150 - training loss: 0.028474009277900007 \t validation loss: 0.02772282250225544\n",
      "Epoch #151 - training loss: 0.028586342736560712 \t validation loss: 0.027683302760124207\n",
      "Epoch #152 - training loss: 0.028480526120460023 \t validation loss: 0.027684448286890984\n",
      "Epoch #153 - training loss: 0.02839159910067411 \t validation loss: 0.027651524171233177\n",
      "Epoch #154 - training loss: 0.028427445252530555 \t validation loss: 0.027668248862028122\n",
      "Epoch #155 - training loss: 0.028502667097809745 \t validation loss: 0.027679434046149254\n",
      "Epoch #156 - training loss: 0.028600731007954017 \t validation loss: 0.02767372503876686\n",
      "Epoch #157 - training loss: 0.028412420644612447 \t validation loss: 0.02767387591302395\n",
      "Epoch #158 - training loss: 0.028472647726628764 \t validation loss: 0.027687370777130127\n",
      "Epoch #159 - training loss: 0.028565435004611492 \t validation loss: 0.02769266441464424\n",
      "Best validation loss at epoch=160, saving model\n",
      "Epoch #160 - training loss: 0.02849501949766528 \t validation loss: 0.027626749128103256\n",
      "Best validation loss at epoch=161, saving model\n",
      "Epoch #161 - training loss: 0.02849972432266768 \t validation loss: 0.02762426622211933\n",
      "Best validation loss at epoch=162, saving model\n",
      "Epoch #162 - training loss: 0.028480226483607728 \t validation loss: 0.027614755555987358\n",
      "Epoch #163 - training loss: 0.02854641171092576 \t validation loss: 0.02768431231379509\n",
      "Epoch #164 - training loss: 0.028446651285484355 \t validation loss: 0.027716614305973053\n",
      "Epoch #165 - training loss: 0.02843983508912106 \t validation loss: 0.027751503512263298\n",
      "Best validation loss at epoch=166, saving model\n",
      "Epoch #166 - training loss: 0.02850873422030353 \t validation loss: 0.02761455811560154\n",
      "Epoch #167 - training loss: 0.028374936806831556 \t validation loss: 0.02765555866062641\n",
      "Best validation loss at epoch=168, saving model\n",
      "Epoch #168 - training loss: 0.028354117220994783 \t validation loss: 0.027573173865675926\n",
      "Epoch #169 - training loss: 0.028484484801689785 \t validation loss: 0.02757909707725048\n",
      "Epoch #170 - training loss: 0.028447325113968202 \t validation loss: 0.027604006230831146\n",
      "Epoch #171 - training loss: 0.02840298340770411 \t validation loss: 0.027629969641566277\n",
      "Epoch #172 - training loss: 0.028384840804851097 \t validation loss: 0.027601458132267\n",
      "Epoch #173 - training loss: 0.028396202975123748 \t validation loss: 0.02764495275914669\n",
      "Epoch #174 - training loss: 0.028459125450760816 \t validation loss: 0.02761854976415634\n",
      "Epoch #175 - training loss: 0.028407438492996755 \t validation loss: 0.027617154642939568\n",
      "Epoch #176 - training loss: 0.028437454531503818 \t validation loss: 0.027611861005425453\n",
      "Best validation loss at epoch=177, saving model\n",
      "Epoch #177 - training loss: 0.028436462907867634 \t validation loss: 0.027539050206542015\n",
      "Epoch #178 - training loss: 0.028415014965662237 \t validation loss: 0.02756359428167343\n",
      "Epoch #179 - training loss: 0.028406336964166154 \t validation loss: 0.02756633423268795\n",
      "Epoch #180 - training loss: 0.02839452465796933 \t validation loss: 0.027564210817217827\n",
      "Epoch #181 - training loss: 0.028475894445782517 \t validation loss: 0.027552878484129906\n",
      "Epoch #182 - training loss: 0.028340549585899197 \t validation loss: 0.027564549818634987\n",
      "Best validation loss at epoch=183, saving model\n",
      "Epoch #183 - training loss: 0.02833537121573978 \t validation loss: 0.02753283828496933\n",
      "Epoch #184 - training loss: 0.02849608436874288 \t validation loss: 0.027586009353399277\n",
      "Epoch #185 - training loss: 0.028476040655126173 \t validation loss: 0.02754475176334381\n",
      "Epoch #186 - training loss: 0.02839862262587762 \t validation loss: 0.02762642689049244\n",
      "Epoch #187 - training loss: 0.02835584552327765 \t validation loss: 0.02760363183915615\n",
      "Epoch #188 - training loss: 0.02834422255114199 \t validation loss: 0.027664322406053543\n",
      "Epoch #189 - training loss: 0.028415142581756633 \t validation loss: 0.027547424659132957\n",
      "Epoch #190 - training loss: 0.028334849459283492 \t validation loss: 0.027581684291362762\n",
      "Epoch #191 - training loss: 0.028467163838671917 \t validation loss: 0.027630966156721115\n",
      "Epoch #192 - training loss: 0.028407759817546223 \t validation loss: 0.02764153853058815\n",
      "Epoch #193 - training loss: 0.02831934864089929 \t validation loss: 0.027561074122786522\n",
      "Epoch #194 - training loss: 0.02834007660185988 \t validation loss: 0.027572112157940865\n",
      "Epoch #195 - training loss: 0.02855130892625467 \t validation loss: 0.027563633397221565\n",
      "Epoch #196 - training loss: 0.028438192876902493 \t validation loss: 0.027591677382588387\n",
      "Epoch #197 - training loss: 0.028344967496314096 \t validation loss: 0.0275797750800848\n",
      "Epoch #198 - training loss: 0.028370403749420042 \t validation loss: 0.027606472373008728\n",
      "Epoch #199 - training loss: 0.028375235559674455 \t validation loss: 0.027613308280706406\n",
      "Best validation loss at epoch=200, saving model\n",
      "Epoch #200 - training loss: 0.028395025887146406 \t validation loss: 0.02752736397087574\n",
      "Epoch #201 - training loss: 0.028321920944719765 \t validation loss: 0.027573170140385628\n",
      "Best validation loss at epoch=202, saving model\n",
      "Epoch #202 - training loss: 0.0284073176078246 \t validation loss: 0.027509741485118866\n",
      "Epoch #203 - training loss: 0.028305263924221516 \t validation loss: 0.027559833601117134\n",
      "Best validation loss at epoch=204, saving model\n",
      "Epoch #204 - training loss: 0.0283470673689124 \t validation loss: 0.027503354474902153\n",
      "Epoch #205 - training loss: 0.028453725426658697 \t validation loss: 0.027519715949892998\n",
      "Epoch #206 - training loss: 0.02837527866530068 \t validation loss: 0.027558963745832443\n",
      "Epoch #207 - training loss: 0.02839715937036297 \t validation loss: 0.027521079406142235\n",
      "Epoch #208 - training loss: 0.02831758860724442 \t validation loss: 0.027598248794674873\n",
      "Epoch #209 - training loss: 0.02831159089152401 \t validation loss: 0.02751326747238636\n",
      "Epoch #210 - training loss: 0.02835352061923972 \t validation loss: 0.0275510773062706\n",
      "Epoch #211 - training loss: 0.02830201809654766 \t validation loss: 0.027514752000570297\n",
      "Epoch #212 - training loss: 0.028421277388658062 \t validation loss: 0.02756348066031933\n",
      "Epoch #213 - training loss: 0.028353223688369233 \t validation loss: 0.027582339942455292\n",
      "Epoch #214 - training loss: 0.02820684294649624 \t validation loss: 0.027503499761223793\n",
      "Epoch #215 - training loss: 0.028321459910993375 \t validation loss: 0.02753445692360401\n",
      "Best validation loss at epoch=216, saving model\n",
      "Epoch #216 - training loss: 0.02832238045778398 \t validation loss: 0.0274918582290411\n",
      "Epoch #217 - training loss: 0.028422662413906497 \t validation loss: 0.027571827173233032\n",
      "Epoch #218 - training loss: 0.028377293920353614 \t validation loss: 0.027564233168959618\n",
      "Best validation loss at epoch=219, saving model\n",
      "Epoch #219 - training loss: 0.028407272969915427 \t validation loss: 0.0274917334318161\n",
      "Epoch #220 - training loss: 0.028477419002575936 \t validation loss: 0.02755052037537098\n",
      "Best validation loss at epoch=221, saving model\n",
      "Epoch #221 - training loss: 0.028323030401413027 \t validation loss: 0.027464913204312325\n",
      "Best validation loss at epoch=222, saving model\n",
      "Epoch #222 - training loss: 0.02839197792941223 \t validation loss: 0.027458278462290764\n",
      "Epoch #223 - training loss: 0.02831924736496466 \t validation loss: 0.02749599888920784\n",
      "Epoch #224 - training loss: 0.028494154465004615 \t validation loss: 0.027526140213012695\n",
      "Epoch #225 - training loss: 0.028375566159267104 \t validation loss: 0.027493061497807503\n",
      "Epoch #226 - training loss: 0.028341353553846982 \t validation loss: 0.027535567060112953\n",
      "Epoch #227 - training loss: 0.028402779899610608 \t validation loss: 0.02748263254761696\n",
      "Epoch #228 - training loss: 0.02831425491318691 \t validation loss: 0.027571510523557663\n",
      "Epoch #229 - training loss: 0.028408496629001835 \t validation loss: 0.027563326060771942\n",
      "Epoch #230 - training loss: 0.028389876461861765 \t validation loss: 0.027621576562523842\n",
      "Best validation loss at epoch=231, saving model\n",
      "Epoch #231 - training loss: 0.028232575771888577 \t validation loss: 0.027456454932689667\n",
      "Epoch #232 - training loss: 0.028394065678630974 \t validation loss: 0.027497192844748497\n",
      "Best validation loss at epoch=233, saving model\n",
      "Epoch #233 - training loss: 0.02823090854759947 \t validation loss: 0.02745555154979229\n",
      "Epoch #234 - training loss: 0.028361703871804124 \t validation loss: 0.02752545289695263\n",
      "Epoch #235 - training loss: 0.028361730353073374 \t validation loss: 0.027535133063793182\n",
      "Epoch #236 - training loss: 0.02843327838728294 \t validation loss: 0.02748126909136772\n",
      "Epoch #237 - training loss: 0.02816777822516739 \t validation loss: 0.027456505224108696\n",
      "Best validation loss at epoch=238, saving model\n",
      "Epoch #238 - training loss: 0.02843342944705749 \t validation loss: 0.027438407763838768\n",
      "Epoch #239 - training loss: 0.028273958742698245 \t validation loss: 0.027514204382896423\n",
      "Epoch #240 - training loss: 0.02830356172138224 \t validation loss: 0.0274509247392416\n",
      "Epoch #241 - training loss: 0.028302911845817763 \t validation loss: 0.027483465149998665\n",
      "Epoch #242 - training loss: 0.028219980051584954 \t validation loss: 0.027468999847769737\n",
      "Epoch #243 - training loss: 0.028409553801312167 \t validation loss: 0.027561411261558533\n",
      "Epoch #244 - training loss: 0.028250681573388944 \t validation loss: 0.027489952743053436\n",
      "Epoch #245 - training loss: 0.028305218087711807 \t validation loss: 0.027478573843836784\n",
      "Epoch #246 - training loss: 0.02824737598418977 \t validation loss: 0.02752077952027321\n",
      "Epoch #247 - training loss: 0.028332574647451203 \t validation loss: 0.027460413053631783\n",
      "Epoch #248 - training loss: 0.02830134316076941 \t validation loss: 0.027490345761179924\n",
      "Epoch #249 - training loss: 0.028324390046046168 \t validation loss: 0.027527792379260063\n",
      "Epoch #250 - training loss: 0.02827307610455469 \t validation loss: 0.027477452531456947\n",
      "Epoch #251 - training loss: 0.0281797953459655 \t validation loss: 0.027472486719489098\n",
      "Epoch #252 - training loss: 0.028356326652654244 \t validation loss: 0.027466287836432457\n",
      "Epoch #253 - training loss: 0.028345243284834048 \t validation loss: 0.02750902995467186\n",
      "Best validation loss at epoch=254, saving model\n",
      "Epoch #254 - training loss: 0.028313491009541003 \t validation loss: 0.027421901002526283\n",
      "Epoch #255 - training loss: 0.02841105250767872 \t validation loss: 0.027468902990221977\n",
      "Epoch #256 - training loss: 0.02842020783689597 \t validation loss: 0.027443543076515198\n",
      "Epoch #257 - training loss: 0.028174399033872786 \t validation loss: 0.027426302433013916\n",
      "Epoch #258 - training loss: 0.028211928332148122 \t validation loss: 0.0274715106934309\n",
      "Epoch #259 - training loss: 0.028275359707507646 \t validation loss: 0.027477959170937538\n",
      "Best validation loss at epoch=260, saving model\n",
      "Epoch #260 - training loss: 0.028405567421951397 \t validation loss: 0.027419723570346832\n",
      "Epoch #261 - training loss: 0.028257756277341096 \t validation loss: 0.02743193507194519\n",
      "Best validation loss at epoch=262, saving model\n",
      "Epoch #262 - training loss: 0.02833785197519948 \t validation loss: 0.027365053072571754\n",
      "Epoch #263 - training loss: 0.028260001073526667 \t validation loss: 0.027444988489151\n",
      "Epoch #264 - training loss: 0.028304842198616204 \t validation loss: 0.027421461418271065\n",
      "Epoch #265 - training loss: 0.028303567501060573 \t validation loss: 0.027380378916859627\n",
      "Epoch #266 - training loss: 0.028293251367900087 \t validation loss: 0.027438044548034668\n",
      "Epoch #267 - training loss: 0.028331486139705476 \t validation loss: 0.027365900576114655\n",
      "Epoch #268 - training loss: 0.02824101916351646 \t validation loss: 0.027405166998505592\n",
      "Epoch #269 - training loss: 0.028273595765120153 \t validation loss: 0.02745877392590046\n",
      "Epoch #270 - training loss: 0.028246687956771908 \t validation loss: 0.027402209118008614\n",
      "Epoch #271 - training loss: 0.02813212248312439 \t validation loss: 0.027430091053247452\n",
      "Epoch #272 - training loss: 0.028213709645942782 \t validation loss: 0.027457065880298615\n",
      "Epoch #273 - training loss: 0.028255468430312985 \t validation loss: 0.027391809970140457\n",
      "Epoch #274 - training loss: 0.02827655509987484 \t validation loss: 0.027600537985563278\n",
      "Epoch #275 - training loss: 0.02823415973909372 \t validation loss: 0.027430448681116104\n",
      "Epoch #276 - training loss: 0.028320622801408835 \t validation loss: 0.027452420443296432\n",
      "Epoch #277 - training loss: 0.02824445303160956 \t validation loss: 0.027431394904851913\n",
      "Epoch #278 - training loss: 0.02822734952743253 \t validation loss: 0.027440648525953293\n",
      "Epoch #279 - training loss: 0.0282689878823312 \t validation loss: 0.02742333896458149\n",
      "Epoch #280 - training loss: 0.028130500164852475 \t validation loss: 0.027466733008623123\n",
      "Epoch #281 - training loss: 0.028274332650450727 \t validation loss: 0.027365250512957573\n",
      "Best validation loss at epoch=282, saving model\n",
      "Epoch #282 - training loss: 0.02825014574499718 \t validation loss: 0.027349604293704033\n",
      "Epoch #283 - training loss: 0.02817473208379007 \t validation loss: 0.02745579555630684\n",
      "Epoch #284 - training loss: 0.028253254928060023 \t validation loss: 0.02735823392868042\n",
      "Epoch #285 - training loss: 0.02830422185311582 \t validation loss: 0.027366001158952713\n",
      "Epoch #286 - training loss: 0.02818876031019232 \t validation loss: 0.027429649606347084\n",
      "Best validation loss at epoch=287, saving model\n",
      "Epoch #287 - training loss: 0.02822446193190793 \t validation loss: 0.027340799570083618\n",
      "Epoch #288 - training loss: 0.028142991792012184 \t validation loss: 0.02734992653131485\n",
      "Epoch #289 - training loss: 0.02825461786801457 \t validation loss: 0.027351226657629013\n",
      "Epoch #290 - training loss: 0.028207496016945378 \t validation loss: 0.02741163596510887\n",
      "Epoch #291 - training loss: 0.028311039952796994 \t validation loss: 0.027429230511188507\n",
      "Epoch #292 - training loss: 0.028373589684419772 \t validation loss: 0.027387885376811028\n",
      "Epoch #293 - training loss: 0.028256053341562577 \t validation loss: 0.02739642560482025\n",
      "Epoch #294 - training loss: 0.028378019492024033 \t validation loss: 0.027416789904236794\n",
      "Epoch #295 - training loss: 0.02819855175389445 \t validation loss: 0.02734317258000374\n",
      "Epoch #296 - training loss: 0.028220271865991665 \t validation loss: 0.027402281761169434\n",
      "Epoch #297 - training loss: 0.02814303333464339 \t validation loss: 0.027400324121117592\n",
      "Epoch #298 - training loss: 0.028250569067629794 \t validation loss: 0.027372460812330246\n",
      "Epoch #299 - training loss: 0.028119520508350534 \t validation loss: 0.027383796870708466\n",
      "Epoch #300 - training loss: 0.02811155956962317 \t validation loss: 0.027483001351356506\n",
      "Epoch #301 - training loss: 0.02822127349429005 \t validation loss: 0.027446236461400986\n",
      "Epoch #302 - training loss: 0.028169051512458267 \t validation loss: 0.027379490435123444\n",
      "Epoch #303 - training loss: 0.028110175495618644 \t validation loss: 0.027366463094949722\n",
      "Epoch #304 - training loss: 0.0281670026621431 \t validation loss: 0.02740449644625187\n",
      "Epoch #305 - training loss: 0.028352510767578814 \t validation loss: 0.027360353618860245\n",
      "Epoch #306 - training loss: 0.028194717256063446 \t validation loss: 0.02739429473876953\n",
      "Epoch #307 - training loss: 0.02826588210498718 \t validation loss: 0.02744334749877453\n",
      "Epoch #308 - training loss: 0.02833618669573105 \t validation loss: 0.027424074709415436\n",
      "Epoch #309 - training loss: 0.02819446585956237 \t validation loss: 0.02736062929034233\n",
      "Epoch #310 - training loss: 0.028321807742543822 \t validation loss: 0.02742020972073078\n",
      "Epoch #311 - training loss: 0.028267795547059407 \t validation loss: 0.027440736070275307\n",
      "Epoch #312 - training loss: 0.028250726674469238 \t validation loss: 0.02753109112381935\n",
      "Epoch #313 - training loss: 0.028208894467515847 \t validation loss: 0.027401581406593323\n",
      "Epoch #314 - training loss: 0.028224915429557976 \t validation loss: 0.0273702722042799\n",
      "Epoch #315 - training loss: 0.028302423461753736 \t validation loss: 0.02740851603448391\n",
      "Epoch #316 - training loss: 0.02826695962403959 \t validation loss: 0.027373602613806725\n",
      "Epoch #317 - training loss: 0.028282313312963315 \t validation loss: 0.027348706498742104\n",
      "Epoch #318 - training loss: 0.028170596845547624 \t validation loss: 0.027357514947652817\n",
      "Epoch #319 - training loss: 0.028343966695581316 \t validation loss: 0.027412666007876396\n",
      "Epoch #320 - training loss: 0.028218862046147306 \t validation loss: 0.02735804207623005\n",
      "Epoch #321 - training loss: 0.02830847713402381 \t validation loss: 0.027423547580838203\n",
      "Epoch #322 - training loss: 0.028287698064039257 \t validation loss: 0.027424313127994537\n",
      "Epoch #323 - training loss: 0.02823077215283525 \t validation loss: 0.027507569640874863\n",
      "Epoch #324 - training loss: 0.028285476475382976 \t validation loss: 0.027411697432398796\n",
      "Epoch #325 - training loss: 0.02824676781809771 \t validation loss: 0.02735184319317341\n",
      "Epoch #326 - training loss: 0.028245431423326705 \t validation loss: 0.027468865737318993\n",
      "Epoch #327 - training loss: 0.02827266330370902 \t validation loss: 0.02740463614463806\n",
      "Epoch #328 - training loss: 0.028404337176787475 \t validation loss: 0.027372093871235847\n",
      "Epoch #329 - training loss: 0.02820879424326188 \t validation loss: 0.02739943191409111\n",
      "Epoch #330 - training loss: 0.02820458214683541 \t validation loss: 0.027512619271874428\n",
      "Best validation loss at epoch=331, saving model\n",
      "Epoch #331 - training loss: 0.028192460086236503 \t validation loss: 0.02733643911778927\n",
      "Epoch #332 - training loss: 0.028260821307247334 \t validation loss: 0.027344686910510063\n",
      "Epoch #333 - training loss: 0.028117712208612717 \t validation loss: 0.027384227141737938\n",
      "Best validation loss at epoch=334, saving model\n",
      "Epoch #334 - training loss: 0.028123639928220163 \t validation loss: 0.027310939505696297\n",
      "Epoch #335 - training loss: 0.02820229375612077 \t validation loss: 0.027313312515616417\n",
      "Epoch #336 - training loss: 0.028121198421267795 \t validation loss: 0.027368543669581413\n",
      "Epoch #337 - training loss: 0.028296296295515676 \t validation loss: 0.027481134980916977\n",
      "Epoch #338 - training loss: 0.028143692173980137 \t validation loss: 0.02741202712059021\n",
      "Epoch #339 - training loss: 0.028127849103680377 \t validation loss: 0.02734753116965294\n",
      "Epoch #340 - training loss: 0.02821272653037825 \t validation loss: 0.027351129800081253\n",
      "Epoch #341 - training loss: 0.028192138882875178 \t validation loss: 0.027406832203269005\n",
      "Epoch #342 - training loss: 0.028224457166557748 \t validation loss: 0.02732986956834793\n",
      "Epoch #343 - training loss: 0.028280086293252127 \t validation loss: 0.02736665867269039\n",
      "Epoch #344 - training loss: 0.028169001374599555 \t validation loss: 0.027373196557164192\n",
      "Epoch #345 - training loss: 0.028257116051174952 \t validation loss: 0.027399009093642235\n",
      "Epoch #346 - training loss: 0.02816129678863627 \t validation loss: 0.027466902509331703\n",
      "Epoch #347 - training loss: 0.028299351667723023 \t validation loss: 0.027367714792490005\n",
      "Best validation loss at epoch=348, saving model\n",
      "Epoch #348 - training loss: 0.028201259780549003 \t validation loss: 0.02730880118906498\n",
      "Epoch #349 - training loss: 0.02826211103379275 \t validation loss: 0.02737421542406082\n",
      "Epoch #350 - training loss: 0.028127808248335346 \t validation loss: 0.02738320454955101\n",
      "Epoch #351 - training loss: 0.02819255033072994 \t validation loss: 0.02732102945446968\n",
      "Epoch #352 - training loss: 0.028211455999702588 \t validation loss: 0.027336640283465385\n",
      "Epoch #353 - training loss: 0.02819364532785168 \t validation loss: 0.027410047128796577\n",
      "Epoch #354 - training loss: 0.028028641279567692 \t validation loss: 0.027388745918869972\n",
      "Epoch #355 - training loss: 0.02815303689619251 \t validation loss: 0.027359263971447945\n",
      "Epoch #356 - training loss: 0.028242407284457895 \t validation loss: 0.027450231835246086\n",
      "Epoch #357 - training loss: 0.028090299018318374 \t validation loss: 0.02743837982416153\n",
      "Epoch #358 - training loss: 0.02838041452913414 \t validation loss: 0.02732432633638382\n",
      "Epoch #359 - training loss: 0.028164466113389974 \t validation loss: 0.027395006269216537\n",
      "Epoch #360 - training loss: 0.028112227816812017 \t validation loss: 0.02746412716805935\n",
      "Epoch #361 - training loss: 0.028338195197450233 \t validation loss: 0.027369461953639984\n",
      "Epoch #362 - training loss: 0.028062300497596275 \t validation loss: 0.02731928788125515\n",
      "Epoch #363 - training loss: 0.02817406992091002 \t validation loss: 0.02744407020509243\n",
      "Epoch #364 - training loss: 0.028183784513483603 \t validation loss: 0.027361959218978882\n",
      "Epoch #365 - training loss: 0.028168138971555574 \t validation loss: 0.02735081873834133\n",
      "Epoch #366 - training loss: 0.02826537043785879 \t validation loss: 0.027315765619277954\n",
      "Best validation loss at epoch=367, saving model\n",
      "Epoch #367 - training loss: 0.02815885079025213 \t validation loss: 0.027301695197820663\n",
      "Best validation loss at epoch=368, saving model\n",
      "Epoch #368 - training loss: 0.02814055806268239 \t validation loss: 0.027300668880343437\n",
      "Epoch #369 - training loss: 0.02819542156834547 \t validation loss: 0.027327878400683403\n",
      "Epoch #370 - training loss: 0.02821896941884157 \t validation loss: 0.027382098138332367\n",
      "Epoch #371 - training loss: 0.028190968430363665 \t validation loss: 0.027344245463609695\n",
      "Epoch #372 - training loss: 0.028189908830140203 \t validation loss: 0.027346009388566017\n",
      "Epoch #373 - training loss: 0.028172993080428206 \t validation loss: 0.0273378174751997\n",
      "Best validation loss at epoch=374, saving model\n",
      "Epoch #374 - training loss: 0.028159331149336966 \t validation loss: 0.027269961312413216\n",
      "Epoch #375 - training loss: 0.02811598296753677 \t validation loss: 0.027344658970832825\n",
      "Epoch #376 - training loss: 0.028254245139771338 \t validation loss: 0.02733565866947174\n",
      "Epoch #377 - training loss: 0.028137556164327896 \t validation loss: 0.02738342061638832\n",
      "Epoch #378 - training loss: 0.028196081607112802 \t validation loss: 0.027345430105924606\n",
      "Epoch #379 - training loss: 0.028105825229768712 \t validation loss: 0.02731524407863617\n",
      "Epoch #380 - training loss: 0.028170327538933188 \t validation loss: 0.02735264226794243\n",
      "Epoch #381 - training loss: 0.02814657883735211 \t validation loss: 0.027335163205862045\n",
      "Epoch #382 - training loss: 0.0282501693667243 \t validation loss: 0.02746039256453514\n",
      "Epoch #383 - training loss: 0.028313182789912038 \t validation loss: 0.02740093693137169\n",
      "Epoch #384 - training loss: 0.02816961970970486 \t validation loss: 0.027285633608698845\n",
      "Epoch #385 - training loss: 0.02824825016904536 \t validation loss: 0.027308592572808266\n",
      "Epoch #386 - training loss: 0.028144486924988648 \t validation loss: 0.027289554476737976\n",
      "Best validation loss at epoch=387, saving model\n",
      "Epoch #387 - training loss: 0.028087133562454618 \t validation loss: 0.02725660428404808\n",
      "Epoch #388 - training loss: 0.028224963306344768 \t validation loss: 0.027357181534171104\n",
      "Epoch #389 - training loss: 0.028162011753854056 \t validation loss: 0.027384137734770775\n",
      "Epoch #390 - training loss: 0.02820016706276041 \t validation loss: 0.02743503637611866\n",
      "Epoch #391 - training loss: 0.02824989423726863 \t validation loss: 0.027331698685884476\n",
      "Epoch #392 - training loss: 0.028173616904692323 \t validation loss: 0.027279837056994438\n",
      "Epoch #393 - training loss: 0.028282095150235182 \t validation loss: 0.02731546014547348\n",
      "Epoch #394 - training loss: 0.028166782521392066 \t validation loss: 0.02725956216454506\n",
      "Epoch #395 - training loss: 0.02824733374348166 \t validation loss: 0.027277367189526558\n",
      "Epoch #396 - training loss: 0.028117350779088637 \t validation loss: 0.027354054152965546\n",
      "Epoch #397 - training loss: 0.028156938637256518 \t validation loss: 0.027349064126610756\n",
      "Epoch #398 - training loss: 0.028141945520824288 \t validation loss: 0.027416253462433815\n",
      "Epoch #399 - training loss: 0.028146902251151955 \t validation loss: 0.027313832193613052\n",
      "Epoch #400 - training loss: 0.028143478878699044 \t validation loss: 0.027393313124775887\n",
      "Epoch #401 - training loss: 0.028239684292312824 \t validation loss: 0.02743537910282612\n",
      "Epoch #402 - training loss: 0.028189961817580378 \t validation loss: 0.0273137129843235\n",
      "Epoch #403 - training loss: 0.0282153938598134 \t validation loss: 0.027357403188943863\n",
      "Epoch #404 - training loss: 0.02819859797430495 \t validation loss: 0.027345897629857063\n",
      "Epoch #405 - training loss: 0.02813811308277668 \t validation loss: 0.02732972614467144\n",
      "Epoch #406 - training loss: 0.028094314257888234 \t validation loss: 0.02732033282518387\n",
      "Epoch #407 - training loss: 0.028256713333846786 \t validation loss: 0.02728170156478882\n",
      "Epoch #408 - training loss: 0.0280587552812568 \t validation loss: 0.02726673148572445\n",
      "Epoch #409 - training loss: 0.028145581104505456 \t validation loss: 0.027294663712382317\n",
      "Best validation loss at epoch=410, saving model\n",
      "Epoch #410 - training loss: 0.02808900592091648 \t validation loss: 0.02722916193306446\n",
      "Epoch #411 - training loss: 0.02828984855592862 \t validation loss: 0.027277015149593353\n",
      "Epoch #412 - training loss: 0.028146074810887083 \t validation loss: 0.02725679613649845\n",
      "Epoch #413 - training loss: 0.02830077598781164 \t validation loss: 0.02730698324739933\n",
      "Epoch #414 - training loss: 0.028203512992670033 \t validation loss: 0.027328412979841232\n",
      "Epoch #415 - training loss: 0.02816721491943945 \t validation loss: 0.027302302420139313\n",
      "Epoch #416 - training loss: 0.028086662457661567 \t validation loss: 0.027315573766827583\n",
      "Epoch #417 - training loss: 0.02813509617535374 \t validation loss: 0.027382666245102882\n",
      "Epoch #418 - training loss: 0.02806746968982904 \t validation loss: 0.0273105688393116\n",
      "Epoch #419 - training loss: 0.02825534924177483 \t validation loss: 0.027357934042811394\n",
      "Epoch #420 - training loss: 0.028151288769687668 \t validation loss: 0.027293164283037186\n",
      "Epoch #421 - training loss: 0.028095937410366194 \t validation loss: 0.02733551524579525\n",
      "Epoch #422 - training loss: 0.028227215216938346 \t validation loss: 0.027332156896591187\n",
      "Epoch #423 - training loss: 0.028154684061353855 \t validation loss: 0.027290234342217445\n",
      "Epoch #424 - training loss: 0.028183943376197427 \t validation loss: 0.027348771691322327\n",
      "Epoch #425 - training loss: 0.028154450622279323 \t validation loss: 0.027341701090335846\n",
      "Epoch #426 - training loss: 0.02818521238621092 \t validation loss: 0.027327101677656174\n",
      "Epoch #427 - training loss: 0.028109440580010414 \t validation loss: 0.02733730711042881\n",
      "Epoch #428 - training loss: 0.02812868801946627 \t validation loss: 0.027327150106430054\n",
      "Epoch #429 - training loss: 0.028287026640226975 \t validation loss: 0.027313530445098877\n",
      "Epoch #430 - training loss: 0.028079205616784023 \t validation loss: 0.027298850938677788\n",
      "Epoch #431 - training loss: 0.028277186180486504 \t validation loss: 0.027288228273391724\n",
      "Epoch #432 - training loss: 0.028137012551673116 \t validation loss: 0.027282249182462692\n",
      "Epoch #433 - training loss: 0.028057817556462028 \t validation loss: 0.02727820724248886\n",
      "Epoch #434 - training loss: 0.028085563582519266 \t validation loss: 0.02734612487256527\n",
      "Epoch #435 - training loss: 0.028078931042635664 \t validation loss: 0.02729525789618492\n",
      "Epoch #436 - training loss: 0.028127473637913212 \t validation loss: 0.027346039190888405\n",
      "Epoch #437 - training loss: 0.028261504403303862 \t validation loss: 0.02731691114604473\n",
      "Epoch #438 - training loss: 0.028122870419207635 \t validation loss: 0.027332527562975883\n",
      "Epoch #439 - training loss: 0.028105703360980474 \t validation loss: 0.027303416281938553\n",
      "Best validation loss at epoch=440, saving model\n",
      "Epoch #440 - training loss: 0.02811899940182792 \t validation loss: 0.027217814698815346\n",
      "Epoch #441 - training loss: 0.028190345387182028 \t validation loss: 0.02727925218641758\n",
      "Epoch #442 - training loss: 0.028108643997898874 \t validation loss: 0.027263104915618896\n",
      "Epoch #443 - training loss: 0.02828762093193683 \t validation loss: 0.02729082480072975\n",
      "Epoch #444 - training loss: 0.028267602933447895 \t validation loss: 0.027334295213222504\n",
      "Epoch #445 - training loss: 0.02804809740085467 \t validation loss: 0.027383485808968544\n",
      "Epoch #446 - training loss: 0.02832209340706733 \t validation loss: 0.02735007181763649\n",
      "Epoch #447 - training loss: 0.028093906249784584 \t validation loss: 0.0272512324154377\n",
      "Epoch #448 - training loss: 0.028115077681321512 \t validation loss: 0.027275580912828445\n",
      "Epoch #449 - training loss: 0.028235286282485023 \t validation loss: 0.02729182504117489\n",
      "Epoch #450 - training loss: 0.02812598327194704 \t validation loss: 0.027309656143188477\n",
      "Epoch #451 - training loss: 0.02814081831753865 \t validation loss: 0.027289455756545067\n",
      "Epoch #452 - training loss: 0.028135299534037206 \t validation loss: 0.02725924365222454\n",
      "Epoch #453 - training loss: 0.028178741447395128 \t validation loss: 0.02731853537261486\n",
      "Epoch #454 - training loss: 0.028257888667916283 \t validation loss: 0.027222435921430588\n",
      "Epoch #455 - training loss: 0.028262340984972933 \t validation loss: 0.027289271354675293\n",
      "Epoch #456 - training loss: 0.0281884537581454 \t validation loss: 0.02722635120153427\n",
      "Epoch #457 - training loss: 0.028104310187074537 \t validation loss: 0.027268296107649803\n",
      "Epoch #458 - training loss: 0.028170704212431494 \t validation loss: 0.027342837303876877\n",
      "Epoch #459 - training loss: 0.028215810676053856 \t validation loss: 0.027234695851802826\n",
      "Epoch #460 - training loss: 0.02805904282917685 \t validation loss: 0.02723359875380993\n",
      "Epoch #461 - training loss: 0.0282043796729914 \t validation loss: 0.027246534824371338\n",
      "Epoch #462 - training loss: 0.028222789972565682 \t validation loss: 0.02726772055029869\n",
      "Epoch #463 - training loss: 0.028019408811192543 \t validation loss: 0.02727402187883854\n",
      "Epoch #464 - training loss: 0.028114911897602672 \t validation loss: 0.027242334559559822\n",
      "Epoch #465 - training loss: 0.02811854739246787 \t validation loss: 0.027278661727905273\n",
      "Epoch #466 - training loss: 0.028233781393033033 \t validation loss: 0.027269111946225166\n",
      "Epoch #467 - training loss: 0.028024131577435395 \t validation loss: 0.027261678129434586\n",
      "Epoch #468 - training loss: 0.02811205846219325 \t validation loss: 0.02727610431611538\n",
      "Epoch #469 - training loss: 0.02812597420275769 \t validation loss: 0.027339961379766464\n",
      "Epoch #470 - training loss: 0.028202203120671065 \t validation loss: 0.027249813079833984\n",
      "Epoch #471 - training loss: 0.02821807393973524 \t validation loss: 0.02736983262002468\n",
      "Epoch #472 - training loss: 0.028113846253743146 \t validation loss: 0.02722117304801941\n",
      "Epoch #473 - training loss: 0.027954935223202814 \t validation loss: 0.027263158932328224\n",
      "Epoch #474 - training loss: 0.028111196168716632 \t validation loss: 0.02722606621682644\n",
      "Epoch #475 - training loss: 0.02820472356758733 \t validation loss: 0.02731839008629322\n",
      "Epoch #476 - training loss: 0.028099715525413897 \t validation loss: 0.027257248759269714\n",
      "Epoch #477 - training loss: 0.028210293750632255 \t validation loss: 0.027306433767080307\n",
      "Epoch #478 - training loss: 0.02817022545867452 \t validation loss: 0.027226625010371208\n",
      "Epoch #479 - training loss: 0.028113553855807227 \t validation loss: 0.02721802145242691\n",
      "Epoch #480 - training loss: 0.028266756835189415 \t validation loss: 0.027348093688488007\n",
      "Epoch #481 - training loss: 0.028307232910305687 \t validation loss: 0.02733810804784298\n",
      "Best validation loss at epoch=482, saving model\n",
      "Epoch #482 - training loss: 0.028139744403833482 \t validation loss: 0.0272146575152874\n",
      "Epoch #483 - training loss: 0.028145653514420603 \t validation loss: 0.02724144048988819\n",
      "Epoch #484 - training loss: 0.028140441581786156 \t validation loss: 0.027223117649555206\n",
      "Epoch #485 - training loss: 0.028115706316588874 \t validation loss: 0.027260977774858475\n",
      "Epoch #486 - training loss: 0.02816196370856594 \t validation loss: 0.02727370709180832\n",
      "Epoch #487 - training loss: 0.028171880840557943 \t validation loss: 0.0272897370159626\n",
      "Epoch #488 - training loss: 0.028268287262137242 \t validation loss: 0.027293767780065536\n",
      "Epoch #489 - training loss: 0.02817700794979319 \t validation loss: 0.027215881273150444\n",
      "Epoch #490 - training loss: 0.028097378252714925 \t validation loss: 0.027251895517110825\n",
      "Epoch #491 - training loss: 0.02806246721844808 \t validation loss: 0.027229759842157364\n",
      "Epoch #492 - training loss: 0.028173342351295358 \t validation loss: 0.027271486818790436\n",
      "Best validation loss at epoch=493, saving model\n",
      "Epoch #493 - training loss: 0.028139038961845553 \t validation loss: 0.027180610224604607\n",
      "Epoch #494 - training loss: 0.028171125964598594 \t validation loss: 0.027260247617959976\n",
      "Epoch #495 - training loss: 0.028162991859636364 \t validation loss: 0.027229182422161102\n",
      "Epoch #496 - training loss: 0.028138530509523158 \t validation loss: 0.027216220274567604\n",
      "Epoch #497 - training loss: 0.028118180560940835 \t validation loss: 0.02725239470601082\n",
      "Epoch #498 - training loss: 0.028105267823247137 \t validation loss: 0.027272945269942284\n",
      "Epoch #499 - training loss: 0.02800047007155291 \t validation loss: 0.027204543352127075\n",
      "Epoch #500 - training loss: 0.028145151626179244 \t validation loss: 0.027268828824162483\n",
      "         productid  predictedyQuantity\n",
      "0    410318185_red            3.661353\n",
      "1  410331269_green            1.155787\n",
      "2  420000057_white            3.357788\n",
      "3  420000109_white            2.301638\n",
      "4   420280962_navy           14.885006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38732/97459344.py:31: FutureWarning: Passing 'use_legacy_dataset=True' to get the legacy behaviour is deprecated as of pyarrow 8.0.0, and the legacy implementation will be removed in a future version.\n",
      "  pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=f\"{run_dir}/experiments/exp_neural_network/predictions/ModelForEachBrickProductLevel/{of}/predictions\", filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)\n"
     ]
    }
   ],
   "source": [
    "experiment(\"womenKurtas\", epochs = 500, lr = 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with 0-4 months sales, wishlist, availableQuantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/data/Archive/bhavesh/inventoryPrediction/temp/local\"\n",
    "train_path = {\n",
    "    \"menShirts\": f\"{base_dir}/menShirts/data/train\",\n",
    "    \"womenKurtas\": f\"{base_dir}/womenKurtas/data/train\",\n",
    "}\n",
    "test_path = {\n",
    "    \"menShirts\": f\"{base_dir}/menShirts/data/test\",\n",
    "    \"womenKurtas\": f\"{base_dir}/womenKurtas/data/test\",\n",
    "}\n",
    "\n",
    "def fetch_data2(of):\n",
    "    cols_to_read = [f\"{i}_sales\" for i in range(4)] + [f\"{i}_wishlist\" for i in range(4)] + [f\"{i}_availableQuantity\" for i in range(4)]\n",
    "    cols_to_read = [\"productid\", \"similargrouplevel\"] + cols_to_read\n",
    "    train = pq.ParquetDataset(train_path[of], fs).read(columns=cols_to_read + [\"yQuantity\"]).to_pandas()\n",
    "    test = pq.ParquetDataset(test_path[of], fs).read(columns=cols_to_read).to_pandas()\n",
    "\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    train.info()\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment2(of, epochs = 200, lr = 0.001):\n",
    "    train, test = fetch_data2(of)\n",
    "\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    cols = train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]).columns\n",
    "    print(cols)\n",
    "    train.loc[:, cols] = scaler_x.fit_transform(train[cols])\n",
    "    train[\"yQuantity\"] = scaler_y.fit_transform(train[[\"yQuantity\"]])\n",
    "    print(scaler_y.data_min_, scaler_y.data_max_)\n",
    "\n",
    "    train, val = train_val_split(train)\n",
    "    print(train.shape, val.shape)\n",
    "\n",
    "    x_train, y_train = train.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), train[\"yQuantity\"]\n",
    "    x_val, y_val = val.drop(columns=[\"productid\", \"similargrouplevel\", \"yQuantity\"]), val[\"yQuantity\"]\n",
    "\n",
    "    neural_network = Model(NeuralNetwork, {\"input_size\": 12, \"output_size\": 1, \"hidden_size1\": 24, \"hidden_size2\": 6})\n",
    "    print(neural_network.device)\n",
    "    neural_network.compile(nn.BCELoss(), optim.Adam, lr)\n",
    "    neural_network.fit(x_train.values, y_train.values, epochs, x_val=x_val.values, y_val=y_val.values)\n",
    "\n",
    "    x_test = test.drop(columns=[\"productid\", \"similargrouplevel\"])\n",
    "    x_test.loc[:, cols] = scaler_x.fit_transform(x_test[cols])\n",
    "    y_pred = neural_network.predict(x_test.values).reshape(-1)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    predictions = pd.DataFrame({\"productid\": test[\"productid\"], \"predictedyQuantity\": y_pred})\n",
    "    print(predictions.head())\n",
    "\n",
    "    pq.write_to_dataset(table=pa.Table.from_pandas(predictions), root_path=f\"{base_dir}/experiments/exp_neural_network_new/predictions/ModelForEachBrickProductLevel/{of}/predictions\", filesystem=fs, compression=\"snappy\", use_legacy_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2(\"menShirts\", epochs = 200, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2(\"womenKurtas\", epochs = 200, lr = 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_env",
   "language": "python",
   "name": "api_env"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
