{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "fs = pa.hdfs.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(embedding_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 15)\n",
    "        self.fc3 = nn.Linear(15, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x)\n",
    "        x = self.flatten(x_embed)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x_embed, x\n",
    "\n",
    "\n",
    "def calculate_accuracy(predictions, labels):\n",
    "    _, predicted_classes = torch.max(predictions, 1)\n",
    "    correct_predictions = (predicted_classes == labels).sum().item()\n",
    "    total_predictions = labels.size(0)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def calculate_f1_score(predictions, labels):\n",
    "    _, predicted_classes = torch.max(predictions, 1)\n",
    "    predicted_classes = predicted_classes.numpy()\n",
    "    labels = labels.numpy()\n",
    "    \n",
    "    f1 = f1_score(labels, predicted_classes, average='macro')  # Calculate macro-averaged F1 score\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def calculate_mae(predictions, labels):\n",
    "    loss_function = nn.L1Loss()\n",
    "    loss = loss_function(predictions, labels)\n",
    "    return loss.item() \n",
    "\n",
    "def calculate_error(predictions, labels):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = loss_function(predictions, labels)\n",
    "    return loss.item()    \n",
    "    \n",
    "def train_model_for_embedding(epochs, input_dimension, embedding_size, x_data, y_data, x_val, y_val):\n",
    "    \n",
    "    model = MyModel(input_dim=input_dimension, embedding_size=embedding_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    x_data = torch.tensor(x_data, dtype=torch.long)\n",
    "    y_data = torch.tensor(y_data, dtype=torch.float).unsqueeze(1)\n",
    "    x_val = torch.tensor(x_val, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        optimizer.zero_grad()\n",
    "        embed, outputs = model(x_data)\n",
    "        loss = criterion(outputs, y_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_accuracy = calculate_mae(outputs, y_data)  # Calculate training accuracy\n",
    "        train_error = loss.item()  # Training error\n",
    "\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            _, val_outputs = model(x_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "            val_accuracy = calculate_mae(val_outputs, y_val)  # Calculate validation accuracy\n",
    "            val_error = val_loss.item()  # Validation error\n",
    "\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Train mae: {:.4f}, Val Loss: {:.4f}, Val mae: {:.4f}'\n",
    "              .format(epoch+1, epochs, train_error, train_accuracy, val_loss, val_accuracy))\n",
    "\n",
    "    return model, embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pq.ParquetDataset(\"/data/Archive/bhavesh/inventoryPrediction/embeddings/data\", fs).read().to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"similargrouplevel\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menShirts = df[df[\"similargrouplevel\"] == \"830216013\"].copy().drop(columns=[\"similargrouplevel\"]).fillna(\"Null\")\n",
    "womenKurtas = df[df[\"similargrouplevel\"] == \"830303011\"].copy().drop(columns=[\"similargrouplevel\"]).fillna(\"Null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(df, attribute, proxy_name, embedding_size = 2, epochs=500):\n",
    "\n",
    "    y_data = df[\"yQuantity\"].values\n",
    "    y_data = (y_data - y_data.min())/(y_data.max() - y_data.min())\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    x_data = label_encoder.fit_transform(df[attribute])\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "    input_dimension = len(set(x_data))\n",
    "    \n",
    "    model, embed_tensor = train_model_for_embedding(epochs, input_dimension, embedding_size, x_train, y_train, x_val, y_val)\n",
    "    embed = embed_tensor.detach().numpy()\n",
    "\n",
    "    embed_df = pd.DataFrame(embed, columns=[proxy_name + \"_\" + str(i) for i in range(embedding_size)]).reset_index()\n",
    "    pid_df = pd.DataFrame(x_train, columns=['value']).reset_index()\n",
    "    attr_df = pd.merge(pid_df, embed_df, on=['index'], how='inner').drop(\"index\", axis=1)\n",
    "    attr_df[attribute] = label_encoder.inverse_transform(attr_df[\"value\"])\n",
    "    # Find magnitude, i.e, sqrt(x_1^2 + x_2^2, x_3^2, ..., x_embedding_size^2)\n",
    "    attr_df[\"magnitude\"] = np.sqrt(np.sum(np.square(attr_df.drop(columns=[attribute, \"value\"])), axis=1))\n",
    "    for column in attr_df.drop(columns=[attribute, \"value\", \"magnitude\"]).columns:\n",
    "        attr_df[f\"normalized_{column}\"] = attr_df[column] / attr_df[\"magnitude\"]\n",
    "\n",
    "    return attr_df.drop_duplicates().reset_index(drop=True).drop(columns=[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_proxy = {\n",
    "    \"colorfamily\": \"color\",\n",
    "    \"brandname\": \"brand\",\n",
    "    \"styletype\": \"style\",\n",
    "    \"pattern\": \"pattern\",\n",
    "    \"sleeve\": \"sleeve\"\n",
    "}.items()\n",
    "attr_proxy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Men shirts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr, proxy in attr_proxy:\n",
    "    attr_ms = generate_embeddings(menShirts, attr, proxy, epochs=10)\n",
    "    if not os.path.exists(\"/app/data/bhavesh/inventoryPrediction/embeddings/menShirts\"):\n",
    "        os.makedirs(\"/app/data/bhavesh/inventoryPrediction/embeddings/menShirts\")\n",
    "    attr_ms.to_csv(f\"/app/data/bhavesh/inventoryPrediction/embeddings/menShirts/{proxy}.csv\", index=False)\n",
    "    with open(f\"/app/data/bhavesh/inventoryPrediction/embeddings/menShirts/{proxy}.csv\", \"rb\") as f:\n",
    "        pa.hdfs.HadoopFileSystem.upload(fs, f\"/data/Archive/bhavesh/inventoryPrediction/embeddings/menShirts/{attr}.csv\", f)\n",
    "    os.remove(f\"/app/data/bhavesh/inventoryPrediction/embeddings/menShirts/{proxy}.csv\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Women Kurtas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr, proxy in attr_proxy:\n",
    "    attr_wk = generate_embeddings(womenKurtas, attr, proxy, epochs=500)\n",
    "    if not os.path.exists(\"/app/data/bhavesh/inventoryPrediction/embeddings/womenKurtas\"):\n",
    "        os.makedirs(\"/app/data/bhavesh/inventoryPrediction/embeddings/womenKurtas\")\n",
    "    attr_wk.to_csv(f\"/app/data/bhavesh/inventoryPrediction/embeddings/womenKurtas/{proxy}.csv\", index=False)\n",
    "    with open(f\"/app/data/bhavesh/inventoryPrediction/embeddings/womenKurtas/{proxy}.csv\", \"rb\") as f:\n",
    "        pa.hdfs.HadoopFileSystem.upload(fs, f\"/data/Archive/bhavesh/inventoryPrediction/embeddings/womenKurtas/{attr}.csv\", f)\n",
    "    os.remove(f\"/app/data/bhavesh/inventoryPrediction/embeddings/womenKurtas/{proxy}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_env",
   "language": "python",
   "name": "api_env"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
