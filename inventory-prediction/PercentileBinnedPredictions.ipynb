{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f \n",
    "{\n",
    "    \"conf\":\n",
    "    {\n",
    "        \"spark.app.name\":\"Bhavesh_Relevancy_notebook\",\n",
    "        \"spark.yarn.queue\": \"default\",\n",
    "        \"spark.jars\": \"/apps/Jars/obelisk-retail-legos.jar,/apps/Jars/mysql-connector-java-5.1.40.jar\",\n",
    "        \"spark.driver.extraJavaOptions\": \"-Dscala.color\",\n",
    "        \"spark.driver.memory\": \"2g\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.instances\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.io._\n",
    "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.implicits._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package ai.couture.obelisk.retail.legos.inventoryprediction\n",
    "\n",
    "import ai.couture.obelisk.commons.utils.BaseBlocks\n",
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.io._\n",
    "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "object PercentileBinnedComputation extends BaseBlocks {\n",
    "\n",
    "  var getDaysInBetween: UserDefinedFunction = udf((startDate: String, endDate: String) => {\n",
    "    getNumberOfDaysBetweenTwoDates(startDate, endDate, \"yyyy-MM-dd\")\n",
    "  })\n",
    "\n",
    "  override def load(): Unit = {\n",
    "\n",
    "  }\n",
    "\n",
    "  override def doTransformations(): Unit = {\n",
    "      \n",
    "      var interactionsDB = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    interactionsDB = (\n",
    "        interactionsDB.filter(col(DATE).between(\"2022-03-13\", \"2023-03-13\") && col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "                              && col(USERID).isNotNull && col(QUANTITY) > 0 && col(\"booked_rev\") > 0 && col(PRODUCTID).isNotNull\n",
    "                             )\n",
    "        .groupBy(PRODUCTID, DATE)\n",
    "        .agg(sum(QUANTITY) as \"sales\")\n",
    "        .transform(saveNLoadDF(\"salesDayWise\"))\n",
    "    )\n",
    "\n",
    "    var trimmedProducts = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/TransformedDataset/date_when_prediction_is_made=2023-04-16/combined/data/test\")\n",
    "      .select(PRODUCTID).distinct()\n",
    "\n",
    "    val distinctDates: DataFrame = interactionsDB.select(DATE).distinct()\n",
    "    var operationDF = trimmedProducts.crossJoin(broadcast(distinctDates))\n",
    "      .join(interactionsDB, Seq(PRODUCTID, DATE), \"left\")\n",
    "      .na.fill(0)\n",
    "      .withColumn(\"sales\", col(\"sales\").cast(DoubleType))\n",
    "      .transform(saveNLoadDF(\"trimmed/salesCompleteTrain\"))\n",
    "      .withColumn(\"daysInHistory\", getDaysInBetween(col(DATE), lit(\"2023-03-15\")) - lit(1))\n",
    "      .withColumn(\"monthsInHistory\", floor(col(\"daysInHistory\") / 30) + 1)\n",
    "      .filter(col(\"monthsInHistory\") <= 12)\n",
    "      .transform(saveNLoadDF(\"trimmed/operationDF\"))\n",
    "\n",
    "    var percentileLimits = operationDF.groupBy(PRODUCTID).agg(\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.1)).alias(\"percentile_10\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.2)).alias(\"percentile_20\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.3)).alias(\"percentile_30\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.4)).alias(\"percentile_40\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.5)).alias(\"percentile_50\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.6)).alias(\"percentile_60\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.7)).alias(\"percentile_70\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.8)).alias(\"percentile_80\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(0.9)).alias(\"percentile_90\"),\n",
    "      callUDF(\"percentile_approx\", col(\"sales\"), lit(1.0)).alias(\"percentile_100\")\n",
    "    ).transform(saveNLoadDF(\"trimmed/productPercentiles\"))\n",
    "\n",
    "    var X = operationDF.join(percentileLimits, Seq(PRODUCTID)).withColumn(\n",
    "      \"bin\",\n",
    "      when(col(\"sales\") <= col(\"percentile_20\"), lit(1)).otherwise(\n",
    "        when(col(\"sales\") <= col(\"percentile_40\"), lit(2)).otherwise(\n",
    "          when(col(\"sales\") <= col(\"percentile_60\"), lit(3)).otherwise(\n",
    "            when(col(\"sales\") <= col(\"percentile_80\"), lit(4)).otherwise(\n",
    "              lit(5)\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    ).select(PRODUCTID, \"monthsInHistory\", \"bin\", \"daysInHistory\")\n",
    "      .transform(saveNLoadDF(\"trimmed/operationDFBinned\"))\n",
    "    X = X.groupBy(PRODUCTID, \"monthsInHistory\", \"bin\")\n",
    "      .agg(count(\"daysInHistory\") as \"count\")\n",
    "      .groupBy(PRODUCTID, \"bin\")\n",
    "      .agg(sum(\"count\").cast(DoubleType) / lit(12.0) as \"count\")\n",
    "      .transform(saveNLoadDF(\"trimmed/featureStats\"))\n",
    "\n",
    "    var y = (\n",
    "      ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "        .filter(col(DATE).between(\"2023-03-15\", \"2023-04-14\") && col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\"\n",
    "          && col(USERID).isNotNull && col(QUANTITY) > 0 && col(\"booked_rev\") > 0 && col(PRODUCTID).isNotNull\n",
    "        )\n",
    "        .groupBy(PRODUCTID, DATE)\n",
    "        .agg(sum(QUANTITY).cast(DoubleType) as \"sales\")\n",
    "        .transform(saveNLoadDF(\"salesDayWiseTarget\"))\n",
    "      )\n",
    "\n",
    "    var distinctDatesTarget = y.select(DATE).distinct()\n",
    "    var distinctCombinationsTarget = trimmedProducts.crossJoin(broadcast(distinctDatesTarget))\n",
    "    y = distinctCombinationsTarget.join(y, Seq(PRODUCTID, DATE), \"left\")\n",
    "      .na.fill(0)\n",
    "      .transform(saveNLoadDF(\"trimmed/operationDFTarget\"))\n",
    "    y = (\n",
    "      y.join(percentileLimits, Seq(PRODUCTID))\n",
    "        .withColumn(\"bin\",\n",
    "          when(col(\"sales\") <= col(\"percentile_20\"), lit(1)).otherwise(\n",
    "            when(col(\"sales\") <= col(\"percentile_40\"), lit(2)).otherwise(\n",
    "              when(col(\"sales\") <= col(\"percentile_60\"), lit(3)).otherwise(\n",
    "                when(col(\"sales\") <= col(\"percentile_80\"), lit(4)).otherwise(\n",
    "                  lit(5)\n",
    "                )\n",
    "              )\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "        .select(PRODUCTID, \"bin\", DATE)\n",
    "        .repartition(210)\n",
    "        .transform(saveNLoadDF(\"trimmed/operationDFTargetBinned\"))\n",
    "        .groupBy(PRODUCTID, \"bin\")\n",
    "        .agg(count(DATE) as \"count\")\n",
    "        .transform(saveNLoadDF(\"trimmed/targetStats\"))\n",
    "      )\n",
    "\n",
    "    val binColumns = Array(\"percentile_10\", \"percentile_30\", \"percentile_50\", \"percentile_70\", \"percentile_90\")\n",
    "\n",
    "    // Use the stack function to stack the columns into key-value pairs\n",
    "    val stackedDF = percentileLimits.select(col(\"productid\"),\n",
    "      expr(s\"stack(${binColumns.length}, ${binColumns.map(c => s\"'$c', $c\").mkString(\", \")}) as (bin, value)\"))\n",
    "\n",
    "    var percentileLimitsBinned = stackedDF.withColumn(\"bin\", substring(col(\"bin\"), 12, 2).cast(IntegerType))\n",
    "      .withColumn(\"bin\", (col(\"bin\") + lit(10)) / lit(20))\n",
    "      .transform(saveNLoadDF(\"trimmed/percentileLimitsBinned\"))\n",
    "      \n",
    "    var test = X.toDF(PRODUCTID, \"bin\", \"count_x\").join(y.toDF(PRODUCTID, \"bin\", \"count_y\"), Seq(PRODUCTID, \"bin\"), \"outer\").na.fill(0.0).withColumn(\"count\", (col(\"count_x\") + col(\"count_y\"))/lit(2.0)).drop(\"count_x\", \"count_y\")\n",
    "    test = test.transform(saveNLoadDF(\"trimmed/test\"))\n",
    "    test = test.join(percentileLimitsBinned, Seq(PRODUCTID, \"bin\")).withColumn(\"predictedyQuantity\", col(\"count\") * col(\"value\")).groupBy(PRODUCTID).agg(sum(\"predictedyQuantity\") as \"predictedyQuantity\").transform(saveNLoadDF(\"trimmed/predictions\"))\n",
    "      \n",
    "      // Later split predictions into menShirts/womenKurtas according to productAttrs\n",
    "  }\n",
    "\n",
    "  def saveNLoadDF(key: String)(df: DataFrame): DataFrame = {\n",
    "    var tempPath = \"/data/Archive/bhavesh/inventoryPrediction/temp/exp_files\"\n",
    "    DFToParquet.putDF(tempPath + \"/\" + key, df)\n",
    "    ParquetToDF.getDF(tempPath + \"/\" + key)\n",
    "  }\n",
    "\n",
    "  override def save(): Unit = {\n",
    "\n",
    "  }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitwise Code (There are some errors in this, look out for them by comparing with above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var getDaysInBetween: UserDefinedFunction = udf((startDate: String, endDate: String) => {\n",
    "    getNumberOfDaysBetweenTwoDates(startDate, endDate, \"yyyy-MM-dd\")\n",
    "})\n",
    "\n",
    "def saveNLoadDF(key: String)(df: DataFrame): DataFrame = {\n",
    "    var tempPath = \"/data/Archive/bhavesh/inventoryPrediction/temp/exp_files\"\n",
    "    DFToParquet.putDF(tempPath + \"/\" + key, df)\n",
    "    ParquetToDF.getDF(tempPath + \"/\" + key)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var interactionsDB = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "interactionsDB = (\n",
    "    interactionsDB.filter(col(DATE).between(\"2022-03-13\", \"2023-03-13\") && col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "                          && col(USERID).isNotNull && col(QUANTITY) > 0 && col(\"booked_rev\") > 0 && col(PRODUCTID).isNotNull\n",
    "                         )\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(sum(QUANTITY) as \"sales\")\n",
    "    .transform(saveNLoadDF(\"salesDayWise\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var distinctDates = interactionsDB.select(DATE).distinct()\n",
    "var distinctProducts = interactionsDB.select(PRODUCTID).distinct()\n",
    "var distinctCombinations = distinctProducts.crossJoin(broadcast(distinctDates))\n",
    "\n",
    "var operationDF = (\n",
    "    distinctCombinations.join(interactionsDB, Seq(PRODUCTID, DATE), \"left\")\n",
    "    .na.fill(0)\n",
    "    .withColumn(\"sales\", col(\"sales\").cast(DoubleType))\n",
    "    .transform(saveNLoadDF(\"salesCompleteTrain\"))\n",
    "    .withColumn(\"daysInHistory\", getDaysInBetween(col(DATE), lit(\"2023-03-15\")) - lit(1))\n",
    "    .withColumn(\"monthsInHistory\", floor(col(\"daysInHistory\")/30) + 1)\n",
    "    .filter(col(\"monthsInHistory\") <= 12)\n",
    "    .repartition(210)\n",
    "    .transform(saveNLoadDF(\"operationDF\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var percentileLimits = operationDF.groupBy(PRODUCTID).agg(\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.1)).alias(\"percentile_10\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.2)).alias(\"percentile_20\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.3)).alias(\"percentile_30\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.4)).alias(\"percentile_40\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.5)).alias(\"percentile_50\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.6)).alias(\"percentile_60\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.7)).alias(\"percentile_70\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.8)).alias(\"percentile_80\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(0.9)).alias(\"percentile_90\"),\n",
    "    callUDF(\"percentile_approx\", col(\"sales\"), lit(1.0)).alias(\"percentile_100\")\n",
    ").transform(saveNLoadDF(\"productPercentiles\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val binColumns = Array(\"percentile_10\", \"percentile_30\", \"percentile_50\", \"percentile_70\", \"percentile_90\")\n",
    "\n",
    "// Use the stack function to stack the columns into key-value pairs\n",
    "val stackedDF = percentileLimits.select($\"productid\", expr(s\"stack(${binColumns.length}, ${binColumns.map(c => s\"'$c', $c\").mkString(\", \")})\").as(\"bin\", \"value\"))\n",
    "\n",
    "var percentileLimitsBinned = stackedDF.withColumn(\"bin\", substring(col(\"bin\"), 12, 2).cast(IntegerType)).withColumn((col(\"bin\") + lit(10))/lit(20)).transform(saveNLoadDF(\"percentileLimitsBinned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    var X = operationDF.join(percentileLimits, Seq(PRODUCTID)).withColumn(\n",
    "      \"bin\",\n",
    "      when(col(\"sales\") <= col(\"percentile_20\"), lit(1)).otherwise(\n",
    "        when(col(\"sales\") <= col(\"percentile_40\"), lit(2)).otherwise(\n",
    "          when(col(\"sales\") <= col(\"percentile_60\"), lit(3)).otherwise(\n",
    "            when(col(\"sales\") <= col(\"percentile_80\"), lit(4)).otherwise(\n",
    "              lit(5)\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    ).select(PRODUCTID, \"monthsInHistory\", \"bin\", \"daysInHistory\").transform(saveNLoadDF(\"operationDFBinned\"))\n",
    "X = X.groupBy(PRODUCTID, \"monthsInHistory\", \"bin\").agg(count(\"daysInHistory\") as \"count\").groupBy(PRODUCTID, \"bin\").agg(avg(\"count\") as \"count\").transform(saveNLoadDF(\"featureStats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var y = (\n",
    "    ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    .filter(col(DATE).between(\"2023-03-15\", \"2023-04-14\") && col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "                          && col(USERID).isNotNull && col(QUANTITY) > 0 && col(\"booked_rev\") > 0 && col(PRODUCTID).isNotNull\n",
    "                         )\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(sum(QUANTITY).cast(DoubleType) as \"sales\")\n",
    "    .transform(saveNLoadDF(\"salesDayWiseTarget\"))\n",
    ")\n",
    "\n",
    "var distinctDatesTarget = y.select(DATE).distinct()\n",
    "var distinctProductsTarget = y.select(PRODUCTID).distinct()\n",
    "var distinctCombinationsTarget = distinctProductsTarget.crossJoin(broadcast(distinctDatesTarget))\n",
    "y = distinctCombinationsTarget.join(y, Seq(PRODUCTID, DATE), \"left\").na.fill(0).transform(saveNLoadDF(\"operationDFTarget\"))\n",
    "y = (\n",
    "    y.join(percetileLimits, Seq(PRODUCTID))\n",
    "    .withColumn(\"bin\",\n",
    "                when(col(\"sales\")<=col(\"percentile_20\", lit(1))).otherwise(\n",
    "                    when(col(\"sales\")<=col(\"percentile_40\"), lit(2)).otherwise(\n",
    "                        when(col(\"sales\")<=col(\"percentile_60\"), lit(3)).otherwise(\n",
    "                            when(\"sales\")<=col(\"percentile_80\", lit(4)).otherwise(\n",
    "                                lit(5)\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    .select(PRODUCTID, \"bin\", DATE)\n",
    "    .repartition(210)\n",
    "    .transform(saveNLoadDF(\"operationDFTargetBinned\"))\n",
    "    .groupBy(PRODUCTID, \"bin\")\n",
    "    .agg(count(DATE) as \"count\")\n",
    "    .transform(saveNLoadDF(\"targetStats\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var x = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/temp/exp_files/trimmed/featureStats\").withColumn(\"count\", col(\"count\").cast(DoubleType))\n",
    "var y = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/temp/exp_files/trimmed/targetStats\").withColumn(\"count\", col(\"count\").cast(DoubleType))\n",
    "x.printSchema\n",
    "y.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var test = x.toDF(PRODUCTID, \"bin\", \"count_x\").join(y.toDF(PRODUCTID, \"bin\", \"count_y\"), Seq(PRODUCTID, \"bin\"), \"outer\").na.fill(0.0).withColumn(\"count\", (col(\"count_x\") + col(\"count_y\"))/lit(2.0)).drop(\"count_x\", \"count_y\")\n",
    "test = test.transform(saveNLoadDF(\"trimmed/test\"))\n",
    "test.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var percentileLimitsBinned = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction/temp/exp_files/trimmed/percentileLimitsBinned\")\n",
    "percentileLimitsBinned.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.join(percentileLimitsBinned, Seq(PRODUCTID, \"bin\")).withColumn(\"predictedyQuantity\", col(\"count\") * col(\"value\")).groupBy(PRODUCTID).agg(sum(\"predictedyQuantity\") as \"predictedyQuantity\").transform(saveNLoadDF(\"trimmed/predictions\"))\n",
    "test.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var productAttrs = ParquetToDF.getDF(\"/data/Archive/inventory/productAttributesLegosFNL\").select(PRODUCTID, SIMILAR_GROUP_LEVEL)\n",
    "productAttrs.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.join(productAttrs, Seq(PRODUCTID))\n",
    "DFToParquet.putDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_my_exp/predictions/ModelForEachBrickProductLevel/menShirts/predictions\", test.filter(col(SIMILAR_GROUP_LEVEL) === \"830216013\").drop(SIMILAR_GROUP_LEVEL))\n",
    "DFToParquet.putDF(\"/data/Archive/bhavesh/inventoryPrediction/experiments/exp_my_exp/predictions/ModelForEachBrickProductLevel/womenKurtas/predictions\", test.filter(col(SIMILAR_GROUP_LEVEL) === \"830303011\").drop(SIMILAR_GROUP_LEVEL))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
