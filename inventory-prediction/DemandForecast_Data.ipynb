{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'Bhavesh_Demand_Forecast_Notebook', 'spark.yarn.queue': 'workflow', 'spark.jars': '/apps/Jars/obelisk-retail-legos-Bhavesh.jar,/apps/Jars/mysql-connector-java-5.1.40.jar', 'spark.driver.extraJavaOptions': '-Dscala.color', 'spark.driver.memory': '4g', 'spark.executor.memory': '4g', 'spark.executor.instances': '4'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{\n",
    "    \"conf\":\n",
    "    {\n",
    "        \"spark.app.name\":\"Bhavesh_Demand_Forecast_Notebook\",\n",
    "        \"spark.yarn.queue\": \"workflow\",\n",
    "        \"spark.jars\": \"/apps/Jars/obelisk-retail-legos-Bhavesh.jar,/apps/Jars/mysql-connector-java-5.1.40.jar\",\n",
    "        \"spark.driver.extraJavaOptions\": \"-Dscala.color\",\n",
    "        \"spark.driver.memory\": \"4g\",\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.instances\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1338</td><td>application_1685341929318_1780</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://JMNGD1BAF370C10:8088/proxy/application_1685341929318_1780/\">Link</a></td><td><a target=\"_blank\" href=\"http://JMNGD1BAE170V10:8042/node/containerlogs/container_e112_1685341929318_1780_01_000001/couture\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import ai.couture.obelisk.commons.Constants._\n",
      "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
      "import ai.couture.obelisk.commons.Constants._\n",
      "import ai.couture.obelisk.commons.io._\n",
      "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
      "import ai.couture.obelisk.commons.utils.DataFrameUtil.minMaxScaler\n",
      "import org.apache.spark.sql._\n",
      "import org.apache.spark.sql.expressions._\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.types._\n",
      "import spark.implicits._\n"
     ]
    }
   ],
   "source": [
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.io._\n",
    "import ai.couture.obelisk.commons.utils.DateTimeUtil._\n",
    "import ai.couture.obelisk.commons.utils.DataFrameUtil.minMaxScaler\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mbaseDir\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/Archive/bhavesh/InventoryPrediction\n",
      "\u001b[1m\u001b[34mrunDate\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-04-30\n",
      "\u001b[1m\u001b[34mrunDir\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/Archive/bhavesh/InventoryPrediction/2023-04-30\n",
      "\u001b[1m\u001b[34mdataStartDate\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2022-02-01\n",
      "\u001b[1m\u001b[34mpredictionStart\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-05-01\n",
      "\u001b[1m\u001b[34mpredictionEnd\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-05-31\n",
      "\u001b[1m\u001b[34mtrainStart\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2022-04-01\n",
      "\u001b[1m\u001b[34mtrainEnd\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-04-29\n",
      "\u001b[1m\u001b[34mtrainXStart\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2022-04-01\n",
      "\u001b[1m\u001b[34mtrainXEnd\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-03-30\n",
      "\u001b[1m\u001b[34mtrainYStart\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-04-01\n",
      "\u001b[1m\u001b[34mtrainYEnd\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-04-30\n",
      "\u001b[1m\u001b[34mtestXStart\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2022-05-01\n",
      "\u001b[1m\u001b[34mtestXEnd\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-04-29\n",
      "\u001b[1m\u001b[34mtestYStart\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-05-01\n",
      "\u001b[1m\u001b[34mtestYEnd\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2023-05-31\n",
      "\u001b[1m\u001b[34msuffix\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = colorfamily_pricebucket_styletype_pattern_sleeve_brandname\n",
      "\u001b[1m\u001b[34mPREDICTED\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = predicted\n",
      "\u001b[1m\u001b[34mACTUAL\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = actual\n",
      "\u001b[1m\u001b[34mgetOldDateFromHereUDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = UserDefinedFunction(<function4>,StringType,Some(List(IntegerType, StringType, StringType, StringType)))\n",
      "\u001b[1m\u001b[34mgetMonths\u001b[0m: \u001b[1m\u001b[32m(dateFormat: String)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "var baseDir = \"/data/Archive/bhavesh/InventoryPrediction\"\n",
    "var runDate = \"2023-04-30\"\n",
    "var runDir = s\"$baseDir/$runDate\"\n",
    "\n",
    "// 15 months of train data for arima\n",
    "var dataStartDate = \"2022-02-01\"\n",
    "var predictionStart = \"2023-05-01\"\n",
    "var predictionEnd = \"2023-05-31\"\n",
    "\n",
    "var trainStart = \"2022-04-01\"\n",
    "var trainEnd = \"2023-04-29\"\n",
    "\n",
    "var trainXStart = \"2022-04-01\"\n",
    "var trainXEnd = \"2023-03-30\"\n",
    "var trainYStart = \"2023-04-01\"\n",
    "var trainYEnd = \"2023-04-29\"\n",
    "\n",
    "var testXStart = \"2022-05-01\"\n",
    "var testXEnd = \"2023-04-29\"\n",
    "var testYStart = \"2023-05-01\"\n",
    "var testYEnd = \"2023-05-31\"\n",
    "\n",
    "var suffix = \"colorfamily_pricebucket_styletype_pattern_sleeve_brandname\"\n",
    "\n",
    "var PREDICTED=\"predicted\"\n",
    "var ACTUAL=\"actual\"\n",
    "\n",
    "val getOldDateFromHereUDF = udf((numDays: Int, date: String, inputDateFormat: String, outputDateFormat: String) => {\n",
    "    getOldDateFromHere(numDays, date, inputDateFormat, outputDateFormat)\n",
    "})\n",
    "\n",
    "def getMonths(dateFormat: String)(df: DataFrame): DataFrame = {\n",
    "    df.withColumn(\"dateFormatted\", getOldDateFromHereUDF(lit(0), col(DATE), lit(dateFormat), lit(\"yyyy-MM-dd\")))\n",
    "    .withColumn(\"year\", split(col(\"dateFormatted\"), \"-\")(0))\n",
    "    .withColumn(\"month\", split(col(\"dateFormatted\"), \"-\")(1))\n",
    "    .withColumn(\"monthsInHis\", dense_rank().over(Window.orderBy(desc(\"year\"), desc(\"month\"))) - lit(1))\n",
    "    .drop(\"dateFormatted\", \"year\", \"month\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Attributes && QueryProductMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var productAttrs = ParquetToDF.getDF(\"/data/Archive/inventory/2023-04-30/queryProductMap_colorfamily_pricebucket_styletype_pattern_sleeve_brandname/processedProductAttr\")\n",
    "productAttrs.printSchema\n",
    "\n",
    "DFToParquet.putDF(s\"$runDir/ProductAttributes\", productAttrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var queryToProductMap = ParquetToDF.getDF(s\"/data/ecomm/ajio/processed/inventory/2023-04-30/queryProductMap_$suffix\")\n",
    "queryToProductMap.printSchema\n",
    "\n",
    "DFToParquet.putDF(s\"$runDir/QueryProductMapping/suffix=$suffix\", queryToProductMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Level Daily/Monthly Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mproductAttrs\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [productid: string]\n",
      "\u001b[1m\u001b[34minteractionsDB\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, date: date ... 1 more field]\n",
      "\u001b[1m\u001b[34mproductDailyQuantity\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, date: date ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "var productAttrs = ParquetToDF.getDF(s\"$runDir/ProductAttributes\").select(PRODUCTID).distinct()\n",
    "\n",
    "var interactionsDB = (\n",
    "    ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    .filter(col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "            && col(\"productid\").isNotNull && col(\"booked_rev\") > 0 \n",
    "            && col(\"userid\").isNotNull && col(\"quantity\") > 0 \n",
    "            && col(\"date\")<=predictionEnd && col(\"date\")>=dataStartDate\n",
    "            && col(\"date\") =!= runDate\n",
    "           )\n",
    "    .groupBy(\"productid\", \"date\")\n",
    "    .agg(sum(\"quantity\").cast(DoubleType).as(\"quantity\"))\n",
    ")\n",
    "\n",
    "DFToParquet.putDF(s\"$runDir/DailyQuantity/product\", interactionsDB.join(productAttrs, Seq(PRODUCTID)))\n",
    "\n",
    "var productDailyQuantity = ParquetToDF.getDF(s\"$runDir/DailyQuantity/product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mmonthMap\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [monthsInHis: int, tempDateForTimeSeries: date]\n",
      "\u001b[1m\u001b[34mproductMonthlyQuantity\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [monthsInHis: int, productid: string ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "var monthMap = ParquetToDF.getDF(\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/MonthsInHistoryMap\").select(col(\"monthsInHis\"), col(\"monthStartDate\").as(\"tempDateForTimeSeries\"))\n",
    "\n",
    "var productMonthlyQuantity = (\n",
    "    ParquetToDF.getDF(s\"$runDir/DailyQuantity/product\").transform(getMonths(\"yyyy-MM-dd\")).filter(col(\"monthsInHis\") <= 15)\n",
    "    .groupBy(PRODUCTID, \"monthsInHis\")\n",
    "    .agg(sum(\"quantity\") as \"quantity\")\n",
    "    .join(monthMap, Seq(\"monthsInHis\"))\n",
    ")\n",
    "\n",
    "DFToParquet.putDF(s\"$runDir/MonthlyQuantity/product\", productMonthlyQuantity.repartition(210))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mqueryProductMap\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [query: string, productid: string ... 2 more fields]\n",
      "\u001b[1m\u001b[34mqueryDailyQuantity\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [similargrouplevel: int, query: string ... 3 more fields]\n"
     ]
    }
   ],
   "source": [
    "var queryProductMap = ParquetToDF.getDF(s\"$runDir/QueryProductMapping/suffix=$suffix\").na.drop()\n",
    "\n",
    "var queryDailyQuantity = (\n",
    "    ParquetToDF.getDF(s\"$runDir/DailyQuantity/product\")\n",
    "    .join(queryProductMap, Seq(PRODUCTID))\n",
    "    .groupBy(\"similargrouplevel\", \"query\", \"template\", \"date\")\n",
    "    .agg(sum(\"quantity\") as \"quantity\")\n",
    ")\n",
    "DFToParquet.putDF(s\"$runDir/DailyQuantity/query\", queryDailyQuantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e148fc85a448e48d6a803cfcc66a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var monthMap = ParquetToDF.getDF(\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/MonthsInHistoryMap\").select(col(\"monthsInHis\"), col(\"monthStartDate\").as(\"tempDateForTimeSeries\"))\n",
    "\n",
    "var queryMonthlyQuantity = (\n",
    "    ParquetToDF.getDF(s\"$runDir/DailyQuantity/query\").transform(getMonths(\"yyyy-MM-dd\")).filter(col(\"monthsInHis\") <= 15)\n",
    "    .groupBy(\"similargrouplevel\", \"query\", \"template\", \"monthsInHis\")\n",
    "    .agg(sum(\"quantity\") as \"quantity\")\n",
    "    .join(monthMap, Seq(\"monthsInHis\"))\n",
    ")\n",
    "DFToParquet.putDF(s\"$runDir/MonthlyQuantity/query\", queryMonthlyQuantity.repartition(210))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction space Products/queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var productMonthlyQuantity = ParquetToDF.getDF(s\"$runDir/MonthlyQuantity/product\")\n",
    "\n",
    "var productsToPredict = productMonthlyQuantity.filter(col(\"monthsInHis\") >= 1 && col(\"monthsInHis\") <= 12).select(PRODUCTID).distinct()\n",
    "DFToParquet.putDF(s\"$runDir/PredictionSpace/products\", productsToPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mqueryProductMap\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [query: string, productid: string ... 2 more fields]\n",
      "\u001b[1m\u001b[34mqueriesToPredict\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [similargrouplevel: int, query: string ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "var queryProductMap = ParquetToDF.getDF(s\"$runDir/QueryProductMapping/suffix=$suffix\").na.drop().filter(col(\"query\") =!= \"\")\n",
    "\n",
    "var queriesToPredict = (\n",
    "    ParquetToDF.getDF(s\"$runDir/PredictionSpace/products\")\n",
    "    .join(queryProductMap, Seq(PRODUCTID))\n",
    "    .select(\"similargrouplevel\", \"query\", \"template\")\n",
    "    .distinct()\n",
    ")\n",
    "DFToParquet.putDF(s\"$runDir/PredictionSpace/queries\", queriesToPredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard deviation for products/queries based on 12 months data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var productMonthlyQuantity = ParquetToDF.getDF(s\"$runDir/MonthlyQuantity/product\")\n",
    "\n",
    "var productSigma = (\n",
    "    productMonthlyQuantity.filter(col(\"monthsInHis\") >= 1 && col(\"monthsInHis\") <=12)\n",
    "    .drop(\"tempDateForTimeSeries\")\n",
    "    .groupBy(PRODUCTID).agg(stddev(\"quantity\").as(\"stddev\"),avg(\"quantity\").as(\"avg\")).na.fill(0.0, Seq(\"stddev\"))\n",
    ")\n",
    "\n",
    "DFToParquet.putDF(s\"$runDir/Statistics12Months/product\", productSigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var queryMonthlyQuantity = ParquetToDF.getDF(s\"$runDir/MonthlyQuantity/query\")\n",
    "\n",
    "var querySigma = (\n",
    "    queryMonthlyQuantity.filter(col(\"monthsInHis\") >= 1 && col(\"monthsInHis\") <=12)\n",
    "    .drop(\"tempDateForTimeSeries\")\n",
    "    .groupBy(\"similargrouplevel\", \"query\", \"template\").agg(stddev(\"quantity\").as(\"stddev\"),avg(\"quantity\").as(\"avg\")).na.fill(0.0, Seq(\"stddev\"))\n",
    ")\n",
    "\n",
    "DFToParquet.putDF(s\"$runDir/Statistics12Months/query\", querySigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var actualProduct = (\n",
    "    ParquetToDF.getDF(s\"$runDir/MonthlyQuantity/product\")\n",
    "    .filter(col(\"monthsInHis\") === 0)\n",
    "    .select(col(PRODUCTID), col(\"quantity\").alias(\"sales\"))\n",
    ")\n",
    "\n",
    "DFToParquet.putDF(s\"$runDir/Actual/sales/product\", actualProduct)\n",
    "\n",
    "var queryProductMap = ParquetToDF.getDF(s\"$runDir/QueryProductMapping/suffix=$suffix\").na.drop()\n",
    "var actualQuery = (\n",
    "    ParquetToDF.getDF(s\"$runDir/Actual/sales/product\")\n",
    "    .join(queryProductMap, Seq(PRODUCTID))\n",
    "    .groupBy(\"similargrouplevel\", \"query\", \"template\")\n",
    "    .agg(sum(\"sales\") as \"sales\")\n",
    ")\n",
    "\n",
    "DFToParquet.putDF(s\"$runDir/Actual/sales/query\", actualQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mactualQuery\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [similargrouplevel: int, query: string ... 2 more fields]\n",
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- template: string (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      "\n",
      "\u001b[1m\u001b[34mqueriesToPredict\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [similargrouplevel: int, query: string ... 1 more field]\n",
      "root\n",
      " |-- similargrouplevel: integer (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- template: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var actualQuery = ParquetToDF.getDF(s\"$runDir/Actual/sales/query\")\n",
    "actualQuery.printSchema\n",
    "\n",
    "var queriesToPredict = ParquetToDF.getDF(s\"$runDir/PredictionSpace/queries\")\n",
    "queriesToPredict.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mtarget\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, yQuantity: double]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- yQuantity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var target = ParquetToDF.getDF(s\"$runDir/MonthlyQuantity/product\").filter(col(\"monthsInHis\") === 1).select(col(\"productid\"), col(\"quantity\").alias(\"yQuantity\"))\n",
    "target.printSchema\n",
    "\n",
    "DFToParquet.putDF(s\"$runDir/dataset/train/target\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, yQuantity: bigint ... 6 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- yQuantity: long (nullable = true)\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- colorfamily: string (nullable = true)\n",
      " |-- brandname: string (nullable = true)\n",
      " |-- sleeve: string (nullable = true)\n",
      " |-- pattern: string (nullable = true)\n",
      " |-- styletype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var df = ParquetToDF.getDF(\"/data/Archive/bhavesh/inventoryPrediction-deprecated/embeddings/data\")\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mtarget\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, yQuantity: double]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- yQuantity: double (nullable = true)\n",
      "\n",
      "\u001b[1m\u001b[34mproductAttrs\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, similargrouplevel: string ... 5 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- colorfamily: string (nullable = true)\n",
      " |-- brandname: string (nullable = true)\n",
      " |-- sleeve: string (nullable = true)\n",
      " |-- pattern: string (nullable = true)\n",
      " |-- styletype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var target = ParquetToDF.getDF(s\"$runDir/dataset/train/target\")\n",
    "target.printSchema\n",
    "\n",
    "var productAttrs = ParquetToDF.getDF(s\"$runDir/ProductAttributes\").select(PRODUCTID, \"similargrouplevel\", \"colorfamily\", \"brandname\", \"sleeve\", \"pattern\", \"styletype\")\n",
    "productAttrs.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34membeddingsData\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, similargrouplevel: string ... 6 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- colorfamily: string (nullable = true)\n",
      " |-- brandname: string (nullable = true)\n",
      " |-- sleeve: string (nullable = true)\n",
      " |-- pattern: string (nullable = true)\n",
      " |-- styletype: string (nullable = true)\n",
      " |-- yQuantity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var embeddingsData = productAttrs.join(target, Seq(PRODUCTID))\n",
    "embeddingsData.printSchema\n",
    "\n",
    "DFToParquet.putDF(s\"$runDir/Embeddings/data\", embeddingsData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mdatasetDir\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /data/Archive/bhavesh/InventoryPrediction/2023-04-30/dataset\n",
      "\u001b[1m\u001b[34mproductAttrs\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [productid: string, similargrouplevel: string]\n",
      "\u001b[1m\u001b[34msaveNLoadDF\u001b[0m: \u001b[1m\u001b[32m(path: String)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\u001b[0m\n",
      "\u001b[1m\u001b[34mgetMonthlyMeanStd\u001b[0m: \u001b[1m\u001b[32m(monthsLimit: Int, colsForStatistics: Array[String], calculateStd: Boolean)(df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "var datasetDir = s\"$runDir/dataset\"\n",
    "var productAttrs = (\n",
    "    ParquetToDF.getDF(s\"$runDir/ProductAttributes\").select(col(PRODUCTID), col(SIMILAR_GROUP_LEVEL))\n",
    "    .dropDuplicates(PRODUCTID)\n",
    "    .filter(col(SIMILAR_GROUP_LEVEL).isin(\"830216013\", \"830303011\"))\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "def saveNLoadDF(path: String)(df: DataFrame): DataFrame = {\n",
    "    DFToParquet.putDF(path, df)\n",
    "    ParquetToDF.getDF(path)\n",
    "}\n",
    "\n",
    "def getMonthlyMeanStd(monthsLimit: Int, colsForStatistics: Array[String], calculateStd: Boolean)(df: DataFrame): DataFrame = {\n",
    "    var intermediateDF = df\n",
    "    val monthsArray = (0 until monthsLimit).toArray\n",
    "    val colsToUseForCalculations = colsForStatistics.map(column => (column, monthsArray.map(month => s\"${month}_${column}\"))).toMap\n",
    "    colsToUseForCalculations.foreach(\n",
    "        columnInfo => {\n",
    "            val attr = columnInfo._1\n",
    "            val colsToUse = columnInfo._2\n",
    "            val meanExpr = colsToUse.map(col).reduce((col1, col2) => col1 + col2)\n",
    "            intermediateDF = intermediateDF.withColumn(s\"${attr}_avg_${monthsLimit}_months\", meanExpr/lit(colsToUse.length).cast(DoubleType))\n",
    "            \n",
    "            if(calculateStd){\n",
    "                print(attr)\n",
    "                val stdExpr = colsToUse.map(column => pow(col(column) - col(s\"${attr}_avg_${monthsLimit}_months\"), lit(2)))\n",
    "                .reduce((col1, col2) => col1 + col2)\n",
    "                intermediateDF = intermediateDF.withColumn(s\"${attr}_std_${monthsLimit}_months\",\n",
    "                                                           sqrt(stdExpr/lit(colsToUse.length - 1)))\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    intermediateDF\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mgenerateSalesXData\u001b[0m: \u001b[1m\u001b[32m(of: String, startDate: String, endDate: String, productAttrs: org.apache.spark.sql.DataFrame, baseDir: String)Unit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generateSalesXData(of: String, startDate: String, endDate: String, productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "\n",
    "    val features = Array(\"sales\")\n",
    "    val tempDir = \"/data/Archive/bhavesh/InventoryPrediction/temp/generateSalesXData\"\n",
    "    \n",
    "    var interactions = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    .filter(col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "            && col(\"productid\").isNotNull && col(\"booked_rev\") > 0 \n",
    "            && col(\"userid\").isNotNull && col(\"quantity\") > 0\n",
    "            && col(DATE).between(startDate, endDate)\n",
    "           )\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(sum(QUANTITY) as \"sales\")\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/interactions_1\"))\n",
    "    .transform(getMonths(\"yyyy-MM-dd\"))\n",
    "    .filter(col(\"monthsInHis\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"monthsInHis\")\n",
    "    .agg(sum(\"sales\").cast(DoubleType) as \"sales\", first(\"sales\") as \"temp\")\n",
    "    \n",
    "    val colsToDrop = interactions.columns.filter(_.endsWith(\"temp\"))\n",
    "    interactions = interactions.drop(colsToDrop: _*).na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/interactions_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, Array(\"sales\"), false))\n",
    "    .transform(getMonthlyMeanStd(6, Array(\"sales\"), false))\n",
    "    .transform(getMonthlyMeanStd(12, Array(\"sales\"), true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/sales\", interactions)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "\n",
    "generateSalesXData(\"train\", trainXStart, trainXEnd, productAttrs, datasetDir)\n",
    "generateSalesXData(\"test\", testXStart, testXEnd, productAttrs, datasetDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mgenerateGAXData\u001b[0m: \u001b[1m\u001b[32m(of: String, startDate: String, endDate: String, productAttrs: org.apache.spark.sql.DataFrame, baseDir: String)Unit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generateGAXData(of: String, startDate: String, endDate: String, productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "\n",
    "    val features = Array(\"PLPViewsPerDay\", \"PLPClicksPerDay\", \"PDPCountPerDay\", \"TotalAddToCartPerDay\")\n",
    "    val aggExprs = features.map(feature => sum(feature).cast(DoubleType).alias(feature))\n",
    "    val tempDir = \"/data/Archive/bhavesh/InventoryPrediction/temp/generateGAXData\"\n",
    "    \n",
    "    val ga = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/ProcessedGAData\")\n",
    "    .filter(col(DATE).between(startDate, endDate))\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(aggExprs.head, aggExprs.tail: _*)\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/ga_1\"))\n",
    "    .transform(getMonths(\"yyyy-MM-dd\"))\n",
    "    .filter(col(\"monthsInHis\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"monthsInHis\")\n",
    "    .agg(aggExprs.head, aggExprs.tail: _*)\n",
    "    .na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/ga_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, features, false))\n",
    "    .transform(getMonthlyMeanStd(6, features, false))\n",
    "    .transform(getMonthlyMeanStd(12, features, true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/ga\", ga)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "\n",
    "generateGAXData(\"train\", trainXStart, trainXEnd, productAttrs, datasetDir)\n",
    "generateGAXData(\"test\", testXStart, testXEnd, productAttrs, datasetDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mgenerateWishlistData\u001b[0m: \u001b[1m\u001b[32m(of: String, startDate: String, endDate: String, productAttrs: org.apache.spark.sql.DataFrame, baseDir: String)Unit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generateWishlistData(of: String, startDate: String, endDate: String, productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val features = Array(\"wishlist\")\n",
    "    val tempDir = \"/data/Archive/bhavesh/InventoryPrediction/temp/generateWishlistData\"\n",
    "    \n",
    "    var wishlist = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/processedWishlist\")\n",
    "    .filter(col(DATE).between(startDate, endDate))\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(countDistinct(\"wishlistid\").cast(DoubleType) as \"wishlist\")\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/wishlist_1\"))\n",
    "    .transform(getMonths(\"yyyy-MM-dd\"))\n",
    "    .filter(col(\"monthsInHis\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"monthsInHis\")\n",
    "    .agg(sum(\"wishlist\") as \"wishlist\", first(\"wishlist\") as \"temp\")\n",
    "    \n",
    "    val colsToDrop = wishlist.columns.filter(_.endsWith(\"temp\"))\n",
    "    wishlist = wishlist.drop(colsToDrop: _*).na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/wishlist_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, features, false))\n",
    "    .transform(getMonthlyMeanStd(6, features, false))\n",
    "    // .transform(getMonthlyMeanStd(12, features, true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/wishlist\", wishlist)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "generateWishlistData(\"train\", trainXStart, trainXEnd, productAttrs, datasetDir)\n",
    "generateWishlistData(\"test\", testXStart, testXEnd, productAttrs, datasetDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mgenerateReturnsData\u001b[0m: \u001b[1m\u001b[32m(of: String, startDate: String, endDate: String, productAttrs: org.apache.spark.sql.DataFrame, baseDir: String)Unit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generateReturnsData(of: String, startDate: String, endDate: String, productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val features = Array(\"returns\")\n",
    "    val tempDir = \"/data/Archive/bhavesh/InventoryPrediction/temp/generateReturns\"\n",
    "    \n",
    "    var returns = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    .filter(col(\"event\") === \"Return\" && col(QUANTITY) >= 0 &&\n",
    "        col(USERID).isNotNull && col(PRODUCTID).isNotNull)\n",
    "    .filter(col(DATE).between(startDate, endDate))\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(sum(QUANTITY).cast(DoubleType) as \"returns\")\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/returns_1\"))\n",
    "    .transform(getMonths(\"yyyy-MM-dd\"))\n",
    "    .filter(col(\"monthsInHis\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"monthsInHis\")\n",
    "    .agg(sum(\"returns\") as \"returns\", first(\"returns\") as \"temp\")\n",
    "    \n",
    "    val colsToDrop = returns.columns.filter(_.endsWith(\"temp\"))\n",
    "    returns = returns.drop(colsToDrop: _*).na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/returns_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, features, false))\n",
    "    .transform(getMonthlyMeanStd(6, features, false))\n",
    "    .transform(getMonthlyMeanStd(12, features, true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/returns\", returns)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "generateReturnsData(\"train\", trainXStart, trainXEnd, productAttrs, datasetDir)\n",
    "generateReturnsData(\"test\", testXStart, testXEnd, productAttrs, datasetDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mgenerateUsersData\u001b[0m: \u001b[1m\u001b[32m(of: String, startDate: String, endDate: String, productAttrs: org.apache.spark.sql.DataFrame, baseDir: String)Unit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generateUsersData(of: String, startDate: String, endDate: String, productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val features = Array(\"users\")\n",
    "    val tempDir = \"/data/Archive/bhavesh/InventoryPrediction/temp/generateUsersData\"\n",
    "    \n",
    "    var interactions = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/interactionsDB\")\n",
    "    .filter(col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" \n",
    "            && col(\"productid\").isNotNull && col(\"booked_rev\") > 0 \n",
    "            && col(\"userid\").isNotNull && col(\"quantity\") > 0\n",
    "            && col(DATE).between(startDate, endDate)\n",
    "           )\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/interactions_1\"))\n",
    "    .transform(getMonths(\"yyyy-MM-dd\"))\n",
    "    .filter(col(\"monthsInHis\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"monthsInHis\")\n",
    "    .agg(countDistinct(USERID).cast(DoubleType) as \"users\", first(USERID) as \"temp\")\n",
    "    \n",
    "    val colsToDrop = interactions.columns.filter(_.endsWith(\"temp\"))\n",
    "    interactions = interactions.drop(colsToDrop: _*).na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/interactions_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, features, false))\n",
    "    .transform(getMonthlyMeanStd(6, features, false))\n",
    "    .transform(getMonthlyMeanStd(12, features, true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/users\", interactions)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "generateUsersData(\"train\", trainXStart, trainXEnd, productAttrs, datasetDir)\n",
    "generateUsersData(\"test\", testXStart, testXEnd, productAttrs, datasetDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mgenerateAvailabilityQuantityData\u001b[0m: \u001b[1m\u001b[32m(of: String, startDate: String, endDate: String, productAttrs: org.apache.spark.sql.DataFrame, baseDir: String)Unit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generateAvailabilityQuantityData(of: String, startDate: String, endDate: String, productAttrs: DataFrame, baseDir: String): Unit = {\n",
    "    val features = Array(\"totalAvailableQuantity\", \"maxAvailableQuantity\")\n",
    "    val tempDir = \"/data/Archive/bhavesh/InventoryPrediction/temp/generateAvailabilityQuantityData\"\n",
    "    \n",
    "    var inventory = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/LiveProductsInventoryHistoryLegos\")\n",
    "    .withColumnRenamed(ITEM_ID, PRODUCTID)\n",
    "    .filter(col(DATE).between(startDate, endDate))\n",
    "    .groupBy(PRODUCTID, DATE)\n",
    "    .agg(sum(\"availablequantity\").as(\"availableQuantity\"))\n",
    "    .join(productAttrs, Seq(PRODUCTID))\n",
    "    .transform(saveNLoadDF(s\"$tempDir/inventory_1\"))\n",
    "    .transform(getMonths(\"yyyy-MM-dd\"))\n",
    "    .filter(col(\"monthsInHis\") < 12)\n",
    "    .groupBy(PRODUCTID)\n",
    "    .pivot(\"monthsInHis\")\n",
    "    .agg(sum(\"availableQuantity\").cast(DoubleType) as \"totalAvailableQuantity\", max(\"availableQuantity\") as \"maxAvailableQuantity\")\n",
    "    \n",
    "    inventory = inventory.na.fill(0)\n",
    "    .transform(saveNLoadDF(s\"$tempDir/inventory_2\"))\n",
    "    .transform(getMonthlyMeanStd(3, features, false))\n",
    "    .transform(getMonthlyMeanStd(6, features, false))\n",
    "    .transform(getMonthlyMeanStd(12, features, true))\n",
    "    \n",
    "    DFToParquet.putDF(s\"$baseDir/$of/availableQuantity\", inventory)\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "generateAvailabilityQuantityData(\"train\", trainXStart, trainXEnd, productAttrs, datasetDir)\n",
    "generateAvailabilityQuantityData(\"test\", testXStart, testXEnd, productAttrs, datasetDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mCreateDataset\u001b[0m: \u001b[1m\u001b[32m(of: String, xPath: String, yPath: String, basePath: String)Unit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def CreateDataset(of: String, xPath: String, yPath: String, basePath: String): Unit = {\n",
    "    val tempDir = \"/data/Archive/bhavesh/InventoryPrediction/temp/createDataset\"\n",
    "    var xData = ParquetToDF.getDF(s\"$xPath/$of/sales\").drop(SIMILAR_GROUP_LEVEL)\n",
    "    .join(ParquetToDF.getDF(s\"$xPath/$of/ga\").drop(SIMILAR_GROUP_LEVEL), Seq(PRODUCTID), \"outer\")\n",
    "    .transform(saveNLoadDF(s\"$tempDir/dataset_1\"))\n",
    "    .join(ParquetToDF.getDF(s\"$xPath/$of/wishlist\").drop(SIMILAR_GROUP_LEVEL), Seq(PRODUCTID), \"outer\")\n",
    "    .join(ParquetToDF.getDF(s\"$xPath/$of/returns\").drop(SIMILAR_GROUP_LEVEL), Seq(PRODUCTID), \"outer\")\n",
    "    .transform(saveNLoadDF(s\"$tempDir/dataset_2\"))\n",
    "    .join(ParquetToDF.getDF(s\"$xPath/$of/users\").drop(SIMILAR_GROUP_LEVEL), Seq(PRODUCTID), \"outer\")\n",
    "    .join(ParquetToDF.getDF(s\"$xPath/$of/availableQuantity\").drop(SIMILAR_GROUP_LEVEL), Seq(PRODUCTID), \"outer\")\n",
    "    .na.fill(0)\n",
    "    \n",
    "    if(of == \"test\"){\n",
    "        var productsToPredict = ParquetToDF.getDF(\"/data/Archive/bhavesh/InventoryPrediction/2023-04-30/PredictionSpace/products\")\n",
    "        xData = productsToPredict.join(xData, Seq(PRODUCTID), \"left\").na.fill(0)\n",
    "    }\n",
    "    \n",
    "    DFToParquet.putDF(s\"$basePath/$of/XData\", xData)\n",
    "    \n",
    "    if(of == \"train\"){\n",
    "        var yData = ParquetToDF.getDF(s\"$yPath/$of/target\").drop(SIMILAR_GROUP_LEVEL)\n",
    "        DFToParquet.putDF(s\"$basePath/$of/YData\", yData)\n",
    "    }\n",
    "    HdfsUtils.delete(tempDir)\n",
    "}\n",
    "CreateDataset(\"train\", datasetDir, datasetDir, s\"$runDir/OriginalDataset\")\n",
    "CreateDataset(\"test\", datasetDir, datasetDir, s\"$runDir/OriginalDataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mgenerateTransformedDataset\u001b[0m: \u001b[1m\u001b[32m(of: String, datasetPath: String, transformedDatasetPath: String, performNormalization: Boolean, isBaselineTransformation: Boolean)Unit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def generateTransformedDataset(of: String, datasetPath: String, transformedDatasetPath: String, performNormalization: Boolean = true, isBaselineTransformation: Boolean = true): Unit = {\n",
    "    var xData: DataFrame = ParquetToDF.getDF(s\"$datasetPath/$of/XData\")\n",
    "    \n",
    "    if(isBaselineTransformation){\n",
    "        xData = (\n",
    "            xData\n",
    "            .select(\"productid\", \"0_sales\", \"1_sales\", \"2_sales\", \"3_sales\", \"4_sales\", \"5_sales\", \"6_sales\", \"7_sales\", \"8_sales\", \"9_sales\", \"10_sales\", \"11_sales\",\n",
    "               \"sales_avg_12_months\", \"sales_std_12_months\", \"0_returns\", \"0_users\", \"0_PLPViewsPerDay\", \"0_PLPClicksPerDay\", \"0_PDPCountPerDay\",\n",
    "               \"0_TotalAddToCartPerDay\", \"0_wishlist\", \"0_totalAvailableQuantity\")\n",
    "            .toDF(\"productid\", \"0_monthSales\", \"1_monthSales\", \"2_monthSales\", \"3_monthSales\", \"4_monthSales\", \"5_monthSales\", \"6_monthSales\", \"7_monthSales\", \"8_monthSales\", \n",
    "              \"9_monthSales\", \"10_monthSales\", \"11_monthSales\", \"avgSales\", \"stddevSales\", \"totalReturn\", \"totalUsers\", \"totalPLPViews\", \"totalPLPClicks\", \n",
    "              \"totalPDPCount\", \"totalATC\", \"totalWishList\", \"totalAvailableQuantity\")\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    \n",
    "    var data: DataFrame = null\n",
    "    if (of == \"train\") {\n",
    "        var yData: DataFrame = ParquetToDF.getDF(s\"$datasetPath/$of/YData\")\n",
    "        data = xData.join(yData, Seq(PRODUCTID))\n",
    "    }\n",
    "    else if(of == \"test\"){\n",
    "        data = xData\n",
    "    }\n",
    "    \n",
    "    val productAttributesLegosFNL: DataFrame = (\n",
    "        ParquetToDF.getDF(s\"$runDir/ProductAttributes\")\n",
    "        .select(PRODUCTID, SIMILAR_GROUP_LEVEL, \"colorfamily\", \"brandname\", \"styletype\", \"pattern\", \"sleeve\", \"pricebucket\")\n",
    "    )\n",
    "    \n",
    "    data = data.join(productAttributesLegosFNL, Seq(PRODUCTID)).na.fill(\"Null\")\n",
    "    \n",
    "    println(data.count)\n",
    "    var attributes = Array(\"colorfamily\", \"pattern\", \"brandname\", \"styletype\", \"sleeve\")\n",
    "    attributes.foreach(\n",
    "        attribute => {\n",
    "            var attrDFMS = CSVToDF.getDF(s\"$runDir/Embeddings/menShirts/${attribute}.csv\", inferSchema=true)\n",
    "            \n",
    "            attrDFMS = attrDFMS\n",
    "            .select(attribute, attrDFMS.columns.filter(_.startsWith(\"normalized\")): _*)\n",
    "            .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830216013\"))\n",
    "            \n",
    "            var attrDFWK = CSVToDF.getDF(s\"$runDir/Embeddings/womenKurtas/${attribute}.csv\", inferSchema=true)\n",
    "            \n",
    "            attrDFWK = attrDFWK\n",
    "            .select(attribute, attrDFMS.columns.filter(_.startsWith(\"normalized\")): _*)\n",
    "            .withColumn(SIMILAR_GROUP_LEVEL, lit(\"830303011\"))\n",
    "            \n",
    "            var cols = attrDFWK.columns\n",
    "            \n",
    "            var attrDF = attrDFMS.select(cols.head, cols.tail: _*).union(attrDFWK)\n",
    "            \n",
    "            data = data.join(attrDF, Seq(SIMILAR_GROUP_LEVEL, attribute)).drop(attribute)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    println(data.count)\n",
    "    val menShirtsData: DataFrame = data.filter(col(SIMILAR_GROUP_LEVEL) === \"830216013\")\n",
    "    val womenKurtasData: DataFrame = data.filter(col(SIMILAR_GROUP_LEVEL) === \"830303011\")\n",
    "    \n",
    "    DFToParquet.putDF(s\"$transformedDatasetPath/combined/data/$of\", data)\n",
    "    DFToParquet.putDF(s\"$transformedDatasetPath/menShirts/data/$of\", menShirtsData)\n",
    "    DFToParquet.putDF(s\"$transformedDatasetPath/womenKurtas/data/$of\", womenKurtasData)\n",
    "    \n",
    "    if (performNormalization){\n",
    "        \n",
    "        var colsToScale: Array[String] = Array(\n",
    "            \"0_monthSales\", \"1_monthSales\", \"2_monthSales\", \"3_monthSales\", \"4_monthSales\", \"5_monthSales\", \"6_monthSales\", \"7_monthSales\", \n",
    "            \"8_monthSales\", \"9_monthSales\", \"10_monthSales\", \"11_monthSales\", \"avgSales\", \"stddevSales\", \"totalReturn\", \"totalUsers\", \n",
    "            \"totalPLPViews\", \"totalPLPClicks\", \"totalPDPCount\", \"totalATC\", \"totalWishList\", \"totalAvailableQuantity\")\n",
    "\n",
    "        if(of == \"train\"){\n",
    "\n",
    "            colsToScale = colsToScale ++ Array(\"yQuantity\")\n",
    "        }\n",
    "\n",
    "        DFToParquet.putDF(s\"$transformedDatasetPath/combined/normalizedData/$of\", data.transform(minMaxScaler(colsToScale)))\n",
    "        DFToParquet.putDF(s\"$transformedDatasetPath/menShirts/normalizedData/$of\", menShirtsData.transform(minMaxScaler(colsToScale)))\n",
    "        DFToParquet.putDF(s\"$transformedDatasetPath/womenKurtas/normalizedData/$of\", womenKurtasData.transform(minMaxScaler(colsToScale)))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generateTransformedDataset(\"train\", s\"$runDir/OriginalDataset\", s\"$runDir/TransformedDataset/baseline\", true, true)\n",
    "generateTransformedDataset(\"test\", s\"$runDir/OriginalDataset\", s\"$runDir/TransformedDataset/baseline\", true, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generateTransformedDataset(\"train\", s\"$runDir/OriginalDataset\", s\"$runDir/TransformedDataset/complete\", false, false)\n",
    "generateTransformedDataset(\"test\", s\"$runDir/OriginalDataset\", s\"$runDir/TransformedDataset/complete\", false, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres31\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 157640\n"
     ]
    }
   ],
   "source": [
    "productsToPredict.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "productAttrs: org.apache.spark.sql.DataFrame = [productid: string, similargrouplevel: string ... 5 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- similargrouplevel: string (nullable = true)\n",
      " |-- colorfamily: string (nullable = true)\n",
      " |-- brandname: string (nullable = true)\n",
      " |-- sleeve: string (nullable = true)\n",
      " |-- pattern: string (nullable = true)\n",
      " |-- styletype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productAttrs = productAttrs.select(\"productid\", \"similargrouplevel\", \"colorfamily\", \"brandname\", \"sleeve\", \"pattern\", \"styletype\")\n",
    "productAttrs.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "productsToPredict: org.apache.spark.sql.DataFrame = [productid: string, similargrouplevel: string ... 5 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = false)\n",
      " |-- similargrouplevel: string (nullable = false)\n",
      " |-- colorfamily: string (nullable = false)\n",
      " |-- brandname: string (nullable = false)\n",
      " |-- sleeve: string (nullable = false)\n",
      " |-- pattern: string (nullable = false)\n",
      " |-- styletype: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productsToPredict = productsToPredict.join(productAttrs, Seq(PRODUCTID)).na.fill(\"Null\")\n",
    "productsToPredict.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mres37\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 157640\n"
     ]
    }
   ],
   "source": [
    "productsToPredict.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mattribute\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = sleeve\n",
      "\u001b[1m\u001b[34mattrDFMS\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [sleeve_0: double, sleeve_1: double ... 4 more fields]\n",
      "attrDFMS: org.apache.spark.sql.DataFrame = [sleeve: string, normalized_sleeve_0: double ... 2 more fields]\n",
      "\u001b[1m\u001b[34mattrDFWK\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [sleeve_0: double, sleeve_1: double ... 4 more fields]\n",
      "attrDFWK: org.apache.spark.sql.DataFrame = [sleeve: string, normalized_sleeve_0: double ... 2 more fields]\n",
      "\u001b[1m\u001b[34mcols\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array(sleeve, normalized_sleeve_0, normalized_sleeve_1, similargrouplevel)\n",
      "\u001b[1m\u001b[34mattrDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [sleeve: string, normalized_sleeve_0: double ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "var attribute = \"sleeve\"\n",
    "var attrDFMS = CSVToDF.getDF(s\"$runDir/Embeddings/menShirts/${attribute}.csv\", inferSchema=true)\n",
    "            \n",
    "attrDFMS = (attrDFMS\n",
    ".select(attribute, attrDFMS.columns.filter(_.startsWith(\"normalized\")): _*)\n",
    ".withColumn(SIMILAR_GROUP_LEVEL, lit(\"830216013\")))\n",
    "\n",
    "var attrDFWK = CSVToDF.getDF(s\"$runDir/Embeddings/womenKurtas/${attribute}.csv\", inferSchema=true)\n",
    "\n",
    "attrDFWK = (attrDFWK\n",
    ".select(attribute, attrDFMS.columns.filter(_.startsWith(\"normalized\")): _*)\n",
    ".withColumn(SIMILAR_GROUP_LEVEL, lit(\"830303011\")))\n",
    "\n",
    "var cols = attrDFWK.columns\n",
    "\n",
    "var attrDF = attrDFMS.select(cols.head, cols.tail: _*).union(attrDFWK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+------------------+-----------+---------+-----------+---------+\n",
      "|similargrouplevel|sleeve          |productid         |colorfamily|brandname|pattern    |styletype|\n",
      "+-----------------+----------------+------------------+-----------+---------+-----------+---------+\n",
      "|830303011        |stylised sleeve |460208940_offwhite|white      |ajio     |solid      |straight |\n",
      "|830303011        |stylised sleeve |440780256_red     |red        |avaasa   |solid      |a-line   |\n",
      "|830303011        |stylised sleeve |460528805_yellow  |yellow     |indya    |indian     |a-line   |\n",
      "|830303011        |racerback sleeve|460863887_beige   |beige      |svrnaa   |floral     |a-line   |\n",
      "|830303011        |stylised sleeve |440932774_navy    |blue       |avaasa   |floral     |flared   |\n",
      "|830303011        |stylised sleeve |440773531_blue    |blue       |w        |solid      |straight |\n",
      "|830303011        |stylised sleeve |440875319_pink    |pink       |w        |embellished|straight |\n",
      "|830303011        |racerback sleeve|460863887_maroon  |maroon     |svrnaa   |floral     |a-line   |\n",
      "|830303011        |stylised sleeve |440772937_black   |black      |aurelia  |Null       |straight |\n",
      "+-----------------+----------------+------------------+-----------+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productsToPredict.join(attrDF, Seq(SIMILAR_GROUP_LEVEL, attribute), \"leftanti\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [productid: string, 0_sales: double ... 155 more fields]\n",
      "+------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+------------------+------------------+-------------------+-------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+-----------------+------------------+-----------------+-----------------------+-----------------+------------------+-----------------+-----------------------+---------------------------+----------------------------+---------------------------+---------------------------------+---------------------------+----------------------------+---------------------------+---------------------------------+----------------------------+----------------------------+-----------------------------+-----------------------------+----------------------------+----------------------------+----------------------------------+----------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------------------+---------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+----------+--------------------+--------------------+---------------------+---------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+------------------+------------------+-------------------+-------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+-------------------------+-----------------------+-------------------------+-----------------------+-----------------------------------+---------------------------------+-----------------------------------+---------------------------------+------------------------------------+------------------------------------+----------------------------------+----------------------------------+\n",
      "|productid         |0_sales|1_sales|2_sales|3_sales|4_sales|5_sales|6_sales|7_sales|8_sales|9_sales|10_sales|11_sales|sales_avg_3_months|sales_avg_6_months|sales_avg_12_months|sales_std_12_months|0_PLPViewsPerDay|0_PLPClicksPerDay|0_PDPCountPerDay|0_TotalAddToCartPerDay|1_PLPViewsPerDay|1_PLPClicksPerDay|1_PDPCountPerDay|1_TotalAddToCartPerDay|2_PLPViewsPerDay|2_PLPClicksPerDay|2_PDPCountPerDay|2_TotalAddToCartPerDay|3_PLPViewsPerDay|3_PLPClicksPerDay|3_PDPCountPerDay|3_TotalAddToCartPerDay|4_PLPViewsPerDay|4_PLPClicksPerDay|4_PDPCountPerDay|4_TotalAddToCartPerDay|5_PLPViewsPerDay|5_PLPClicksPerDay|5_PDPCountPerDay|5_TotalAddToCartPerDay|6_PLPViewsPerDay|6_PLPClicksPerDay|6_PDPCountPerDay|6_TotalAddToCartPerDay|7_PLPViewsPerDay|7_PLPClicksPerDay|7_PDPCountPerDay|7_TotalAddToCartPerDay|8_PLPViewsPerDay|8_PLPClicksPerDay|8_PDPCountPerDay|8_TotalAddToCartPerDay|9_PLPViewsPerDay|9_PLPClicksPerDay|9_PDPCountPerDay|9_TotalAddToCartPerDay|10_PLPViewsPerDay|10_PLPClicksPerDay|10_PDPCountPerDay|10_TotalAddToCartPerDay|11_PLPViewsPerDay|11_PLPClicksPerDay|11_PDPCountPerDay|11_TotalAddToCartPerDay|PLPViewsPerDay_avg_3_months|PLPClicksPerDay_avg_3_months|PDPCountPerDay_avg_3_months|TotalAddToCartPerDay_avg_3_months|PLPViewsPerDay_avg_6_months|PLPClicksPerDay_avg_6_months|PDPCountPerDay_avg_6_months|TotalAddToCartPerDay_avg_6_months|PLPViewsPerDay_avg_12_months|PLPViewsPerDay_std_12_months|PLPClicksPerDay_avg_12_months|PLPClicksPerDay_std_12_months|PDPCountPerDay_avg_12_months|PDPCountPerDay_std_12_months|TotalAddToCartPerDay_avg_12_months|TotalAddToCartPerDay_std_12_months|0_wishlist|1_wishlist|2_wishlist|3_wishlist|4_wishlist|5_wishlist|6_wishlist|7_wishlist|8_wishlist|9_wishlist|wishlist_avg_3_months|wishlist_avg_6_months|0_returns|1_returns|2_returns|3_returns|4_returns|5_returns|6_returns|7_returns|8_returns|9_returns|10_returns|11_returns|returns_avg_3_months|returns_avg_6_months|returns_avg_12_months|returns_std_12_months|0_users|1_users|2_users|3_users|4_users|5_users|6_users|7_users|8_users|9_users|10_users|11_users|users_avg_3_months|users_avg_6_months|users_avg_12_months|users_std_12_months|0_totalAvailableQuantity|0_maxAvailableQuantity|1_totalAvailableQuantity|1_maxAvailableQuantity|2_totalAvailableQuantity|2_maxAvailableQuantity|3_totalAvailableQuantity|3_maxAvailableQuantity|4_totalAvailableQuantity|4_maxAvailableQuantity|5_totalAvailableQuantity|5_maxAvailableQuantity|6_totalAvailableQuantity|6_maxAvailableQuantity|7_totalAvailableQuantity|7_maxAvailableQuantity|8_totalAvailableQuantity|8_maxAvailableQuantity|9_totalAvailableQuantity|9_maxAvailableQuantity|10_totalAvailableQuantity|10_maxAvailableQuantity|11_totalAvailableQuantity|11_maxAvailableQuantity|totalAvailableQuantity_avg_3_months|maxAvailableQuantity_avg_3_months|totalAvailableQuantity_avg_6_months|maxAvailableQuantity_avg_6_months|totalAvailableQuantity_avg_12_months|totalAvailableQuantity_std_12_months|maxAvailableQuantity_avg_12_months|maxAvailableQuantity_std_12_months|\n",
      "+------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+------------------+------------------+-------------------+-------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+-----------------+------------------+-----------------+-----------------------+-----------------+------------------+-----------------+-----------------------+---------------------------+----------------------------+---------------------------+---------------------------------+---------------------------+----------------------------+---------------------------+---------------------------------+----------------------------+----------------------------+-----------------------------+-----------------------------+----------------------------+----------------------------+----------------------------------+----------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------------------+---------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+----------+--------------------+--------------------+---------------------+---------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+------------------+------------------+-------------------+-------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+-------------------------+-----------------------+-------------------------+-----------------------+-----------------------------------+---------------------------------+-----------------------------------+---------------------------------+------------------------------------+------------------------------------+----------------------------------+----------------------------------+\n",
      "|440780256_red     |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |1.0    |1.0     |0.0     |0.0               |0.0               |0.16666666666666666|0.38924947208076155|0.0             |0.0              |0.0             |0.0                   |0.0             |0.0              |0.0             |0.0                   |0.0             |0.0              |0.0             |0.0                   |0.0             |0.0              |2.0             |0.0                   |0.0             |0.0              |0.0             |0.0                   |0.0             |1.0              |0.0             |0.0                   |0.0             |1.0              |2.0             |0.0                   |0.0             |0.0              |1.0             |0.0                   |0.0             |0.0              |0.0             |0.0                   |3443.0          |164.0            |370.0           |6.0                   |821.0            |43.0              |60.0             |2.0                    |0.0              |0.0               |0.0              |0.0                    |0.0                        |0.0                         |0.0                        |0.0                              |0.0                        |0.16666666666666666         |0.3333333333333333         |0.0                              |355.3333333333333           |1000.5974881706096          |17.416666666666668           |47.77496174458929            |36.25                       |106.48954280713542          |0.6666666666666666                |1.775250729197189                 |0.0       |0.0       |0.0       |0.0       |0.0       |0.0       |0.0       |0.0       |0.0       |0.0       |0.0                  |0.0                  |0.0      |0.0      |0.0      |0.0      |0.0      |0.0      |0.0      |0.0      |0.0      |0.0      |1.0       |0.0       |0.0                 |0.0                 |0.08333333333333333  |0.28867513459481287  |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |1.0    |1.0     |0.0     |0.0               |0.0               |0.16666666666666666|0.38924947208076155|0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                      |0                      |0.0                      |0                      |0.0                                |0.0                              |0.0                                |0.0                              |0.0                                 |0.0                                 |0.0                               |0.0                               |\n",
      "|460208940_offwhite|0.0    |2.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0     |0.0     |0.6666666666666666|0.3333333333333333|0.16666666666666666|0.5773502691896256 |22.0            |1.0              |1.0             |0.0                   |909.0           |126.0            |186.0           |2.0                   |562.0           |69.0             |62.0            |1.0                   |0.0             |0.0              |0.0             |0.0                   |4.0             |2.0              |1.0             |0.0                   |1.0             |1.0              |2.0             |0.0                   |0.0             |0.0              |0.0             |0.0                   |1.0             |0.0              |0.0             |0.0                   |1.0             |0.0              |0.0             |0.0                   |0.0             |0.0              |2.0             |0.0                   |0.0              |0.0               |0.0              |0.0                    |0.0              |0.0               |0.0              |0.0                    |497.6666666666667          |65.33333333333333           |83.0                       |1.0                              |249.66666666666666         |33.166666666666664          |42.0                       |0.5                              |125.0                       |294.66899765977786          |16.583333333333332           |39.70678516171749            |21.166666666666668          |54.83335635994727           |0.25                              |0.621581560508061                 |0.0       |20.0      |6.0       |0.0       |0.0       |0.0       |0.0       |0.0       |0.0       |0.0       |8.666666666666666    |4.333333333333333    |0.0      |1.0      |0.0      |0.0      |0.0      |0.0      |0.0      |0.0      |0.0      |0.0      |0.0       |0.0       |0.3333333333333333  |0.16666666666666666 |0.08333333333333333  |0.2886751345948128   |0.0    |2.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0    |0.0     |0.0     |0.6666666666666666|0.3333333333333333|0.16666666666666666|0.5773502691896256 |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                     |0                     |0.0                      |0                      |0.0                      |0                      |0.0                                |0.0                              |0.0                                |0.0                              |0.0                                 |0.0                                 |0.0                               |0.0                               |\n",
      "+------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+------------------+------------------+-------------------+-------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+----------------+-----------------+----------------+----------------------+-----------------+------------------+-----------------+-----------------------+-----------------+------------------+-----------------+-----------------------+---------------------------+----------------------------+---------------------------+---------------------------------+---------------------------+----------------------------+---------------------------+---------------------------------+----------------------------+----------------------------+-----------------------------+-----------------------------+----------------------------+----------------------------+----------------------------------+----------------------------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------------------+---------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+----------+----------+--------------------+--------------------+---------------------+---------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+------------------+------------------+-------------------+-------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+------------------------+----------------------+-------------------------+-----------------------+-------------------------+-----------------------+-----------------------------------+---------------------------------+-----------------------------------+---------------------------------+------------------------------------+------------------------------------+----------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var df = ParquetToDF.getDF(s\"$runDir/OriginalDataset/test/XData\")\n",
    "df.filter(col(\"productid\").isin(\"460208940_offwhite\", \"440780256_red\")).show(false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
