{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'Bhavesh_Relevancy_notebook', 'spark.yarn.queue': 'default', 'spark.jars': '/apps/Jars/obelisk-retail-legos.jar'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{\"conf\":{\n",
    "     \"spark.app.name\":\"Bhavesh_Relevancy_notebook\", \"spark.yarn.queue\": \"default\", \"spark.jars\": \"/apps/Jars/obelisk-retail-legos.jar\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9756</td><td>application_1672415114980_0753</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://JMNGD1BAF370C10:8088/proxy/application_1672415114980_0753/\">Link</a></td><td><a target=\"_blank\" href=\"http://JMNGD1BAE050V04:8042/node/containerlogs/container_e88_1672415114980_0753_01_000001/couture\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import ai.couture.obelisk.commons.Constants.{DB_PRODUCT_INTERACTIONS_PATH, PROCESSED_GA_DATA, GA_USER_CLICK_DATA}\n",
      "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
      "import ai.couture.obelisk.commons.Constants._\n",
      "import ai.couture.obelisk.commons.io.{DFToParquet, HdfsUtils, ParquetToDF, DFToCSV, CSVToDF}\n",
      "import ai.couture.obelisk.commons.utils.DateTimeUtil.{getFutureDateFromHere, getNumberOfDaysBetweenTwoDates, getOldDateFromHere, getOldDate}\n",
      "import ai.couture.obelisk.retail.legos.BaseBlocksRevised\n",
      "import ai.couture.obelisk.retail.legos.HyperParams.PLPRecommendationConstants._\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.sql.expressions.Window\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.types.DoubleType\n"
     ]
    }
   ],
   "source": [
    "import ai.couture.obelisk.commons.Constants.{DB_PRODUCT_INTERACTIONS_PATH, PROCESSED_GA_DATA, GA_USER_CLICK_DATA}\n",
    "import ai.couture.obelisk.commons.Constants.STANDARD_COL_NAMES._\n",
    "import ai.couture.obelisk.commons.Constants._\n",
    "import ai.couture.obelisk.commons.io.{DFToParquet, HdfsUtils, ParquetToDF, DFToCSV, CSVToDF}\n",
    "import ai.couture.obelisk.commons.utils.DateTimeUtil.{getFutureDateFromHere, getNumberOfDaysBetweenTwoDates, getOldDateFromHere, getOldDate}\n",
    "import ai.couture.obelisk.retail.legos.BaseBlocksRevised\n",
    "import ai.couture.obelisk.retail.legos.HyperParams.PLPRecommendationConstants._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interactionsDB: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productid: string, productsku: string ... 28 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- productsku: string (nullable = true)\n",
      " |-- userid: long (nullable = true)\n",
      " |-- interactionid: string (nullable = true)\n",
      " |-- baseprice: double (nullable = true)\n",
      " |-- cancelledquantity: long (nullable = true)\n",
      " |-- mrp: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- returnwindow: long (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- isluxe: long (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      " |-- ordertotalprice: double (nullable = true)\n",
      " |-- pincode: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- penaltycharge: long (nullable = true)\n",
      " |-- orderid: string (nullable = true)\n",
      " |-- orderquantity: long (nullable = true)\n",
      " |-- refundamount: double (nullable = true)\n",
      " |-- booked_rev: double (nullable = true)\n",
      " |-- net_rev: double (nullable = true)\n",
      " |-- DateTime: string (nullable = true)\n",
      " |-- purchase: string (nullable = true)\n",
      " |-- gross_rev: double (nullable = true)\n",
      " |-- gross_quantity: long (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- promotionaldiscount: double (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var interactionsDB = ParquetToDF.getDF(\n",
    "    DB_PRODUCT_INTERACTIONS_PATH).filter(\n",
    "        col(\"event\") === \"Checkout\" && col(\"purchase\") === \"New\" && col(PRODUCTID).isNotNull && col(USERID).isNotNull && col(QUANTITY) > 0 && col(\"booked_rev\")>0\n",
    "    )\n",
    "interactionsDB.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaData: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 6 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- clusterid: string (nullable = true)\n",
      " |-- PDPCountPerDay: double (nullable = true)\n",
      " |-- PLPViewsPerDay: double (nullable = true)\n",
      " |-- PLPClicksPerDay: double (nullable = true)\n",
      " |-- TotalAddToCartPerDay: double (nullable = true)\n",
      " |-- CheckoutsPerDay: double (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var gaData = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/GaCohortVisibilityLegosFNL\")\n",
    "gaData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userClusterMapping: org.apache.spark.sql.DataFrame = [userid: string, clusterid: string]\n",
      "root\n",
      " |-- userid: string (nullable = true)\n",
      " |-- clusterid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var userClusterMapping = ParquetToDF.getDF(\"/data/ecomm/ajio/processed/segmentationFiles/PremiumClustering/userClusters\")select(USERID, CLUSTERID)\n",
    "userClusterMapping.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experimentDate: String = 2022-12-06\n",
      "interactionsDB: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productid: string, productsku: string ... 28 more fields]\n",
      "gaData: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 6 more fields]\n"
     ]
    }
   ],
   "source": [
    "var experimentDate: String = \"2022-12-06\"\n",
    "interactionsDB = interactionsDB.filter(col(DATE) > getOldDateFromHere(30, experimentDate, \"yyyy-MM-dd\") && col(DATE) <= experimentDate)\n",
    "gaData = gaData.filter(col(DATE) > getOldDateFromHere(30, experimentDate, \"yyyy-MM-dd\") && col(DATE) <= experimentDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interactionsDB: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productid: string, interactionid: string ... 4 more fields]\n"
     ]
    }
   ],
   "source": [
    "interactionsDB = interactionsDB.join(userClusterMapping, Seq(USERID)).select(PRODUCTID, INTERACTIONID, QUANTITY, \"booked_rev\", DATE, CLUSTERID)\n",
    "// interactionsDB.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 4 more fields]\n",
      "+------------------+---------+----------+--------+--------+-----+\n",
      "|productid         |clusterid|date      |Revenue |quantity|sales|\n",
      "+------------------+---------+----------+--------+--------+-----+\n",
      "|462566319_navy    |cluster0 |2022-11-08|118560.0|156     |154  |\n",
      "|469251942_darknavy|cluster0 |2022-11-20|543.23  |1       |1    |\n",
      "|6005533020_multi  |cluster0 |2022-11-15|13056.0 |51      |50   |\n",
      "|441134776_jetblack|cluster0 |2022-11-21|12188.3 |20      |20   |\n",
      "|469254665_black   |cluster0 |2022-11-28|30480.43|24      |24   |\n",
      "+------------------+---------+----------+--------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var df = interactionsDB.groupBy(PRODUCTID, CLUSTERID, DATE).agg(sum(\"booked_rev\") as \"Revenue\", sum(QUANTITY) as \"quantity\", countDistinct(INTERACTIONID) as \"sales\")\n",
    "df.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 9 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- clusterid: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- Revenue: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- sales: long (nullable = false)\n",
      " |-- PDPCountPerDay: double (nullable = true)\n",
      " |-- PLPViewsPerDay: double (nullable = true)\n",
      " |-- PLPClicksPerDay: double (nullable = true)\n",
      " |-- TotalAddToCartPerDay: double (nullable = true)\n",
      " |-- CheckoutsPerDay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.join(gaData, Seq(PRODUCTID, CLUSTERID, DATE))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 5 more fields]\n",
      "res16: Array[org.apache.spark.sql.Row] = Array([2022-12-06,2022-11-07])\n"
     ]
    }
   ],
   "source": [
    "df = df.select(PRODUCTID, CLUSTERID, DATE, QUANTITY, \"sales\", \"Revenue\", \"PLPViewsPerDay\")\n",
    "df.select(max(DATE), min(DATE)).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DFToCSV.putDF(\"/data/bhavesh/CSV Files/newTargetAnalysisDF.csv\", df.filter(col(CLUSTERID) === \"cluster1\").repartition(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 3 more fields]\n",
      "e_theta: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 3 more fields]\n",
      "mle: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 3 more fields]\n",
      "e_theta: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 3 more fields]\n"
     ]
    }
   ],
   "source": [
    "var mle = df.filter(col(DATE) === experimentDate).select(PRODUCTID, CLUSTERID, \"Revenue\", \"sales\", \"PLPViewsPerDay\")\n",
    "var e_theta = df.filter(col(DATE) < experimentDate).select(PRODUCTID, CLUSTERID, \"Revenue\", \"sales\", \"PLPViewsPerDay\")\n",
    "mle = mle.groupBy(PRODUCTID, CLUSTERID).agg(sum(\"Revenue\") as \"revenue\", sum(\"sales\") as \"sales\", sum(\"PLPViewsPerDay\") as \"PLPViewsPerDay\")\n",
    "e_theta = e_theta.groupBy(PRODUCTID, CLUSTERID).agg(sum(\"Revenue\") as \"revenue\", sum(\"sales\") as \"sales\", sum(\"PLPViewsPerDay\") as \"PLPViewsPerDay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 4 more fields]\n",
      "mle: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 5 more fields]\n",
      "e_theta: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 4 more fields]\n",
      "e_theta: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 5 more fields]\n"
     ]
    }
   ],
   "source": [
    "mle = mle.withColumn(\"revenue_per_product_plp_views\", when(col(\"PLPViewsPerDay\")>0, col(\"revenue\")/col(\"PLPViewsPerDay\")).otherwise(lit(0.0)))\n",
    "mle = mle.withColumn(\"sales_per_product_plp_views\", when(col(\"PLPViewsPerDay\")>0, col(\"sales\")/col(\"PLPViewsPerDay\")).otherwise(lit(0.0)))\n",
    "\n",
    "e_theta = e_theta.withColumn(\"revenue_per_product_plp_views\", when(col(\"PLPViewsPerDay\")>0, col(\"revenue\")/col(\"PLPViewsPerDay\")).otherwise(lit(0.0)))\n",
    "e_theta = e_theta.withColumn(\"sales_per_product_plp_views\", when(col(\"PLPViewsPerDay\")>0, col(\"sales\")/col(\"PLPViewsPerDay\")).otherwise(lit(0.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_df: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 7 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- clusterid: string (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- sales: integer (nullable = true)\n",
      " |-- PLPViewsPerDay: double (nullable = true)\n",
      " |-- revenue_per_product_plp_views: double (nullable = true)\n",
      " |-- sales_per_product_plp_views: double (nullable = true)\n",
      " |-- revenue_per_product_plp_views_e_theta: double (nullable = false)\n",
      " |-- sales_per_product_plp_views_e_theta: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var final_df = mle.join(e_theta.withColumnRenamed(\"revenue_per_product_plp_views\", \"revenue_per_product_plp_views_e_theta\").withColumnRenamed(\"sales_per_product_plp_views\", \"sales_per_product_plp_views_e_theta\").select(PRODUCTID, CLUSTERID, \"revenue_per_product_plp_views_e_theta\", \"sales_per_product_plp_views_e_theta\"), Seq(PRODUCTID, CLUSTERID), \"left_outer\").na.fill(0.0, Seq(\"revenue_per_product_plp_views_e_theta\", \"sales_per_product_plp_views_e_theta\"))\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_df: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 8 more fields]\n",
      "final_df: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "final_df = final_df.withColumn(\"BayesianParameterOnRevenuePerView\", (col(\"revenue_per_product_plp_views_e_theta\")/(lit(1.0)+col(\"PLPViewsPerDay\"))) + ((col(\"PLPViewsPerDay\")*col(\"revenue_per_product_plp_views\"))/(lit(1.0)+col(\"PLPViewsPerDay\"))))\n",
    "final_df = final_df.withColumn(\"BayesianParameterOnSalesPerView\", (col(\"sales_per_product_plp_views_e_theta\")/(lit(1.0)+col(\"PLPViewsPerDay\"))) + ((col(\"PLPViewsPerDay\")*col(\"sales_per_product_plp_views\"))/(lit(1.0)+col(\"PLPViewsPerDay\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DFToCSV.putDF(\"/data/bhavesh/CSV Files/BayesianTarget.csv\", final_df.filter(col(CLUSTERID) === \"cluster1\").repartition(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DFToCSV.putDF(\"/data/bhavesh/CSV Files/ExperimentDayTargetVariable.csv\",mle.filter(col(CLUSTERID) === \"cluster1\").repartition(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DFToCSV.putDF(\"/data/bhavesh/CSV Files/HistoryDaysTargetVariable.csv\",e_theta.filter(col(CLUSTERID) === \"cluster1\").repartition(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_df: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "var final_df = CSVToDF.getDF(\"/data/bhavesh/CSV Files/BayesianTarget.csv\", inferSchema=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "productAttributes: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [productid: string, brickid: string]\n"
     ]
    }
   ],
   "source": [
    "var productAttributes = (ParquetToDF.getDF(PRODUCTS_LEGOS_PATH)\n",
    ".withColumnRenamed(ITEM_ID,PRODUCTID)\n",
    ".withColumnRenamed(SIMILAR_GROUP_LEVEL,BRICKID)\n",
    ".select(PRODUCTID,BRICKID).distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brickDetails: org.apache.spark.sql.DataFrame = [l1name: string, l2name: string ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "var brickDetails = (ParquetToDF.getDF(BRICK_DETAILS_LEGOS_PATH)\n",
    ".select(col(L1NAME), col(L2NAME), col(BRICKNAME), col(SIMILAR_GROUP_LEVEL).as(BRICKID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- clusterid: string (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- sales: integer (nullable = true)\n",
      " |-- PLPViewsPerDay: double (nullable = true)\n",
      " |-- revenue_per_product_plp_views: double (nullable = true)\n",
      " |-- sales_per_product_plp_views: double (nullable = true)\n",
      " |-- revenue_per_product_plp_views_e_theta: double (nullable = true)\n",
      " |-- sales_per_product_plp_views_e_theta: double (nullable = true)\n",
      " |-- BayesianParameterOnRevenuePerView: double (nullable = true)\n",
      " |-- BayesianParameterOnSalesPerView: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: Double = 1325.0\n",
      "C_revenue_per_view: Double = 25.90462420368837\n",
      "C_sales_per_view: Double = 0.02539355319889995\n",
      "(1325.0,25.90462420368837,0.02539355319889995)\n"
     ]
    }
   ],
   "source": [
    "var m: Double = final_df.agg(callUDF(\"percentile_approx\", col(\"PLPViewsPerDay\"), lit(0.9))).head().get(0) match { case d: Double => d}\n",
    "var C_revenue_per_view: Double = final_df.agg(mean(\"revenue_per_product_plp_views\")).head().get(0) match { case d: Double => d}\n",
    "var C_sales_per_view: Double = final_df.agg(mean(\"sales_per_product_plp_views\")).head().get(0) match { case d: Double => d}\n",
    "println(m, C_revenue_per_view, C_sales_per_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_df: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 11 more fields]\n"
     ]
    }
   ],
   "source": [
    "final_df = (\n",
    "    final_df\n",
    "    .withColumn(\"NormalizedRevenuePerViews\", ((col(\"revenue_per_product_plp_views\")*col(\"PLPViewsPerDay\")) + lit(m*C_revenue_per_view))/(col(\"PLPViewsPerDay\")+lit(m)))\n",
    "    .withColumn(\"NormalizedSalesPerViews\", ((col(\"sales_per_product_plp_views\")*col(\"PLPViewsPerDay\")) + lit(m*C_sales_per_view))/(col(\"PLPViewsPerDay\")+lit(m)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- clusterid: string (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- sales: integer (nullable = true)\n",
      " |-- PLPViewsPerDay: double (nullable = true)\n",
      " |-- revenue_per_product_plp_views: double (nullable = true)\n",
      " |-- sales_per_product_plp_views: double (nullable = true)\n",
      " |-- revenue_per_product_plp_views_e_theta: double (nullable = true)\n",
      " |-- sales_per_product_plp_views_e_theta: double (nullable = true)\n",
      " |-- BayesianParameterOnRevenuePerView: double (nullable = true)\n",
      " |-- BayesianParameterOnSalesPerView: double (nullable = true)\n",
      " |-- NormalizedRevenuePerViews: double (nullable = true)\n",
      " |-- NormalizedSalesPerViews: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_df: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 17 more fields]\n"
     ]
    }
   ],
   "source": [
    "final_df = (\n",
    "    final_df\n",
    "    .withColumn(\"RankByRevenuePerViews\", row_number().over(Window.partitionBy(CLUSTERID).orderBy(desc(\"revenue_per_product_plp_views\"))))\n",
    "    .withColumn(\"RankBySalesPerViews\", row_number().over(Window.partitionBy(CLUSTERID).orderBy(desc(\"sales_per_product_plp_views\"))))\n",
    "    .withColumn(\"RankByBayesianRevenuePerViews\", row_number().over(Window.partitionBy(CLUSTERID).orderBy(desc(\"BayesianParameterOnRevenuePerView\"))))\n",
    "    .withColumn(\"RankByBayesianSalesPerViews\", row_number().over(Window.partitionBy(CLUSTERID).orderBy(desc(\"BayesianParameterOnSalesPerView\"))))\n",
    "    .withColumn(\"RankByNormalizedRevenuePerViews\", row_number().over(Window.partitionBy(CLUSTERID).orderBy(desc(\"NormalizedRevenuePerViews\"))))\n",
    "    .withColumn(\"RankByNormalizedSalesPerViews\", row_number().over(Window.partitionBy(CLUSTERID).orderBy(desc(\"NormalizedSalesPerViews\"))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- clusterid: string (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- sales: integer (nullable = true)\n",
      " |-- PLPViewsPerDay: double (nullable = true)\n",
      " |-- revenue_per_product_plp_views: double (nullable = true)\n",
      " |-- sales_per_product_plp_views: double (nullable = true)\n",
      " |-- revenue_per_product_plp_views_e_theta: double (nullable = true)\n",
      " |-- sales_per_product_plp_views_e_theta: double (nullable = true)\n",
      " |-- BayesianParameterOnRevenuePerView: double (nullable = true)\n",
      " |-- BayesianParameterOnSalesPerView: double (nullable = true)\n",
      " |-- NormalizedRevenuePerViews: double (nullable = true)\n",
      " |-- NormalizedSalesPerViews: double (nullable = true)\n",
      " |-- RankByRevenuePerViews: integer (nullable = true)\n",
      " |-- RankBySalesPerViews: integer (nullable = true)\n",
      " |-- RankByBayesianRevenuePerViews: integer (nullable = true)\n",
      " |-- RankByBayesianSalesPerViews: integer (nullable = true)\n",
      " |-- RankByNormalizedRevenuePerViews: integer (nullable = true)\n",
      " |-- RankByNormalizedSalesPerViews: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DFToCSV.putDF(\"/data/bhavesh/CSV Files/TargetVariableRanked.csv\", final_df.repartition(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_df: org.apache.spark.sql.DataFrame = [productid: string, clusterid: string ... 17 more fields]\n",
      "root\n",
      " |-- productid: string (nullable = true)\n",
      " |-- clusterid: string (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- sales: integer (nullable = true)\n",
      " |-- PLPViewsPerDay: double (nullable = true)\n",
      " |-- revenue_per_product_plp_views: double (nullable = true)\n",
      " |-- sales_per_product_plp_views: double (nullable = true)\n",
      " |-- revenue_per_product_plp_views_e_theta: double (nullable = true)\n",
      " |-- sales_per_product_plp_views_e_theta: double (nullable = true)\n",
      " |-- BayesianParameterOnRevenuePerView: double (nullable = true)\n",
      " |-- BayesianParameterOnSalesPerView: double (nullable = true)\n",
      " |-- NormalizedRevenuePerViews: double (nullable = true)\n",
      " |-- NormalizedSalesPerViews: double (nullable = true)\n",
      " |-- RankByRevenuePerViews: integer (nullable = true)\n",
      " |-- RankBySalesPerViews: integer (nullable = true)\n",
      " |-- RankByBayesianRevenuePerViews: integer (nullable = true)\n",
      " |-- RankByBayesianSalesPerViews: integer (nullable = true)\n",
      " |-- RankByNormalizedRevenuePerViews: integer (nullable = true)\n",
      " |-- RankByNormalizedSalesPerViews: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = CSVToDF.getDF(\"/data/bhavesh/CSV Files/TargetVariableRanked.csv\", inferSchema=true)\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranks: Seq[Int] = List(100, 1000, 5000, 10000)\n",
      "PLPLevelNames: Seq[String] = List(l1name, l2name, brickname)\n",
      "RankColumn: Seq[String] = List(RankByRevenuePerViews, RankBySalesPerViews, RankByBayesianRevenuePerViews, RankByBayesianSalesPerViews, RankByNormalizedRevenuePerViews, RankByNormalizedSalesPerViews)\n"
     ]
    }
   ],
   "source": [
    "val Ranks = Seq(100, 1000, 5000, 10000)\n",
    "val PLPLevelNames=Seq(L1NAME,L2NAME,BRICKNAME)\n",
    "val RankColumn = Seq(\"RankByRevenuePerViews\", \"RankBySalesPerViews\", \"RankByBayesianRevenuePerViews\", \"RankByBayesianSalesPerViews\", \"RankByNormalizedRevenuePerViews\", \"RankByNormalizedSalesPerViews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rankDF: org.apache.spark.sql.DataFrame = null\n",
      "rankDistributionDFForAllRankColumns: org.apache.spark.sql.DataFrame = null\n"
     ]
    }
   ],
   "source": [
    "var rankDF: DataFrame = null\n",
    "var rankDistributionDFForAllRankColumns: DataFrame = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "productBrandMapping: org.apache.spark.sql.DataFrame = [productid: string, brandname: string]\n",
      "premiumBrands: org.apache.spark.sql.DataFrame = [brandname: string, isPremiumBrand: int]\n"
     ]
    }
   ],
   "source": [
    "var productBrandMapping = (\n",
    "    ParquetToDF.getDF(PRODUCTS_LEGOS_PATH)\n",
    "        .selectExpr(s\"$ITEM_ID as $PRODUCTID\", s\"$BRANDNAME\")\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \"&\", \" \"))\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \"_\", \" \"))\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \"\\\\.\", \" \"))\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \"@\", \" \"))\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \" +\", \" \")) //replace multiple spaces with a singe space\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \" \", \"-\"))\n",
    "        .distinct()\n",
    "        .na.drop\n",
    ")\n",
    "\n",
    "var premiumBrands = (\n",
    "    CSVToDF.getDF(\"/data/ecomm/ajio/metadata/premiumbrands.csv\")\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \"&\", \" \"))\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \"_\", \" \"))\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \"\\\\.\", \" \"))\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \"@\", \" \"))\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \" +\", \" \")) //replace multiple spaces with a singe space\n",
    "        .withColumn(BRANDNAME, regexp_replace(lower(col(BRANDNAME)), \" \", \"-\"))\n",
    "        /*\n",
    "        //We deliberately avoid substitutions like these in code. We prefer to edit the csv itself.\n",
    "        //If there is an issue with the substition, it is A LOT easier to submit a corrected csv than it is to redeploy a jar\n",
    "        .withColumn(BRANDNAME,\n",
    "          when(col(BRANDNAME)===\"accessorize\",\"accessorize-london\")\n",
    "            .otherwise(col(BRANDNAME))\n",
    "        )*/ .distinct()\n",
    "        .withColumn(\"isPremiumBrand\", lit(1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792e6d91d19841d288e3874c58e52a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for(plpLevelName <- PLPLevelNames){\n",
    "    for(rank <- Ranks){\n",
    "        rankDistributionDFForAllRankColumns = null\n",
    "        for(rankColumn <- RankColumn){\n",
    "            rankDF = (\n",
    "                final_df\n",
    "                .filter(col(rankColumn) <= rank && col(rankColumn) > 0)\n",
    "                .join(productAttributes.select(PRODUCTID, BRICKID), Seq(PRODUCTID))\n",
    "                .join(brickDetails, Seq(BRICKID), \"left\")\n",
    "                .groupBy(CLUSTERID,plpLevelName)\n",
    "                .agg(countDistinct(PRODUCTID).alias(\"ProductCount\"))\n",
    "            )\n",
    "            if (rankDistributionDFForAllRankColumns == null) {\n",
    "                  rankDistributionDFForAllRankColumns = rankDF.withColumnRenamed(\"ProductCount\", s\"ProductCount$rankColumn\")\n",
    "            } else {\n",
    "                  rankDistributionDFForAllRankColumns = (\n",
    "                      rankDistributionDFForAllRankColumns\n",
    "                      .join(\n",
    "                        rankDF.withColumnRenamed(\"ProductCount\", s\"ProductCount$rankColumn\"),\n",
    "                        Seq(CLUSTERID, plpLevelName), \"full_outer\"\n",
    "                      )\n",
    "                  )\n",
    "            }\n",
    "        }\n",
    "        rankDistributionDFForAllRankColumns = rankDistributionDFForAllRankColumns.na.fill(0)\n",
    "        DFToCSV.putDF(s\"/data/bhavesh/CSV Files/PLPResultsEvaluation/2022-12-06/RankingComparisonMetrics/RankwiseProductDistributionDF/$plpLevelName/rankLimit=$rank\", rankDistributionDFForAllRankColumns.repartition(1), hasHeader = true)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5242a7623454eaba00e1f59e1a021a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7b0d1c2a81f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'var brandDistributionDFForAllRankColumns: DataFrame = null\\n\\nfor(rank <- Ranks){\\n    brandDistributionDFForAllRankColumns = null\\n    for(rankColumn <- RankColumn){\\n        rankDF = (\\n            final_df\\n            .filter(col(rankColumn) <= rank && col(rankColumn) > 0)\\n            .join(productBrandMapping, Seq(PRODUCTID), \"left\")\\n            .groupBy(CLUSTERID,BRANDNAME)\\n            .agg(countDistinct(PRODUCTID).alias(\"ProductCount\"))\\n        )\\n        if (brandDistributionDFForAllRankColumns == null) {\\n              brandDistributionDFForAllRankColumns = rankDF.withColumnRenamed(\"ProductCount\", s\"ProductCount$rankColumn\")\\n        } else {\\n              brandDistributionDFForAllRankColumns = (\\n                  brandDistributionDFForAllRankColumns\\n                  .join(\\n                    rankDF.withColumnRenamed(\"ProductCount\", s\"ProductCount$rankColumn\"),\\n                    Seq(CLUSTERID, BRANDNAME), \"full_outer\"\\n                  )\\n              )\\n        }\\n    }\\n    brandDistributionDFForAllRankColumns = brandDistributionDFForAllRankColumns.na.fill(0).join(premiumBrands, Seq(BRANDNAME), \"left\").na.fill(0, Seq(\"isPremiumBrand\"))\\n    brandDistributionDFForAllRankColumns = brandDistributionDFForAllRankColumns.select(CLUSTERID,BRANDNAME,\"isPremiumBrand\", \"ProductCountRankByRevenuePerViews\", \"ProductCountRankBySalesPerViews\", \"ProductCountRankByBayesianRevenuePerViews\", \"ProductCountRankByBayesianSalesPerViews\", \"ProductCountRankByNormalizedRevenuePerViews\", \"ProductCountRankByNormalizedSalesPerViews\")\\n    DFToCSV.putDF(s\"/data/bhavesh/CSV Files/PLPResultsEvaluation/2022-12-06/RankingComparisonMetrics/RankwiseBrandDistributionDF/rankLimit=$rank\", brandDistributionDFForAllRankColumns.repartition(1), hasHeader = true)\\n}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2372\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-125>\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/kernels/kernelmagics.py\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001b[0m in \u001b[0;36mexecute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmimetype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_session_on_spark_statement_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36mrun_command\u001b[0;34m(self, command, client_name)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36m_get_statement_output\u001b[0;34m(self, session, statement_id)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'progress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(self, retries)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "var brandDistributionDFForAllRankColumns: DataFrame = null\n",
    "\n",
    "for(rank <- Ranks){\n",
    "    brandDistributionDFForAllRankColumns = null\n",
    "    for(rankColumn <- RankColumn){\n",
    "        rankDF = (\n",
    "            final_df\n",
    "            .filter(col(rankColumn) <= rank && col(rankColumn) > 0)\n",
    "            .join(productBrandMapping, Seq(PRODUCTID), \"left\")\n",
    "            .groupBy(CLUSTERID,BRANDNAME)\n",
    "            .agg(countDistinct(PRODUCTID).alias(\"ProductCount\"))\n",
    "        )\n",
    "        if (brandDistributionDFForAllRankColumns == null) {\n",
    "              brandDistributionDFForAllRankColumns = rankDF.withColumnRenamed(\"ProductCount\", s\"ProductCount$rankColumn\")\n",
    "        } else {\n",
    "              brandDistributionDFForAllRankColumns = (\n",
    "                  brandDistributionDFForAllRankColumns\n",
    "                  .join(\n",
    "                    rankDF.withColumnRenamed(\"ProductCount\", s\"ProductCount$rankColumn\"),\n",
    "                    Seq(CLUSTERID, BRANDNAME), \"full_outer\"\n",
    "                  )\n",
    "              )\n",
    "        }\n",
    "    }\n",
    "    brandDistributionDFForAllRankColumns = brandDistributionDFForAllRankColumns.na.fill(0).join(premiumBrands, Seq(BRANDNAME), \"left\").na.fill(0, Seq(\"isPremiumBrand\"))\n",
    "    brandDistributionDFForAllRankColumns = brandDistributionDFForAllRankColumns.select(CLUSTERID,BRANDNAME,\"isPremiumBrand\", \"ProductCountRankByRevenuePerViews\", \"ProductCountRankBySalesPerViews\", \"ProductCountRankByBayesianRevenuePerViews\", \"ProductCountRankByBayesianSalesPerViews\", \"ProductCountRankByNormalizedRevenuePerViews\", \"ProductCountRankByNormalizedSalesPerViews\")\n",
    "    DFToCSV.putDF(s\"/data/bhavesh/CSV Files/PLPResultsEvaluation/2022-12-06/RankingComparisonMetrics/RankwiseBrandDistributionDF/rankLimit=$rank\", brandDistributionDFForAllRankColumns.repartition(1), hasHeader = true)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
